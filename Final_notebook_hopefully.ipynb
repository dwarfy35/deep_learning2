{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwarfy35/deep_learning2/blob/main/Final_notebook_hopefully.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd36ANKxZZuB",
        "outputId": "66a13712-e26f-438e-bf97-e6cb5f7fa762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from torch.autograd import grad\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.autograd import grad\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from itertools import product\n",
        "from collections import Counter\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QzLfXw25ayCu",
        "outputId": "d4451e7b-17e6-4ace-9f9c-518d3c4d3c63"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "number of fonts is 14990"
      ],
      "metadata": {
        "id": "uHkxzK671iIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tot_fonts = 14990\n",
        "train_fonts = (0,int(14990*0.8))\n",
        "test_fonts = (int(14990*0.8),int(14990*0.99))\n",
        "val_fonts = (int(14990*0.99),14990)\n",
        "print(train_fonts)\n",
        "print(test_fonts)\n",
        "print(val_fonts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUh8m6q4EMkP",
        "outputId": "b4da4aa9-f005-48ca-e0db-0a868d2eb374"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 11992)\n",
            "(11992, 14840)\n",
            "(14840, 14990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_labels(dataset):\n",
        "    \"\"\"\n",
        "    Count the number of occurrences of each label in the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): An instance of NPZDataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with labels as keys and their counts as values.\n",
        "    \"\"\"\n",
        "    # Extract all labels from the dataset\n",
        "    all_labels = [dataset[i][2].item() for i in range(len(dataset))]  # Label is the third item in each dataset entry\n",
        "\n",
        "    # Count occurrences of each label\n",
        "    label_counts = Counter(all_labels)\n",
        "\n",
        "    # Sort and convert to a dictionary for a cleaner output\n",
        "    sorted_counts = dict(sorted(label_counts.items()))\n",
        "\n",
        "    # Display results\n",
        "    print(\"Label counts in the dataset:\")\n",
        "    for label, count in sorted_counts.items():\n",
        "        print(f\"Label {chr(label + 65)} ({label}): {count}\")\n",
        "\n",
        "    return sorted_counts\n"
      ],
      "metadata": {
        "id": "eH3bur00dPLh"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "fX2qIT2U26nz"
      },
      "outputs": [],
      "source": [
        "class Generator3(nn.Module):\n",
        "    def __init__(self, latent_dim, style_dim=1, img_size=32):\n",
        "        super(Generator3, self).__init__()\n",
        "\n",
        "        self.init_size = img_size // 16  # Initial spatial size after FC layer\n",
        "        self.fc = nn.Linear(latent_dim + style_dim, 512 * self.init_size * self.init_size)\n",
        "\n",
        "        self.deconv_blocks = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z, condition):\n",
        "        condition_features = condition.mean(dim=(2, 3))  # Global average pooling across height and width\n",
        "        condition_features = condition_features.view(-1, 1)  # Ensure shape matches style_dim, likely not necessary but a good precaution\n",
        "        z = torch.cat((z, condition_features), dim=1) #Concatenate to get final z\n",
        "\n",
        "        out = self.fc(z).view(-1, 512, self.init_size, self.init_size) #Pass through initial linear layer and ensure correct shape\n",
        "\n",
        "        img = self.deconv_blocks(out) # Standard generator\n",
        "        return img\n",
        "\n",
        "class Discriminator3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator3, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1 * 2, 32, 4, 2, 1, bias=False),  # Changed to now require 2 input channels since we concatenate\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 32 * 2, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32 * 2, 32 * 4, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32 * 4, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, img, condition_img):\n",
        "        x = torch.cat((img, condition_img), dim=1) # Concatenate the input image and condition along the channel dimension\n",
        "        validity = self.main(x) # Then through the discrim as usual\n",
        "        return validity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "vsoPo3fnZyzQ"
      },
      "outputs": [],
      "source": [
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, npz_file, font_range=None, transform=None, filter_label=None, num_samples=None, missing_p=False):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with optional filtering by font range.\n",
        "\n",
        "        Args:\n",
        "            npz_file (str): Path to the .npz file containing images and labels.\n",
        "            font_range (tuple, optional): Range of fonts to include (start, end). If None, include all fonts.\n",
        "            transform (callable, optional): Transformations to apply to the images.\n",
        "            filter_label (int, optional): Filter dataset by a specific label.\n",
        "            num_samples (int, optional): Limit the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        # Load the data from the .npz file\n",
        "        data = np.load(npz_file)\n",
        "        self.images = data['images']\n",
        "        self.labels = data['labels']\n",
        "        self.transform = transform\n",
        "\n",
        "        font_size = 26  # Number of letters per font (A-Z)\n",
        "\n",
        "        # Limit the dataset to the specified range of fonts\n",
        "        if font_range is not None and missing_p == True:\n",
        "            start_font, end_font = font_range\n",
        "            start_idx = start_font * font_size\n",
        "            end_idx = end_font * font_size  # Exclusive of the last font\n",
        "            self.images = self.images[start_idx:end_idx]\n",
        "            self.labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "        if font_range is not None and missing_p == False:\n",
        "            start_font, end_font = font_range\n",
        "            start_idx = start_font * font_size\n",
        "            end_idx = end_font * font_size  # Exclusive of the last font\n",
        "            self.images = self.images[start_idx:end_idx]\n",
        "            self.labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "        # Filter by label if specified\n",
        "        if filter_label is not None:\n",
        "            # Find indices of the desired label\n",
        "            label_indices = np.where(self.labels == filter_label)[0]\n",
        "\n",
        "            # If num_samples is specified, limit the number of samples\n",
        "            if num_samples is not None:\n",
        "                label_indices = label_indices[:num_samples]\n",
        "\n",
        "            # Filter images and labels\n",
        "            self.images = self.images[label_indices]\n",
        "            self.labels = self.labels[label_indices]\n",
        "\n",
        "        # Validate that labels follow the sequence 0-25 for each font group\n",
        "        self.validate_labels(font_size)\n",
        "\n",
        "    def validate_labels(self, font_size):\n",
        "      \"\"\"\n",
        "      Validate that each font group has labels 0-25 and transitions between groups correctly.\n",
        "\n",
        "      Args:\n",
        "          font_size (int): Number of letters per font (default is 26 for A-Z).\n",
        "      \"\"\"\n",
        "      num_fonts = len(self.labels) // font_size\n",
        "\n",
        "      for font_id in range(num_fonts):\n",
        "          start_idx = font_id * font_size\n",
        "          end_idx = start_idx + font_size\n",
        "\n",
        "          # Extract labels for the current font group\n",
        "          font_labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "          # Check if the labels are sequentially 0-25\n",
        "          expected_labels = list(range(font_size))\n",
        "          if font_labels.tolist() != expected_labels:\n",
        "              print(f\"Error: Font {font_id} does not have labels 0-25.\")\n",
        "              print(f\"Actual labels: {font_labels.tolist()}\")\n",
        "              raise ValueError(f\"Font {font_id} has incorrect labels.\")\n",
        "\n",
        "      # Check transitions between font groups\n",
        "      for font_id in range(num_fonts - 1):\n",
        "          current_end_label = self.labels[(font_id + 1) * font_size - 1]  # Last label of the current font\n",
        "          next_start_label = self.labels[(font_id + 1) * font_size]      # First label of the next font\n",
        "          if current_end_label != 25 or next_start_label != 0:\n",
        "              print(f\"Error in transition between fonts {font_id} and {font_id + 1}.\")\n",
        "              print(f\"Last label of font {font_id}: {current_end_label}\")\n",
        "              print(f\"First label of font {font_id + 1}: {next_start_label}\")\n",
        "              raise ValueError(f\"Incorrect transition between fonts {font_id} and {font_id + 1}.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve an item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Image, condition image, and label.\n",
        "        \"\"\"\n",
        "        # Get the image and label for the given index\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Determine the font ID based on dataset ordering\n",
        "        font_id = idx // 26  # Calculate the font ID based on position in dataset\n",
        "\n",
        "        # Get the \"A\" from the same font as the condition\n",
        "        condition_index = font_id * 26  # First letter (A) of the current font\n",
        "        condition_image = self.images[condition_index]\n",
        "        if self.labels[condition_index] != 0:\n",
        "          raise ValueError(\"Condition image label should be 0.\")\n",
        "\n",
        "        # Reshape the images\n",
        "        image = image[np.newaxis, ...]  # Add channel dimension\n",
        "        condition_image = condition_image[np.newaxis, ...]\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            condition_image = self.transform(condition_image)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        condition_image = torch.tensor(condition_image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, condition_image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "vd0Se_Ttoyh4"
      },
      "outputs": [],
      "source": [
        "def number_to_alphabet(num):\n",
        "    if 0 <= num <= 25:\n",
        "        return chr(num + 65)  # 65 is the ASCII code for 'A'\n",
        "    else:\n",
        "        raise ValueError(\"Number must be between 0 and 25 inclusive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "In_xt_E_5_J0"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, condition_samples, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Compute the gradient penalty for WGAN-GP in a conditional GAN setup where\n",
        "    the discriminator takes both the input and the condition image.\n",
        "\n",
        "    Args:\n",
        "        discriminator: The discriminator model.\n",
        "        real_samples: Batch of real images.\n",
        "        fake_samples: Batch of fake images generated by the generator.\n",
        "        condition_samples: Batch of conditional images (e.g., letter A from the same font).\n",
        "        device: The device to run the computations on.\n",
        "\n",
        "    Returns:\n",
        "        Gradient penalty (scalar).\n",
        "    \"\"\"\n",
        "    # Interpolate between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)  # Mixing factor\n",
        "    interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "\n",
        "    # Compute the discriminator output for the interpolated samples and conditions\n",
        "    d_interpolates = discriminator(interpolated_samples, condition_samples)\n",
        "\n",
        "    # Flatten the discriminator output\n",
        "    d_interpolates = d_interpolates.view(-1)  # Shape: [batch_size]\n",
        "\n",
        "    # Create grad_outputs for the gradient computation\n",
        "    fake = torch.ones(d_interpolates.size(), device=device)\n",
        "\n",
        "    # Compute gradients of the discriminator's output w.r.t. interpolated inputs\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolated_samples,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    # Compute the gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)  # Flatten gradients\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "    return gradient_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_fonts(dataset, start_font, end_font, font_size=26):\n",
        "    \"\"\"\n",
        "    Display a range of fonts from the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (NPZDataset): An instance of the NPZDataset class.\n",
        "        start_font (int): The starting font index (inclusive).\n",
        "        end_font (int): The ending font index (exclusive).\n",
        "        font_size (int): The number of letters per font (default is 26 for A-Z).\n",
        "    \"\"\"\n",
        "    num_fonts = end_font - start_font\n",
        "    if num_fonts <= 0:\n",
        "        print(\"Invalid font range. Ensure end_font > start_font.\")\n",
        "        return\n",
        "\n",
        "    # Calculate the number of rows and columns for the grid\n",
        "    num_rows = num_fonts\n",
        "    num_cols = font_size\n",
        "\n",
        "    # Set up the figure\n",
        "    plt.figure(figsize=(num_cols * 1.5, num_rows * 1.5))\n",
        "\n",
        "    for font_idx in range(start_font, end_font):\n",
        "        for letter_idx in range(font_size):\n",
        "            global_idx = font_idx * font_size + letter_idx\n",
        "            if global_idx >= len(dataset):\n",
        "                print(f\"Reached end of dataset at index {global_idx}.\")\n",
        "                break\n",
        "\n",
        "            image, _, label = dataset[global_idx]\n",
        "            image = image.squeeze().numpy()  # Remove channel dimension for visualization\n",
        "\n",
        "            # Plot the image\n",
        "            plt.subplot(num_rows, num_cols, (font_idx - start_font) * font_size + letter_idx + 1)\n",
        "            plt.imshow(image, cmap=\"gray\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(chr(65 + label.item()), fontsize=8)  # Use label to display the corresponding letter\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rC-jX-W5RXyU"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_labels(self, font_size):\n",
        "    \"\"\"\n",
        "    Validate that each font group has labels 0-25 and transitions between groups correctly.\n",
        "\n",
        "    Args:\n",
        "        font_size (int): Number of letters per font (default is 26 for A-Z).\n",
        "    \"\"\"\n",
        "    num_fonts = len(self.labels) // font_size\n",
        "\n",
        "    for font_id in range(num_fonts):\n",
        "        start_idx = font_id * font_size\n",
        "        end_idx = start_idx + font_size\n",
        "\n",
        "        # Extract labels for the current font group\n",
        "        font_labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "        # Check if the labels are sequentially 0-25\n",
        "        expected_labels = list(range(font_size))\n",
        "        if font_labels.tolist() != expected_labels:\n",
        "            print(f\"Error: Font {font_id} does not have labels 0-25.\")\n",
        "            print(f\"Actual labels: {font_labels.tolist()}\")\n",
        "            raise ValueError(f\"Font {font_id} has incorrect labels.\")\n",
        "\n",
        "    # Check transitions between font groups\n",
        "    for font_id in range(num_fonts - 1):\n",
        "        current_end_label = self.labels[(font_id + 1) * font_size - 1]  # Last label of the current font\n",
        "        next_start_label = self.labels[(font_id + 1) * font_size]      # First label of the next font\n",
        "        if current_end_label != 25 or next_start_label != 0:\n",
        "            print(f\"Error in transition between fonts {font_id} and {font_id + 1}.\")\n",
        "            print(f\"Last label of font {font_id}: {current_end_label}\")\n",
        "            print(f\"First label of font {font_id + 1}: {next_start_label}\")\n",
        "            raise ValueError(f\"Incorrect transition between fonts {font_id} and {font_id + 1}.\")\n"
      ],
      "metadata": {
        "id": "2mwWfgNxXrAN"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "cw5G-vGNmn7a"
      },
      "outputs": [],
      "source": [
        "# Dataset creation\n",
        "#npz_file = \"/content/gdrive/My Drive/character_font.npz\"\n",
        "npz_file = \"/content/gdrive/My Drive/corrected_dataset2.npz\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "#fullData = NPZDataset(npz_file, font_range =(0,14990), transform=transform)\n",
        "\n",
        "train_data = NPZDataset(npz_file, font_range=train_fonts,transform=transform)\n",
        "test_data = NPZDataset(npz_file, font_range=test_fonts,transform=transform)\n",
        "val_data = NPZDataset(npz_file, font_range=val_fonts,transform=transform)\n",
        "#testing_data = NPZDataset(npz_file, font_range=(0,100),transform=transform,remove_missing_p=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_labels(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VB2b5lzKzoxe",
        "outputId": "a10a7b4d-4bc9-473d-f837-2a70285f6958"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-202-3493b0cd4374>:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image = torch.tensor(image, dtype=torch.float32)\n",
            "<ipython-input-202-3493b0cd4374>:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  condition_image = torch.tensor(condition_image, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-208-1a4a41ef6e4c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-200-ac990ea53e67>\u001b[0m in \u001b[0;36mcount_labels\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract all labels from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Label is the third item in each dataset entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Count occurrences of each label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-200-ac990ea53e67>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract all labels from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Label is the third item in each dataset entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Count occurrences of each label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-202-3493b0cd4374>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mcondition_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Convert to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_labels(test_data)"
      ],
      "metadata": {
        "id": "273_ZWZl0QUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_labels(val_data)"
      ],
      "metadata": {
        "id": "aimlMV4l0WNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Group the data by label\n",
        "grouped_data = defaultdict(list)\n",
        "for img, condition_img, label in train_data:  # Unpack condition_img as well\n",
        "    grouped_data[label.item()].append((img, condition_img, label))\n",
        "\n",
        "# Step 2: Prepare datasets for each label\n",
        "grouped_datasets = {\n",
        "    label: [(img, cond_img, lbl) for img, cond_img, lbl in imgs]\n",
        "    for label, imgs in grouped_data.items()\n",
        "}\n",
        "\n",
        "# Step 3: Create dataloaders for each group\n",
        "grouped_dataloaders = {\n",
        "    letter: DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    for letter, dataset in grouped_datasets.items()\n",
        "}\n"
      ],
      "metadata": {
        "id": "ipOhIMuYLyjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def displayGeneratedSample(generator, z_s, condition_imgs, labels, num_classes, class_index=0, device='cuda'):\n",
        "    \"\"\"\n",
        "    Displays one generated image from the generator.\n",
        "\n",
        "    Args:\n",
        "        generator (nn.Module): Pre-trained generator model.\n",
        "        z_s (torch.Tensor): Style vector.\n",
        "        condition_imgs (torch.Tensor): Conditional images (batch).\n",
        "        labels (torch.Tensor): Labels for the batch.\n",
        "        num_classes (int): Number of character classes.\n",
        "        class_index (int): Index of the sample in the batch to display.\n",
        "        device (str): Device for computation ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Ensure the inputs are on the correct device\n",
        "    z_s = z_s.to(device)\n",
        "    condition_imgs = condition_imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Create one-hot encoding for the batch\n",
        "    batch_size = labels.size(0)\n",
        "    z_c = torch.zeros(batch_size, num_classes, device=device)\n",
        "    z_c[torch.arange(batch_size), labels] = 1\n",
        "\n",
        "    # Concatenate style and class vectors\n",
        "    z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        fake_imgs = generator(z, condition_imgs)\n",
        "\n",
        "    # Select the specified class index\n",
        "    generated_img = fake_imgs[class_index].cpu().numpy()[0]  # Extract the first channel\n",
        "\n",
        "    # Rescale the image from [-1, 1] to [0, 255]\n",
        "    generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.imshow(generated_img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Generated Image for Class {chr(65 + labels[class_index].item())}\")  # Convert label to letter\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "C31T6p2jMYR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(images, noise_level=0.1):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to images.\n",
        "\n",
        "    Args:\n",
        "        images: Tensor of images (B, C, H, W).\n",
        "        noise_level: Standard deviation of the Gaussian noise.\n",
        "\n",
        "    Returns:\n",
        "        Noisy images.\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(images) * noise_level\n",
        "    return images + noise\n"
      ],
      "metadata": {
        "id": "1Iso39Z2BtcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_losses = []\n",
        "g_losses = []"
      ],
      "metadata": {
        "id": "W-sZnvEcSycN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/gdrive/My Drive\""
      ],
      "metadata": {
        "id": "ydr4yh_4bch2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# The multiple values are a remnant for when we wanted to do grid search\n",
        "latent_dim_values = [50,100,150]  # Dimension of the latent vector z\n",
        "style_dim = 1  # Remnant of when we tried changing the dimension of the flattened image\n",
        "img_size = 32\n",
        "num_classes = 26\n",
        "epochs = 1500\n",
        "n_critic_values = [1,3,5]\n",
        "zc_weight =  1\n",
        "lr_values = [0.00001,0.0002, 0.002]\n",
        "noise_levels = [0.005, 0.05, 0.2]\n",
        "smooth_values = [0.05,0.1,0.15]\n",
        "lambda_gp_values = [5,10,20]\n",
        "batch_size = 1024\n",
        "\n",
        "lr = 0.0002\n",
        "latent_dim = 100\n",
        "nl = 0.05\n",
        "n_critic = 1\n",
        "lambda_gp = 10\n",
        "sv = 0.1"
      ],
      "metadata": {
        "id": "RSXfIi8679f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "UbchB6_Tz4Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfqBaqlzZn2Z"
      },
      "outputs": [],
      "source": [
        "#for lambda_gp in lambda_gp_values:\n",
        "for h in range(1):\n",
        "  print(f\"Training with lr={lr}, lambda_gp={lambda_gp}, nl={nl}, n_critic={n_critic}, latent_dim={latent_dim}, sv={sv}\")\n",
        "  z_dim = latent_dim + num_classes #Remnant of when we wanted to train with different latent_dims\n",
        "\n",
        "  generator = Generator3(z_dim, style_dim, img_size).to(device)\n",
        "  discriminator = Discriminator3().to(device)\n",
        "\n",
        "    # Optimizers\n",
        "  optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  dlosses = []\n",
        "  glosses = []\n",
        "  d_test_losses = []\n",
        "  g_test_losses = []\n",
        "\n",
        "    # Training Loop\n",
        "  for epoch in range(epochs):\n",
        "        for letter in range(num_classes):\n",
        "            dataloader = grouped_dataloaders[letter]\n",
        "            for i, (real_imgs, condition_imgs, labels) in enumerate(dataloader):\n",
        "                real_imgs = real_imgs.to(device)\n",
        "                condition_imgs = condition_imgs.to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "                current_size = labels.size(0) # Makes sure that we dynamically adjust to anything requiring number of batches\n",
        "\n",
        "                real_imgs = real_imgs.permute(0, 2, 3, 1).cuda() # Fixes it so we are in the correct order for pytorch\n",
        "\n",
        "                condition_imgs = condition_imgs.permute(0, 2, 3, 1).cuda()\n",
        "\n",
        "\n",
        "                # Train Discriminator\n",
        "                z_s = torch.randn( current_size, latent_dim).to(device)  # Latent vector\n",
        "                z_c = torch.zeros( current_size, num_classes).to(device)  # One-hot class vector\n",
        "                z_c[torch.arange( current_size), labels] = 1 # Set the desired target to 1\n",
        "                z = torch.cat((z_s, z_c * zc_weight), dim=1)  # Combine latent vector and one hot\n",
        "                fake_imgs = generator(z, condition_imgs)  # Generate fake images\n",
        "                noisy_condition_imgs = add_noise(condition_imgs, noise_level=nl) # Add a small amount of noise to condition imgs for discrimininator for leniency\n",
        "                real_validity = discriminator(real_imgs,  noisy_condition_imgs)  # Include condition image\n",
        "                real_validity = real_validity - sv # Label smoothing which is supposed to help the generator by artifically decreasing the confidence of the discriminator on the real imgs\n",
        "                fake_validity = discriminator( fake_imgs,  noisy_condition_imgs)  # Include condition image\n",
        "                gradient_penalty = compute_gradient_penalty(\n",
        "                    discriminator,\n",
        "                    real_samples=real_imgs,\n",
        "                    fake_samples=fake_imgs,\n",
        "                    condition_samples= noisy_condition_imgs,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty # Wasserstein loss for discrim\n",
        "\n",
        "                optimizer_D.zero_grad()\n",
        "                d_loss.backward()\n",
        "                optimizer_D.step()\n",
        "\n",
        "                # Train Generator\n",
        "                if i % n_critic == 0: # Makes the generator train less frequently\n",
        "                    z_s = torch.randn (current_size, latent_dim).to(device) # Same as for discrim\n",
        "                    z = torch.cat((z_s, z_c), dim=1)\n",
        "                    fake_imgs = generator(z, condition_imgs)\n",
        "                    fake_validity = discriminator(fake_imgs,  noisy_condition_imgs)\n",
        "                    g_loss = -torch.mean(fake_validity)\n",
        "                    optimizer_G.zero_grad()\n",
        "                    g_loss.backward()\n",
        "                    optimizer_G.step()\n",
        "                # Display generated samples for first image in first batch every 200 epochs\n",
        "                if epoch % 200 == 0 and i == 0:\n",
        "                    displayGeneratedSample(generator, z_s, condition_imgs, labels, num_classes, class_index=0, device=device)\n",
        "\n",
        "\n",
        "        # Print training progress\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}, grad_penalty: {gradient_penalty}\")\n",
        "        # Test and save\n",
        "        if epoch % 10 == 0:\n",
        "          torch.save(generator.state_dict(), \"/content/gdrive/My Drive/generator.pth\")\n",
        "          torch.save(discriminator.state_dict(), \"/content/gdrive/My Drive/discriminator.pth\")\n",
        "          dlosses.append(d_loss.item())\n",
        "          glosses.append(g_loss.item())\n",
        "          # Test. Basically same as above just without updating the weights for either.\n",
        "          generator.eval()\n",
        "          discriminator.eval()\n",
        "          with torch.no_grad():\n",
        "            for i, (real_imgs, condition_imgs, labels) in enumerate(test_dataloader):\n",
        "                real_imgs = real_imgs.to(device)\n",
        "                condition_imgs = condition_imgs.to(device)\n",
        "\n",
        "                labels = labels.to(device)\n",
        "                current_size = labels.size(0)\n",
        "\n",
        "                real_imgs = real_imgs.permute(0, 2, 3, 1).cuda()\n",
        "\n",
        "                condition_imgs = condition_imgs.permute(0, 2, 3, 1).cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                z_s = torch.randn( current_size, latent_dim).to(device)  # Latent vector\n",
        "                z_c = torch.zeros( current_size, num_classes).to(device)  # One-hot class vector\n",
        "                z_c[torch.arange( current_size), labels] = 1 # Set the desired target to 1\n",
        "                z = torch.cat((z_s, z_c * zc_weight), dim=1)  # Combine latent vector and one hot\n",
        "                fake_imgs = generator(z, condition_imgs)  # Generate fake images\n",
        "                noisy_condition_imgs = add_noise(condition_imgs, noise_level=nl) # Add a small amount of noise to condition imgs for discrimininator for leniency\n",
        "                real_validity = discriminator(real_imgs,  noisy_condition_imgs)  # Include condition image\n",
        "                real_validity = real_validity - sv # Label smoothing which is supposed to help the generator by artifically decreasing the confidence of the discriminator on the real imgs\n",
        "                fake_validity = discriminator(fake_imgs.detach(),  noisy_condition_imgs)  # Include condition image\n",
        "                real_imgs.requires_grad = True # Required for gradient penalty\n",
        "                with torch.enable_grad():\n",
        "                  gradient_penalty = compute_gradient_penalty(\n",
        "                    discriminator,\n",
        "                    real_samples=real_imgs,\n",
        "                    fake_samples=fake_imgs,\n",
        "                    condition_samples= noisy_condition_imgs,  # Pass condition for gradient penalty\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                d_test_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "                g_test_loss = -torch.mean(fake_validity)\n",
        "            d_test_losses.append(d_loss.item())\n",
        "            g_test_losses.append(g_loss.item())\n",
        "            generator.train() # Prep for training again\n",
        "            discriminator.train()\n",
        "            print(f\"TESTING, D_loss_test: {d_test_loss.item():.4f}, G_loss_test: {g_test_loss.item():.4f}\")\n",
        "          csv_file = f\"{save_directory}/gan_metrics_lr_{lr}_gp_{lambda_gp}_nl_{nl}_critic_{n_critic}_dim_{latent_dim}_sv_{sv}_train.csv\"\n",
        "  metrics = pd.DataFrame({\n",
        "            \"epoch\": list(range(len(dlosses))),\n",
        "            \"d_loss\": dlosses,\n",
        "            \"g_loss\": glosses\n",
        "        })\n",
        "  metrics.to_csv(csv_file, index=False)\n",
        "  print(f\"Saved metrics to {csv_file}\")\n",
        "\n",
        "  csv_file = f\"{save_directory}/gan_metrics_lr_{lr}_gp_{lambda_gp}_nl_{nl}_critic_{n_critic}_dim_{latent_dim}_sv_{sv}_test.csv\"\n",
        "  metrics = pd.DataFrame({\n",
        "            \"epoch\": list(range(len(d_test_losses))),\n",
        "            \"d_loss\": d_test_losses,\n",
        "            \"g_loss\": g_test_losses\n",
        "        })\n",
        "  metrics.to_csv(csv_file, index=False)\n",
        "  print(f\"Saved metrics to {csv_file}\")\n",
        "\n",
        "\n",
        "  # Saving metrics\n",
        "  csv_file = f\"{save_directory}/gan_metrics_lr_{lr}_gp_{lambda_gp}_nl_{nl}_critic_{n_critic}_dim_{latent_dim}_sv_{sv}_train.csv\"\n",
        "  metrics = pd.DataFrame({\n",
        "            \"epoch\": list(range(len(dlosses))),\n",
        "            \"d_loss\": dlosses,\n",
        "            \"g_loss\": glosses\n",
        "        })\n",
        "  metrics.to_csv(csv_file, index=False)\n",
        "  print(f\"Saved metrics to {csv_file}\")\n",
        "\n",
        "  csv_file = f\"{save_directory}/gan_metrics_lr_{lr}_gp_{lambda_gp}_nl_{nl}_critic_{n_critic}_dim_{latent_dim}_sv_{sv}_test.csv\"\n",
        "  metrics = pd.DataFrame({\n",
        "            \"epoch\": list(range(len(d_test_losses))),\n",
        "            \"d_loss\": d_test_losses,\n",
        "            \"g_loss\": g_test_losses\n",
        "        })\n",
        "  metrics.to_csv(csv_file, index=False)\n",
        "  print(f\"Saved metrics to {csv_file}\")\n",
        "\n",
        "  torch.save(generator.state_dict(), \"/content/gdrive/My Drive/generator.pth\")\n",
        "  torch.save(discriminator.state_dict(), \"/content/gdrive/My Drive/discriminator.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = latent_dim + num_classes #Remnant of when we wanted to train with different latent_dims\n",
        "\n",
        "generator = Generator3(z_dim, style_dim, img_size).to(device)\n",
        "discriminator = Discriminator3().to(device)\n",
        "generator.load_state_dict(torch.load(\"/content/gdrive/My Drive/generator.pth\"))\n",
        "discriminator.load_state_dict(torch.load(\"/content/gdrive/My Drive/discriminator.pth\"))"
      ],
      "metadata": {
        "id": "CUKQZNwLz2Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_data = DataLoader(val_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "snPV2uapolSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveGridOfGeneratedImages(generator, z_s, condition_image, num_classes, output_file, condition_folder,j, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generates a grid of images for all classes and saves it as a single image file.\n",
        "    Also saves the condition image in a separate folder.\n",
        "    \"\"\"\n",
        "    # Ensure condition_image has the correct dimensions\n",
        "    if condition_image.ndim == 5:\n",
        "        condition_image = condition_image.squeeze(0)  # Remove unnecessary batch dimension\n",
        "    print(\"Condition Image Shape After Squeeze:\", condition_image.shape)\n",
        "\n",
        "    # Save the conditional image\n",
        "    condition_img_rescaled = np.uint8(np.interp(condition_image.cpu().numpy()[0, 0], (-1, 1), (0, 255)))\n",
        "    condition_file = os.path.join(condition_folder, f\"condition_image{j//26}.png\")\n",
        "    plt.imsave(condition_file, condition_img_rescaled, cmap='gray')\n",
        "    print(f\"Conditional image saved to {condition_file}\")\n",
        "\n",
        "    # Create a grid for displaying images\n",
        "    fig, axes = plt.subplots(2, 13, figsize=(20, 8))\n",
        "\n",
        "    for class_index in range(num_classes):\n",
        "        # Create the one-hot vector for the class\n",
        "        z_c = torch.zeros(1, num_classes, device=device)\n",
        "        z_c[0, class_index] = 1  # Set the desired class\n",
        "\n",
        "        # Concatenate the style and class vectors\n",
        "        z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "        # Generate the image\n",
        "        with torch.no_grad():\n",
        "            generated_img = generator(z, condition_image).cpu().numpy()[0, 0]  # Extract the first batch and first channel\n",
        "\n",
        "        # Rescale the image from [-1, 1] to [0, 255]\n",
        "        generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "        # Plot the image in the appropriate subplot\n",
        "        row = class_index // 13\n",
        "        col = class_index % 13\n",
        "        ax = axes[row, col]\n",
        "        ax.imshow(generated_img, cmap='gray')\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(chr(65 + class_index), fontsize=12)  # Display A-Z above images\n",
        "\n",
        "    # Adjust layout and save the figure as a single image\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Grid image saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Define output folders\n",
        "output_folder = \"/content/gdrive/My Drive/cGAN_final_fonts\"\n",
        "condition_folder = \"/content/gdrive/My Drive/cGAN_final_conditions\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "os.makedirs(condition_folder, exist_ok=True)\n",
        "\n",
        "# Example conditional image (e.g., the \"A\" image)\n",
        "for j, (real_imgs, condition_imgs, labels) in enumerate(display_data):\n",
        "            real_imgs = real_imgs.cuda()\n",
        "            condition_imgs = condition_imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            #batch_size = labels.size(0)\n",
        "            real_imgs = real_imgs.permute(0, 2, 3, 1).cuda()\n",
        "            condition_imgs = condition_imgs.permute(0, 2, 3, 1).cuda()\n",
        "            #condition_imgs = condition_imgs.cuda()\n",
        "            #plot_condition_and_real_images(condition_imgs,real_imgs, batch_index=j)\n",
        "\n",
        "\n",
        "            # Loop to generate and save 10 grids\n",
        "            #num_classes = 26  # A-Z\n",
        "            # Generate a new random style vector for each grid\n",
        "            #z_dim = 100  # Adjust based on your generator's style vector dimension\n",
        "            z_s = torch.randn(1, 100, device='cuda')\n",
        "\n",
        "            fig, axes = plt.subplots(2, 13, figsize=(20, 8))\n",
        "            #axes = np.expand_dims(axes, axis=1)  # Ensure axes is 2D\n",
        "\n",
        "            for class_index in range(num_classes):\n",
        "                # Create the one-hot vector for the class\n",
        "                z_c = torch.zeros(1, num_classes, device=device)\n",
        "                z_c[0, class_index] = 1  # Set the desired class\n",
        "\n",
        "                # Concatenate the style and class vectors\n",
        "                z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "                # Generate the image\n",
        "                with torch.no_grad():\n",
        "                    generated_img = generator(z, condition_imgs).cpu().numpy()[0, 0]  # Extract the first batch and first channel\n",
        "\n",
        "                # Rescale the image from [-1, 1] to [0, 255]\n",
        "                generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "                # Plot the image in the appropriate subplot\n",
        "                row = class_index // 13\n",
        "                col = class_index % 13\n",
        "                ax = axes[row, col]\n",
        "                ax.imshow(generated_img, cmap='gray')\n",
        "                ax.axis(\"off\")\n",
        "                ax.set_title(chr(65 + class_index), fontsize=12)  # Display A-Z above images\n",
        "\n",
        "            # Adjust layout and save the figure as a single image\n",
        "            plt.tight_layout()\n",
        "            #plt.savefig(output_file, bbox_inches='tight')\n",
        "            #plt.close(fig)\n",
        "\n",
        "            # Define the output file paths\n",
        "            saveGridOfGeneratedImages(generator=generator,z_s=z_s,condition_image=condition_imgs,num_classes=26,output_file=os.path.join(output_folder, f\"grid_{j%26,j//26}.png\"),condition_folder=condition_folder,j=j)\n",
        "print(\"All 10 grids and conditional images saved successfully!\")\n"
      ],
      "metadata": {
        "id": "D3v887XS9C_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVWGEYlOoI8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4k3J7xI9mbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMC77q/XUahQ9/KHxHw1LSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}