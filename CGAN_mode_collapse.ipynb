{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwarfy35/deep_learning2/blob/main/CGAN_mode_collapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd36ANKxZZuB",
        "outputId": "9259d411-1c2d-47cc-86ec-e26471323fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from torch.autograd import grad\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch.autograd import grad\n",
        "from collections import defaultdict\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5g6LYnHMxnzT"
      },
      "outputs": [],
      "source": [
        "ngf = 32\n",
        "nc = 1\n",
        "ndf = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "number of fonts is 14990"
      ],
      "metadata": {
        "id": "uHkxzK671iIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FontStyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size, style_dim):\n",
        "        super(FontStyleEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * (img_size // 4) ** 2, style_dim)  # Global style features\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)  # Returns a feature vector representing the font style\n"
      ],
      "metadata": {
        "id": "lpmgKHkRWdJN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fX2qIT2U26nz"
      },
      "outputs": [],
      "source": [
        "class Generator3(nn.Module):\n",
        "    def __init__(self, latent_dim, style_dim=128, img_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            latent_dim: Dimension of the latent vector (z).\n",
        "            style_dim: Dimension of the font style vector extracted by FontStyleEncoder.\n",
        "            img_size: Spatial size of the input image (assumed square).\n",
        "        \"\"\"\n",
        "        super(Generator3, self).__init__()\n",
        "\n",
        "        self.init_size = img_size // 16  # Initial spatial size after FC layer\n",
        "        self.fc = nn.Linear(latent_dim + style_dim, 512 * self.init_size * self.init_size)\n",
        "\n",
        "        # Deconvolutional blocks\n",
        "        self.deconv_blocks = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        # FontStyleEncoder to extract font style features\n",
        "        self.font_style_encoder = FontStyleEncoder(img_size, style_dim)\n",
        "\n",
        "    def forward(self, z, condition):\n",
        "        \"\"\"\n",
        "        Forward pass for the generator.\n",
        "\n",
        "        Args:\n",
        "            z: Latent vector (batch_size, latent_dim).\n",
        "            condition: Conditional image (batch_size, 1, img_size, img_size).\n",
        "\n",
        "        Returns:\n",
        "            Generated image (batch_size, 1, img_size, img_size).\n",
        "        \"\"\"\n",
        "        # Extract font style features from the condition image\n",
        "        font_style_features = self.font_style_encoder(condition)\n",
        "\n",
        "        # Concatenate latent vector with font style features\n",
        "        print(f\"Latent vector shape: {z.shape}\")\n",
        "        print(f\"Condition features shape: {font_style_features.shape}\")\n",
        "        z = torch.cat((z, font_style_features), dim=1)\n",
        "        print(f\"Combined latent vector shape: {z.shape}\")\n",
        "        # Fully connected layer\n",
        "        out = self.fc(z)\n",
        "        out = out.view(out.size(0), 512, self.init_size, self.init_size)\n",
        "\n",
        "        # Deconvolutional blocks\n",
        "        img = self.deconv_blocks(out)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator3(nn.Module):\n",
        "    def __init__(self, nc=1, ndf=32):\n",
        "\n",
        "        super(Discriminator3, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),  # Input channels = 1 (grayscale)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            #nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.main(img)\n",
        "        return validity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vsoPo3fnZyzQ"
      },
      "outputs": [],
      "source": [
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, npz_file, font_range=None, transform=None, filter_label=None, num_samples=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with optional filtering by font range.\n",
        "\n",
        "        Args:\n",
        "            npz_file (str): Path to the .npz file containing images and labels.\n",
        "            font_range (tuple, optional): Range of fonts to include (start, end). If None, include all fonts.\n",
        "            transform (callable, optional): Transformations to apply to the images.\n",
        "            filter_label (int, optional): Filter dataset by a specific label.\n",
        "            num_samples (int, optional): Limit the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        # Load the data from the .npz file\n",
        "        data = np.load(npz_file)\n",
        "        self.images = data['images']\n",
        "        self.labels = data['labels']\n",
        "        self.transform = transform\n",
        "\n",
        "        font_size = 26  # Number of letters per font (A-Z)\n",
        "\n",
        "        # Limit the dataset to the specified range of fonts\n",
        "        if font_range is not None:\n",
        "            start_font, end_font = font_range\n",
        "            start_idx = start_font * font_size\n",
        "            end_idx = end_font * font_size  # Exclusive of the last font\n",
        "            self.images = self.images[start_idx:end_idx]\n",
        "            self.labels = self.labels[start_idx:end_idx]\n",
        "\n",
        "        # Filter by label if specified\n",
        "        if filter_label is not None:\n",
        "            # Find indices of the desired label\n",
        "            label_indices = np.where(self.labels == filter_label)[0]\n",
        "\n",
        "            # If num_samples is specified, limit the number of samples\n",
        "            if num_samples is not None:\n",
        "                label_indices = label_indices[:num_samples]\n",
        "\n",
        "            # Filter images and labels\n",
        "            self.images = self.images[label_indices]\n",
        "            self.labels = self.labels[label_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve an item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Image, condition image, and label.\n",
        "        \"\"\"\n",
        "        # Get the image and label for the given index\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Determine the font ID based on dataset ordering\n",
        "        font_id = idx // 26  # Calculate the font ID based on position in dataset\n",
        "\n",
        "        # Get the \"A\" from the same font as the condition\n",
        "        condition_index = font_id * 26  # First letter (A) of the current font\n",
        "        condition_image = self.images[condition_index]\n",
        "\n",
        "        # Reshape the images\n",
        "        image = image[np.newaxis, ...]  # Add channel dimension\n",
        "        condition_image = condition_image[np.newaxis, ...]\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            condition_image = self.transform(condition_image)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        condition_image = torch.tensor(condition_image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, condition_image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "my7w8W_HgLLv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vd0Se_Ttoyh4"
      },
      "outputs": [],
      "source": [
        "def number_to_alphabet(num):\n",
        "    if 0 <= num <= 25:\n",
        "        return chr(num + 65)  # 65 is the ASCII code for 'A'\n",
        "    else:\n",
        "        raise ValueError(\"Number must be between 0 and 25 inclusive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-Qn__347eZJ4"
      },
      "outputs": [],
      "source": [
        "def displayGeneratedImage(class_index, generator, z_s, num_classes, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generates and displays an image for a given class using the generator.\n",
        "\n",
        "    Args:\n",
        "        class_index (int): Index of the character class to generate (0 to num_classes - 1).\n",
        "        generator (nn.Module): Pre-trained generator model.\n",
        "        z_dim (int): Dimension of the style vector.\n",
        "        num_classes (int): Number of character classes.\n",
        "        device (str): Device for computation ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Ensure the class index is valid\n",
        "    if not (0 <= class_index < num_classes):\n",
        "        raise ValueError(f\"Invalid class_index: {class_index}. Must be in range [0, {num_classes - 1}].\")\n",
        "\n",
        "    # Create the one-hot vector for the class\n",
        "    z_c = torch.zeros(1, num_classes, device=device)\n",
        "    z_c[0, class_index] = 1  # Set the desired class\n",
        "\n",
        "    # Create the random style vector\n",
        "    #z_s = torch.randn(1, z_dim, device=device)\n",
        "\n",
        "    # Concatenate the style and class vectors\n",
        "    z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "    # Generate the image\n",
        "    with torch.no_grad():\n",
        "        generated_img = generator(z).cpu().numpy()[0, 0]  # Extract the first batch and first channel\n",
        "\n",
        "    # Rescale the image from [-1, 1] to [0, 255]\n",
        "    #generated_img = np.rot90(generated_img, k=-1)\n",
        "    generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(generated_img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Generated Image for Class {number_to_alphabet(class_index)}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YxoHMB3AacDf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "#dataset = NPZDataset(npz_file, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "In_xt_E_5_J0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Compute the gradient penalty for WGAN-GP in a conditional GAN setup where\n",
        "    the discriminator takes the conditional input separately.\n",
        "\n",
        "    Args:\n",
        "        discriminator: The discriminator model.\n",
        "        real_samples: Batch of real images.\n",
        "        fake_samples: Batch of fake images generated by the generator.\n",
        "        condition_samples: Batch of conditional images (e.g., letter A from the same font).\n",
        "        device: The device to run the computations on.\n",
        "\n",
        "    Returns:\n",
        "        Gradient penalty (scalar).\n",
        "    \"\"\"\n",
        "    # Interpolate between real and fake samples\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    interpolated_samples = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "\n",
        "    # Pass interpolated samples and condition samples separately to the discriminator\n",
        "    d_interpolates = discriminator(interpolated_samples)  # Output shape: [batch_size, 1]\n",
        "\n",
        "    # Flatten the discriminator output\n",
        "    d_interpolates = d_interpolates.view(-1)  # Shape: [batch_size]\n",
        "\n",
        "    # Create grad_outputs for the gradient computation\n",
        "    fake = torch.ones(d_interpolates.size(), device=device)\n",
        "\n",
        "    # Compute gradients of the discriminator's output w.r.t. interpolated inputs\n",
        "    gradients = grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolated_samples,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "\n",
        "    # Compute gradient penalty\n",
        "    gradients = gradients.view(gradients.size(0), -1)  # Flatten gradients\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "    return gradient_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset_no_shuffle(full_data, test_size=0.1, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Split the dataset into train, validation, and test sets without shuffling.\n",
        "\n",
        "    Args:\n",
        "        full_data (Dataset): Full dataset to be split.\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        val_size (float): Proportion of the dataset to include in the validation split.\n",
        "\n",
        "    Returns:\n",
        "        train_data (Dataset): Training split.\n",
        "        val_data (Dataset): Validation split.\n",
        "        test_data (Dataset): Test split.\n",
        "    \"\"\"\n",
        "    total_size = len(full_data)\n",
        "    test_size_count = int(total_size * test_size)\n",
        "    val_size_count = int(total_size * val_size)\n",
        "    train_size_count = total_size - test_size_count - val_size_count\n",
        "\n",
        "    # Perform the split without shuffling (sequential split)\n",
        "    train_data = torch.utils.data.Subset(full_data, range(0, train_size_count))\n",
        "    val_data = torch.utils.data.Subset(full_data, range(train_size_count, train_size_count + val_size_count))\n",
        "    test_data = torch.utils.data.Subset(full_data, range(train_size_count + val_size_count, total_size))\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "# Example usage with your dataset (assuming `fullData` is already loaded as a PyTorch Dataset)\n"
      ],
      "metadata": {
        "id": "xjbNODRHvLJb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw5G-vGNmn7a",
        "outputId": "29451597-7e39-4c15-fefc-25bfea0d1255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "z_dim = 100\n",
        "num_classes = 26  # For uppercase alphabets\n",
        "img_size = 32  # Assuming 32x32 images\n",
        "batch_size = 1024\n",
        "print(batch_size)\n",
        "lr = 0.0002\n",
        "lambda_gp = 10  # Gradient penalty weight\n",
        "n_critic = 5  # Number of discriminator updates per generator update\n",
        "epochs = 100\n",
        "npz_file = \"/content/gdrive/My Drive/character_font.npz\"\n",
        "\n",
        "fullData = NPZDataset(npz_file, transform=transform)\n",
        "#train_data, val_data, test_data = split_dataset_no_shuffle(full_data=fullData, test_size=0.1, val_size=0.1)\n",
        "train_data = NPZDataset(npz_file, font_range=(0,100),transform=transform)\n",
        "\n",
        "# Print sizes to verify\n",
        "#print(f\"Train Data: {len(train_data)} samples\")\n",
        "#print(f\"Validation Data: {len(val_data)} samples\")\n",
        "#print(f\"Test Data: {len(test_data)} samples\")\n",
        "#print(len(fullData))\n",
        "\n",
        "#testing_data = NPZDataset(npz_file, transform=transform, num_fonts=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testloader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "CW7b3mxRXu4q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "YUQ2XA4Uh967"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 1: Group the data by label\n",
        "grouped_data = defaultdict(list)\n",
        "for img, condition_img, label in train_data:  # Unpack condition_img as well\n",
        "    grouped_data[label.item()].append((img, condition_img, label))\n",
        "\n",
        "# Step 2: Prepare datasets for each label\n",
        "grouped_datasets = {\n",
        "    label: [(img, cond_img, lbl) for img, cond_img, lbl in imgs]\n",
        "    for label, imgs in grouped_data.items()\n",
        "}\n",
        "\n",
        "# Step 3: Create dataloaders for each group\n",
        "grouped_dataloaders = {\n",
        "    letter: DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    for letter, dataset in grouped_datasets.items()\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipOhIMuYLyjn",
        "outputId": "014a43c3-07b4-4a34-802f-82ef6243f63e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-0151f720c1fe>:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image = torch.tensor(image, dtype=torch.float32)\n",
            "<ipython-input-29-0151f720c1fe>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  condition_image = torch.tensor(condition_image, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def displayGeneratorOutput(generator, z_s, condition_image, class_index, num_classes, device='cuda'):\n",
        "    \"\"\"\n",
        "    Displays the output of the generator for a single class.\n",
        "\n",
        "    Args:\n",
        "        generator (nn.Module): Pre-trained generator model.\n",
        "        z_s (torch.Tensor): Style vector.\n",
        "        condition_image (torch.Tensor): Conditional image (e.g., \"A\").\n",
        "        class_index (int): Index of the class to generate (0-25 for A-Z).\n",
        "        num_classes (int): Total number of character classes.\n",
        "        device (str): Device for computation ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Ensure the inputs are on the correct device\n",
        "    z_s = z_s.to(device)\n",
        "    condition_image = condition_image.to(device)\n",
        "\n",
        "    # Create the one-hot vector for the class\n",
        "    z_c = torch.zeros(batch_size, num_classes, device=device)\n",
        "    z_c[0, class_index] = 1  # Set the desired class\n",
        "\n",
        "    # Concatenate the style and class vectors\n",
        "    z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "    # Generate the image\n",
        "    with torch.no_grad():\n",
        "        generated_img = generator(z, condition_image.unsqueeze(0)).cpu().numpy()[0, 0]\n",
        "\n",
        "    # Rescale the image from [-1, 1] to [0, 255]\n",
        "    generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "    condition_img = np.uint8(np.interp(condition_img, (-1, 1), (0, 255)))\n",
        "    # Display the generated image\n",
        "    plt.imshow(generated_img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Generated Image for Class {chr(65 + class_index)}\")  # Convert index to letter\n",
        "    plt.show()\n",
        "    plt.imshow(condition_img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Condition\")  # Convert index to letter\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "4OCt7taLIoda"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def displayGeneratedSample(generator, z_s, condition_imgs, labels, num_classes, class_index=0, device='cuda'):\n",
        "    \"\"\"\n",
        "    Displays one generated image from the generator.\n",
        "\n",
        "    Args:\n",
        "        generator (nn.Module): Pre-trained generator model.\n",
        "        z_s (torch.Tensor): Style vector.\n",
        "        condition_imgs (torch.Tensor): Conditional images (batch).\n",
        "        labels (torch.Tensor): Labels for the batch.\n",
        "        num_classes (int): Number of character classes.\n",
        "        class_index (int): Index of the sample in the batch to display.\n",
        "        device (str): Device for computation ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Ensure the inputs are on the correct device\n",
        "    z_s = z_s.to(device)\n",
        "    condition_imgs = condition_imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Create one-hot encoding for the batch\n",
        "    batch_size = labels.size(0)\n",
        "    z_c = torch.zeros(batch_size, num_classes, device=device)\n",
        "    z_c[torch.arange(batch_size), labels] = 1\n",
        "\n",
        "    # Concatenate style and class vectors\n",
        "    z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        fake_imgs = generator(z, condition_imgs)\n",
        "\n",
        "    # Select the specified class index\n",
        "    generated_img = fake_imgs[class_index].cpu().numpy()[0]  # Extract the first channel\n",
        "\n",
        "    # Rescale the image from [-1, 1] to [0, 255]\n",
        "    generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.imshow(generated_img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Generated Image for Class {chr(65 + labels[class_index].item())}\")  # Convert label to letter\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "C31T6p2jMYR4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8zCRv08W0YF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77zqy5GAZ7hG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Iso39Z2BtcQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZfqBaqlzZn2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "4efe4113-0de8-400d-f348-5c31191c69b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n",
            "torch.Size([100, 32, 1, 32])\n",
            "torch.Size([100, 1, 32, 32])\n",
            "condition shape: torch.Size([100, 1, 32, 32])\n",
            "z shape: torch.Size([100, 126])\n",
            "condition shape: torch.Size([102400, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 100 but got size 102400 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-223a3aba820c>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_c\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mzc_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Combine latent vector and style features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition_imgs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;31m#encoder_layer = generator.font_style_encoder.encoder[1]  # Example: Hook on the first ReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#encoder_layer.register_forward_hook(print_tensor(\"FontStyleEncoder Feature Map\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-daaa7a1fc722>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, condition)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"z shape: {z.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected: (batch_size, latent_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"condition shape: {condition.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This should match (batch_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mz_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Concatenate latent vector and condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to start deconvolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Deconvolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 100 but got size 102400 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize models\n",
        "latent_dim = 100  # Dimension of the latent vector z\n",
        "style_dim = 64  # Dimension of the font style vector\n",
        "img_size = 32  # Image size\n",
        "num_classes = 26  # Number of character classes (A-Z)\n",
        "z_dim = 126\n",
        "epochs = 1000\n",
        "n_crtic = 5\n",
        "zc_weight =  1\n",
        "print(batch_size)\n",
        "\n",
        "generator = Generator3(z_dim, style_dim, img_size).to(device)\n",
        "discriminator = Discriminator3(nc=1, ndf=32).to(device)\n",
        "encoder = FontStyleEncoder(img_size, style_dim).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    for letter in range(num_classes):\n",
        "        dataloader = grouped_dataloaders[letter]\n",
        "        for i, (real_imgs, condition_imgs, labels) in enumerate(dataloader):\n",
        "            real_imgs = real_imgs.to(device)\n",
        "            condition_imgs = condition_imgs.to(device)\n",
        "            print(condition_imgs.shape)\n",
        "\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "\n",
        "            real_imgs = real_imgs.permute(0, 2, 3, 1).cuda()\n",
        "\n",
        "            condition_imgs = condition_imgs.permute(0, 2, 3, 1).cuda()\n",
        "\n",
        "\n",
        "\n",
        "            # Extract font style features from conditional images\n",
        "            #font_style_features = encoder(condition_imgs) this is now done in the generator\n",
        "\n",
        "            # Train Discriminator\n",
        "            z_s = torch.randn(batch_size, latent_dim).to(device)  # Latent vector\n",
        "            z_c = torch.zeros(batch_size, num_classes).to(device)  # One-hot class vector\n",
        "            z_c[torch.arange(batch_size), labels] = 1\n",
        "            z = torch.cat((z_s, z_c * zc_weight), dim=1)  # Combine latent vector and one hot\n",
        "            fake_imgs = generator(z, condition_imgs)  # Generate fake images\n",
        "\n",
        "\n",
        "            real_validity = discriminator(real_imgs)\n",
        "            fake_validity = discriminator(fake_imgs.detach())\n",
        "            gradient_penalty = compute_gradient_penalty(\n",
        "                discriminator,\n",
        "                real_samples=real_imgs,\n",
        "                fake_samples=fake_imgs,\n",
        "                device=device\n",
        "            )\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train Generator\n",
        "            if i % n_critic == 0:\n",
        "                z_s = torch.randn(batch_size, latent_dim).to(device)\n",
        "                z = torch.cat((z_s, z_c), dim=1)\n",
        "                fake_imgs = generator(z, condition_imgs)\n",
        "                fake_validity = discriminator(fake_imgs)\n",
        "                g_loss = -torch.mean(fake_validity)\n",
        "\n",
        "                optimizer_G.zero_grad()\n",
        "                g_loss.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "            #Display generated sample periodically\n",
        "            if epoch % 10 == 0 and i == 0:\n",
        "                displayGeneratedSample(generator, z_s, condition_imgs, labels, num_classes, class_index=0, device=device)\n",
        "\n",
        "    # Print training progress\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save models periodically\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(generator.state_dict(), \"/content/gdrive/My Drive/generator.pth\")\n",
        "        torch.save(discriminator.state_dict(), \"/content/gdrive/My Drive/discriminator.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_data = DataLoader(testing_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "snPV2uapolSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1RIaotBgs1P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_s = torch.randn(1, z_dim, device='cuda')\n",
        "z_c = torch.zeros(1, num_classes, device='cuda')\n",
        "z_c[0, 0] = 1  # Class index 0 (A)\n",
        "condition_image = torch.randn(1, 1, 32, 32, device='cuda')  # Random condition\n",
        "z = torch.cat((z_s, z_c), dim=1)\n",
        "with torch.no_grad():\n",
        "    output = generator(z, condition_image)\n",
        "    print(\"Generated image range:\", output.min().item(), output.max().item())\n",
        "    plt.imshow(output.cpu().numpy()[0, 0], cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Pa5a3ZhhtrWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_condition_and_real_images(condition_imgs, real_imgs, batch_index):\n",
        "    \"\"\"\n",
        "    Plot all condition images alongside their corresponding real images in the batch.\n",
        "\n",
        "    Args:\n",
        "        condition_imgs (torch.Tensor): Batch of condition images (B, C, H, W).\n",
        "        real_imgs (torch.Tensor): Batch of real images (B, C, H, W).\n",
        "        batch_index (int): Index of the current batch.\n",
        "    \"\"\"\n",
        "    condition_imgs = condition_imgs.cpu().numpy()  # Convert to NumPy for plotting\n",
        "    real_imgs = real_imgs.cpu().numpy()  # Convert to NumPy for plotting\n",
        "\n",
        "    # Determine batch size\n",
        "    batch_size = condition_imgs.shape[0]\n",
        "\n",
        "    # Create a figure for all condition-real image pairs\n",
        "    fig, axes = plt.subplots(2, batch_size, figsize=(15, 5))\n",
        "    fig.suptitle(f\"Condition and Real Images for Batch {batch_index}\", fontsize=16)\n",
        "\n",
        "    # Handle the case where batch_size == 1\n",
        "    if batch_size == 1:\n",
        "        axes = np.expand_dims(axes, axis=1)  # Ensure axes is 2D\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Rescale the condition image from [-1, 1] to [0, 1]\n",
        "        condition_img = (condition_imgs[i, 0] + 1) / 2  # Assuming grayscale\n",
        "        real_img = (real_imgs[i, 0] + 1) / 2  # Rescale the real image as well\n",
        "\n",
        "        # Plot condition image\n",
        "        axes[0, i].imshow(condition_img, cmap='gray')\n",
        "        axes[0, i].set_title(f\"Condition Image {i}\", fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Plot real image\n",
        "        axes[1, i].imshow(real_img, cmap='gray')\n",
        "        axes[1, i].set_title(f\"Real Image {i}\", fontsize=10)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eNeK5Az4sUs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def saveGridOfGeneratedImages(generator, z_s, condition_image, num_classes, output_file, condition_folder,j, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generates a grid of images for all classes and saves it as a single image file.\n",
        "    Also saves the condition image in a separate folder.\n",
        "    \"\"\"\n",
        "    # Ensure condition_image has the correct dimensions\n",
        "    if condition_image.ndim == 5:\n",
        "        condition_image = condition_image.squeeze(0)  # Remove unnecessary batch dimension\n",
        "    print(\"Condition Image Shape After Squeeze:\", condition_image.shape)\n",
        "\n",
        "    # Save the conditional image\n",
        "    condition_img_rescaled = np.uint8(np.interp(condition_image.cpu().numpy()[0, 0], (-1, 1), (0, 255)))\n",
        "    condition_file = os.path.join(condition_folder, f\"condition_image{j//26}.png\")\n",
        "    plt.imsave(condition_file, condition_img_rescaled, cmap='gray')\n",
        "    print(f\"Conditional image saved to {condition_file}\")\n",
        "\n",
        "    # Create a grid for displaying images\n",
        "    fig, axes = plt.subplots(2, 13, figsize=(20, 8))\n",
        "\n",
        "    for class_index in range(num_classes):\n",
        "        # Create the one-hot vector for the class\n",
        "        z_c = torch.zeros(1, num_classes, device=device)\n",
        "        z_c[0, class_index] = 1  # Set the desired class\n",
        "\n",
        "        # Concatenate the style and class vectors\n",
        "        z = torch.cat((z_s, z_c), dim=1)\n",
        "\n",
        "        # Generate the image\n",
        "        with torch.no_grad():\n",
        "            generated_img = generator(z, condition_image).cpu().numpy()[0, 0]  # Extract the first batch and first channel\n",
        "\n",
        "        # Rescale the image from [-1, 1] to [0, 255]\n",
        "        generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
        "\n",
        "        # Plot the image in the appropriate subplot\n",
        "        row = class_index // 13\n",
        "        col = class_index % 13\n",
        "        ax = axes[row, col]\n",
        "        ax.imshow(generated_img, cmap='gray')\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(chr(65 + class_index), fontsize=12)  # Display A-Z above images\n",
        "\n",
        "    # Adjust layout and save the figure as a single image\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Grid image saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Define output folders\n",
        "output_folder = \"/content/gdrive/My Drive/cGAN_overfit_working\"\n",
        "condition_folder = \"/content/gdrive/My Drive/cGAN_conditions_working\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "os.makedirs(condition_folder, exist_ok=True)\n",
        "\n",
        "# Example conditional image (e.g., the \"A\" image)\n",
        "for j, (real_imgs, condition_imgs, labels) in enumerate(display_data):\n",
        "            real_imgs = real_imgs.cuda()\n",
        "            condition_imgs = condition_imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            #batch_size = labels.size(0)\n",
        "            real_imgs = real_imgs.permute(0, 2, 3, 1).cuda()\n",
        "            condition_imgs = condition_imgs.permute(0, 2, 3, 1).cuda()\n",
        "            #condition_imgs = condition_imgs.cuda()\n",
        "            plot_condition_and_real_images(condition_imgs,real_imgs, batch_index=j)\n",
        "\n",
        "\n",
        "            # Loop to generate and save 10 grids\n",
        "            #num_classes = 26  # A-Z\n",
        "            # Generate a new random style vector for each grid\n",
        "            #z_dim = 100  # Adjust based on your generator's style vector dimension\n",
        "            z_s = torch.randn(1, 100, device='cuda')\n",
        "\n",
        "            # Define the output file paths\n",
        "            saveGridOfGeneratedImages(generator=generator,z_s=z_s,condition_image=condition_imgs,num_classes=26,output_file=os.path.join(output_folder, f\"grid_{j%26,j//26}.png\"),condition_folder=condition_folder,j=j)\n",
        "print(\"All 10 grids and conditional images saved successfully!\")\n"
      ],
      "metadata": {
        "id": "D3v887XS9C_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZRAOfbP-dTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4k3J7xI9mbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNIeTxIux4F6rbbK6J323Nd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}