{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+ktTCRYueUeqDvs2ZV+du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwarfy35/deep_learning2/blob/main/ColabNoteBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EEzHZduUqzaM"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinibatchDiscrimination(nn.Module):\n",
        "    def __init__(self, in_features, out_features, intermediate_features):\n",
        "        super().__init__()\n",
        "        self.T = nn.Parameter(torch.randn(in_features, intermediate_features))\n",
        "    def forward(self, x):\n",
        "        M = x.mm(self.T)\n",
        "        M = M.unsqueeze(0) - M.unsqueeze(1)\n",
        "        M = torch.exp(-torch.abs(M).sum(dim=2))\n",
        "        return torch.cat([x, M.mean(dim=0)], dim=1)\n"
      ],
      "metadata": {
        "id": "y46coE27ThDa"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3u6exNATqv1",
        "outputId": "8401f14c-6687-41e9-d4ae-0523838ce490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "nz = 126\n",
        "# Size of feature maps in generator\n",
        "ngf = 32\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 32\n",
        "\n",
        "nc = 1\n",
        "\n",
        "ngpu = 1\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "num_classes = 26\n",
        "num_epochs = 100  # Set the number of training epochs\n",
        "M = 32  # Batch size\n",
        "N_disc = 1 # Number of discriminator iterations per generator iteration\n",
        "lambda_coeff = 10  # Coefficient Î» for the gradient penalty\n",
        "alpha, beta1, beta2 = 0.0002, 0.5 , 0.999  # Adam hyperparameters\n",
        "\n",
        "# Initialize model parameters\n",
        "w = torch.randn((1,), requires_grad=True)  # Discriminator parameters\n",
        "theta = torch.randn((1,), requires_grad=True)  # Generator parameters\n",
        "\n",
        "# Optimizers\n",
        "\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Placeholder functions for model components\n",
        "def sample_real_data(class_label):\n",
        "    return torch.randn(32, 32)  # Dummy 32x32 image\n",
        "\n",
        "def sample_style_vector(batch_size = 1):\n",
        "    return torch.FloatTensor(batch_size, 100).uniform_(-1, 1)\n",
        "\n",
        "class NPZDataset(Dataset):\n",
        "    def __init__(self, npz_file, transform=None, filter_label=None, num_samples=None):\n",
        "        # Load the data from the .npz file\n",
        "        data = np.load(npz_file)\n",
        "        self.images = data['images']\n",
        "        self.labels = data['labels']\n",
        "        self.transform = transform\n",
        "\n",
        "        # Filter by label if specified\n",
        "        if filter_label is not None:\n",
        "            # Find indices of the desired label\n",
        "            label_indices = np.where(self.labels == filter_label)[0]\n",
        "\n",
        "            # If num_samples is specified, limit the number of samples\n",
        "            if num_samples is not None:\n",
        "                label_indices = label_indices[:num_samples]\n",
        "\n",
        "            # Filter images and labels\n",
        "            self.images = self.images[label_indices]\n",
        "            self.labels = self.labels[label_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image and label for the given index\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Reshape the image to add a channel dimension\n",
        "        image = image[np.newaxis, ...]  # Add channel dimension at the beginning\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert to PyTorch tensor if necessary\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "            #nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            #nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            #nn.Sigmoid()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            #nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            #nn.BatchNorm2d(ndf * 8),\n",
        "            #nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            #nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            #nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "def gradient_penalty(discriminator, real_data, fake_data):\n",
        "    alpha = torch.rand(real_data.size(0), 1, 1, 1, device=real_data.device)\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    interpolates.requires_grad_(True)\n",
        "    d_interpolates = discriminator(interpolates)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients_norm = gradients.view(gradients.size(0), -1).norm(2, dim=1)\n",
        "    penalty = ((gradients_norm - 1) ** 2).mean()\n",
        "    #print(f\"Gradient Norm: {gradients_norm.mean().item()}, Penalty: {penalty.item()}\")\n",
        "\n",
        "    return penalty\n",
        "\n",
        "discriminator = Discriminator(ngpu)\n",
        "discriminator.apply(weights_init)\n",
        "generator = Generator(ngpu)\n",
        "generator.apply(weights_init)\n",
        "npz_file = '/content/gdrive/My Drive/character_font.npz'  # Path to your npz file\n",
        "data_test = np.load(npz_file)\n",
        "data_A_s = []\n",
        "#for i in range(100):\n",
        "  #data_A_s.append(data_test['images'][i*26])\n",
        "  #print(data_test['labels'][i*26])\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        # Load the data from the .npz file\n",
        "        self.images = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image and label for the given index\n",
        "        image = self.images[idx]\n",
        "\n",
        "        # Reshape the image to add a channel dimension\n",
        "        # Assuming images are grayscale, add a channel dimension of 1\n",
        "        image = image[np.newaxis, ...]  # Add channel dimension at the beginning\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert to PyTorch tensor if necessary\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "        return image\n",
        "\n",
        "transform = None  # Define any torchvision transforms you want to apply\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "#dataset = NPZDataset(npz_file, transform=transform)\n",
        "dataset = NPZDataset(npz_file, transform, 0, 10000)\n",
        "dataloader = DataLoader(dataset, batch_size=M, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def genTestImg():\n",
        "    z_c = torch.zeros(num_classes)\n",
        "    z_c[c] = 1\n",
        "    z_s = sample_style_vector(1)\n",
        "    #x = torch.tensor(np.random.normal(0, 1, (1, 1, 32, 32)), dtype=torch.float32)\n",
        "    z = torch.cat((z_s, z_c.repeat(1, 1)), dim=1)\n",
        "    z = z.view(1, 126, 1, 1)\n",
        "    img = generator(z)\n",
        "    img = img.detach().numpy()[0, 0]  # Get the single channel for greyscale\n",
        "    #img = np.uint8(np.interp(img, (-1, 1), (0, 255)))  # Scale values for display\n",
        "    plt.imshow(img, cmap='gray')  # Display in greyscale\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WtYub85iqulb"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def genTestImg(c):\n",
        "    z_c = torch.zeros(1, num_classes, device=device)\n",
        "    z_c[c] = 1\n",
        "    z_s = sample_style_vector(1).to(device)\n",
        "    z = torch.cat((z_s, z_c), dim=1).view(1, 126, 1, 1).to(device)\n",
        "    img = generator(z).detach().cpu().numpy()[0, 0]\n",
        "    img = np.uint8(np.interp(img, (-1, 1), (0, 255)))  # Scale to [0, 255]\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IGfk4Aa1B2Bm"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_style_vector(batch_size = 1):\n",
        "    return torch.FloatTensor(batch_size, 100).uniform_(-1, 1)"
      ],
      "metadata": {
        "id": "TlFCHei33cv4"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=alpha, betas=(beta1, beta2))\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=alpha, betas=(beta1, beta2))"
      ],
      "metadata": {
        "id": "6C843KPD-zNs"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asPRdK-anjp6"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "discriminator = Discriminator(ngpu)\n",
        "discriminator.apply(weights_init)\n",
        "generator = Generator(ngpu)\n",
        "generator.apply(weights_init)\n",
        "\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=alpha/2, betas=(beta1, beta2))\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=alpha, betas=(beta1, beta2))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for c in range(1):\n",
        "        # Set one-hot vector for current class\n",
        "        z_c = torch.zeros(num_classes, device=device)\n",
        "        z_c[c] = 1\n",
        "\n",
        "        # Discriminator update\n",
        "        for _ in range(N_disc):\n",
        "            # Initialize discriminator loss\n",
        "            D_loss = 0\n",
        "\n",
        "            for i, (image, label) in enumerate(dataloader):\n",
        "                # Move real images to device\n",
        "                x_real = image.to(device)\n",
        "                x_real = x_real.view(x_real.size()[0], 1, 32, 32)\n",
        "\n",
        "                # Sample style vector and move to device\n",
        "                z_s = sample_style_vector(x_real.size()[0]).to(device)\n",
        "\n",
        "                # Generate fake data\n",
        "                z = torch.cat((z_s, z_c.repeat(x_real.size()[0], 1)), dim=1).to(device)\n",
        "                z = z.view(x_real.size()[0], 126, 1, 1)\n",
        "\n",
        "                x_fake = generator(z)\n",
        "\n",
        "                # Discriminator loss calculation\n",
        "                x_fake = x_fake.view(x_real.size()[0], 1, 32, 32)\n",
        "\n",
        "                # Add noise to real and fake data\n",
        "                real_with_noise = x_real + 0.1 * torch.randn_like(x_real)\n",
        "                fake_with_noise = x_fake + 0.1 * torch.randn_like(x_fake)\n",
        "\n",
        "                D_real = discriminator(real_with_noise)\n",
        "                D_fake = discriminator(fake_with_noise)\n",
        "                print(f\"D_real: {torch.mean(D_real).item()}, D_fake: {torch.mean(D_fake).item()}\")\n",
        "\n",
        "                grad_penalty = 2 * lambda_coeff * gradient_penalty(discriminator, real_with_noise, fake_with_noise)\n",
        "\n",
        "                print(f\"Gradient Penalty: {grad_penalty.item()}\")\n",
        "\n",
        "                D_loss += -torch.mean(D_real) + torch.mean(D_fake) + grad_penalty\n",
        "\n",
        "            D_loss /= len(dataloader)\n",
        "\n",
        "            # Update discriminator weights\n",
        "            optimizer_D.zero_grad()\n",
        "            D_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        # Generator update\n",
        "        for _ in range(5):\n",
        "          G_loss = 0\n",
        "          z_s = sample_style_vector(1).to(device)\n",
        "          for _ in range(M):\n",
        "              # Sample style vector and move to device\n",
        "              #z_s = sample_style_vector(1).to(device)\n",
        "              z = torch.cat((z_s, z_c.repeat(1, 1)), dim=1).to(device)\n",
        "              z = z.view(1, 126, 1, 1)\n",
        "\n",
        "              # Generate fake data and calculate generator loss\n",
        "              x_fake = generator(z)\n",
        "              x_fake = x_fake.view(1, 1, 32, 32)\n",
        "              G_loss += -torch.mean(discriminator(x_fake))\n",
        "\n",
        "          G_loss /= M\n",
        "\n",
        "        # Update generator weights\n",
        "        optimizer_G.zero_grad()\n",
        "        G_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Generate test image\n",
        "        genTestImg(c)\n",
        "    for name, param in generator.named_parameters():\n",
        "      print(f\"{name} - Grad Norm: {param.grad.norm()}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - D Loss: {D_loss.item():.4f}, G Loss: {G_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ed_TTNagGdNn",
        "outputId": "595df599-2329-47a3-e006-bea3b9dbc116"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-169-5ca522be126f>:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image = torch.tensor(image, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_real: 0.00251381890848279, D_fake: 0.0004097787896171212\n",
            "Gradient Penalty: 19.734922409057617\n",
            "D_real: 0.003702374640852213, D_fake: 0.00031397404382005334\n",
            "Gradient Penalty: 19.73600959777832\n",
            "D_real: 0.003700845642015338, D_fake: 0.0004695158277172595\n",
            "Gradient Penalty: 19.73736000061035\n",
            "D_real: 0.0038025116082280874, D_fake: 0.00032544368878006935\n",
            "Gradient Penalty: 19.734277725219727\n",
            "D_real: 0.0025866744108498096, D_fake: 0.00029810937121510506\n",
            "Gradient Penalty: 19.733234405517578\n",
            "D_real: 0.0029332423582673073, D_fake: 0.00031876974389888346\n",
            "Gradient Penalty: 19.735252380371094\n",
            "D_real: 0.0024134707637131214, D_fake: 0.0002126289182342589\n",
            "Gradient Penalty: 19.735139846801758\n",
            "D_real: 0.0022819354198873043, D_fake: 0.00030386721482500434\n",
            "Gradient Penalty: 19.731855392456055\n",
            "D_real: 0.0029876274056732655, D_fake: 0.00041075467015616596\n",
            "Gradient Penalty: 19.7335147857666\n",
            "D_real: 0.003369102254509926, D_fake: 0.00034698302624747157\n",
            "Gradient Penalty: 19.7379150390625\n",
            "D_real: 0.0031889243982732296, D_fake: 0.000433299079304561\n",
            "Gradient Penalty: 19.736520767211914\n",
            "D_real: 0.00403692526742816, D_fake: 0.00046317087253555655\n",
            "Gradient Penalty: 19.736257553100586\n",
            "D_real: 0.0031297700479626656, D_fake: 0.0003543146885931492\n",
            "Gradient Penalty: 19.738759994506836\n",
            "D_real: 0.0036996249109506607, D_fake: 0.00030799105297774076\n",
            "Gradient Penalty: 19.740310668945312\n",
            "D_real: 0.0031756809912621975, D_fake: 0.0003624435339588672\n",
            "Gradient Penalty: 19.733732223510742\n",
            "D_real: 0.004105951637029648, D_fake: 0.0002815135521814227\n",
            "Gradient Penalty: 19.73716926574707\n",
            "D_real: 0.0031386995688080788, D_fake: 0.00020585149468388408\n",
            "Gradient Penalty: 19.73556900024414\n",
            "D_real: 0.002682633697986603, D_fake: 0.00027101082378067076\n",
            "Gradient Penalty: 19.74321937561035\n",
            "D_real: 0.002974452218040824, D_fake: 0.0002926102024503052\n",
            "Gradient Penalty: 19.734390258789062\n",
            "D_real: 0.004289912059903145, D_fake: 0.0002872913610190153\n",
            "Gradient Penalty: 19.737060546875\n",
            "D_real: 0.003021392971277237, D_fake: 0.000385093386285007\n",
            "Gradient Penalty: 19.73966407775879\n",
            "D_real: 0.0024732842575758696, D_fake: 0.00028001118334941566\n",
            "Gradient Penalty: 19.734130859375\n",
            "D_real: 0.00280061736702919, D_fake: 0.00015725987032055855\n",
            "Gradient Penalty: 19.73678207397461\n",
            "D_real: 0.003998219966888428, D_fake: 0.00027024277369491756\n",
            "Gradient Penalty: 19.73451805114746\n",
            "D_real: 0.0025928220711648464, D_fake: 0.0003233259485568851\n",
            "Gradient Penalty: 19.73334503173828\n",
            "D_real: 0.0031324909068644047, D_fake: 0.0003134327125735581\n",
            "Gradient Penalty: 19.73628807067871\n",
            "D_real: 0.0030572153627872467, D_fake: 0.00021632864081766456\n",
            "Gradient Penalty: 19.737146377563477\n",
            "D_real: 0.00294858543202281, D_fake: 0.0002881024847738445\n",
            "Gradient Penalty: 19.736196517944336\n",
            "D_real: 0.0032904045656323433, D_fake: 0.00030422324198298156\n",
            "Gradient Penalty: 19.7388858795166\n",
            "D_real: 0.0028605766128748655, D_fake: 0.00015798227104824036\n",
            "Gradient Penalty: 19.738792419433594\n",
            "D_real: 0.003178514540195465, D_fake: 0.0001670306664891541\n",
            "Gradient Penalty: 19.737735748291016\n",
            "D_real: 0.002306499518454075, D_fake: 0.00031035771826282144\n",
            "Gradient Penalty: 19.73660659790039\n",
            "D_real: 0.002950563794001937, D_fake: 0.0005250690155662596\n",
            "Gradient Penalty: 19.734996795654297\n",
            "D_real: 0.0027571371756494045, D_fake: 0.0003652612795121968\n",
            "Gradient Penalty: 19.739891052246094\n",
            "D_real: 0.003274774644523859, D_fake: 0.0004007858515251428\n",
            "Gradient Penalty: 19.737218856811523\n",
            "D_real: 0.0025128095876425505, D_fake: 0.0002752383297774941\n",
            "Gradient Penalty: 19.738433837890625\n",
            "D_real: 0.0037543093785643578, D_fake: 0.00033787693246267736\n",
            "Gradient Penalty: 19.731977462768555\n",
            "D_real: 0.0030267974361777306, D_fake: 0.00027499301359057426\n",
            "Gradient Penalty: 19.73562240600586\n",
            "D_real: 0.0031318888068199158, D_fake: 0.0003559112665243447\n",
            "Gradient Penalty: 19.735994338989258\n",
            "D_real: 0.0032231826335191727, D_fake: 0.00021854026999790221\n",
            "Gradient Penalty: 19.7403621673584\n",
            "D_real: 0.0038634249940514565, D_fake: 0.00016361084999516606\n",
            "Gradient Penalty: 19.734535217285156\n",
            "D_real: 0.0031831201631575823, D_fake: 0.0002833782054949552\n",
            "Gradient Penalty: 19.731096267700195\n",
            "D_real: 0.0033346503041684628, D_fake: 0.0003037035639863461\n",
            "Gradient Penalty: 19.73627281188965\n",
            "D_real: 0.0037889289669692516, D_fake: 0.00023368929396383464\n",
            "Gradient Penalty: 19.732418060302734\n",
            "D_real: 0.003387412056326866, D_fake: 0.00037201133091002703\n",
            "Gradient Penalty: 19.740192413330078\n",
            "D_real: 0.003544196020811796, D_fake: 0.00037492174305953085\n",
            "Gradient Penalty: 19.737926483154297\n",
            "D_real: 0.0031631174497306347, D_fake: 0.00027664113440550864\n",
            "Gradient Penalty: 19.73699378967285\n",
            "D_real: 0.0037018409930169582, D_fake: 0.00024320906959474087\n",
            "Gradient Penalty: 19.740360260009766\n",
            "D_real: 0.004496597684919834, D_fake: 0.0002326971007278189\n",
            "Gradient Penalty: 19.738136291503906\n",
            "D_real: 0.0037923569325357676, D_fake: 0.0003442986635491252\n",
            "Gradient Penalty: 19.742084503173828\n",
            "D_real: 0.0024845509324222803, D_fake: 0.00041892268927767873\n",
            "Gradient Penalty: 19.735713958740234\n",
            "D_real: 0.002938732272014022, D_fake: 0.00019775555119849741\n",
            "Gradient Penalty: 19.737415313720703\n",
            "D_real: 0.00329603417776525, D_fake: 0.0003877985291182995\n",
            "Gradient Penalty: 19.73166275024414\n",
            "D_real: 0.002898856997489929, D_fake: 0.00031983116059564054\n",
            "Gradient Penalty: 19.735166549682617\n",
            "D_real: 0.0021729092113673687, D_fake: 0.00033924600575119257\n",
            "Gradient Penalty: 19.7362117767334\n",
            "D_real: 0.002805075841024518, D_fake: 0.0003481424937490374\n",
            "Gradient Penalty: 19.732145309448242\n",
            "D_real: 0.003707495518028736, D_fake: 0.00016439966566395015\n",
            "Gradient Penalty: 19.73332405090332\n",
            "D_real: 0.004061782266944647, D_fake: 0.00019087395048700273\n",
            "Gradient Penalty: 19.734209060668945\n",
            "D_real: 0.003275288501754403, D_fake: 0.00032041058875620365\n",
            "Gradient Penalty: 19.737102508544922\n",
            "D_real: 0.0035935332998633385, D_fake: 0.00019138363131787628\n",
            "Gradient Penalty: 19.736915588378906\n",
            "D_real: 0.002316660713404417, D_fake: 0.0003103147028014064\n",
            "Gradient Penalty: 19.735950469970703\n",
            "D_real: 0.002124406863003969, D_fake: 0.00034986011451110244\n",
            "Gradient Penalty: 19.73688507080078\n",
            "D_real: 0.002686928492039442, D_fake: 0.00012159845209680498\n",
            "Gradient Penalty: 19.73636817932129\n",
            "D_real: 0.0032925219275057316, D_fake: 0.00032065773848444223\n",
            "Gradient Penalty: 19.73657989501953\n",
            "D_real: 0.0033490462228655815, D_fake: 0.00022450857795774937\n",
            "Gradient Penalty: 19.73822784423828\n",
            "D_real: 0.0039715697057545185, D_fake: 0.0002766102843452245\n",
            "Gradient Penalty: 19.73509979248047\n",
            "D_real: 0.0034219424705952406, D_fake: 0.00041594120557419956\n",
            "Gradient Penalty: 19.736705780029297\n",
            "D_real: 0.003086184849962592, D_fake: 0.00031113612931221724\n",
            "Gradient Penalty: 19.7365779876709\n",
            "D_real: 0.0032773546408861876, D_fake: 0.0003617735637817532\n",
            "Gradient Penalty: 19.73578453063965\n",
            "D_real: 0.00270767230540514, D_fake: 0.00041138596134260297\n",
            "Gradient Penalty: 19.735239028930664\n",
            "D_real: 0.0023457705974578857, D_fake: 0.00030755007173866034\n",
            "Gradient Penalty: 19.732986450195312\n",
            "D_real: 0.0028104218654334545, D_fake: 0.0003664472606033087\n",
            "Gradient Penalty: 19.734949111938477\n",
            "D_real: 0.002925178501754999, D_fake: 0.0004068893613293767\n",
            "Gradient Penalty: 19.7340030670166\n",
            "D_real: 0.003384446958079934, D_fake: 0.0002319299674127251\n",
            "Gradient Penalty: 19.734474182128906\n",
            "D_real: 0.003522970015183091, D_fake: 0.0003386125899851322\n",
            "Gradient Penalty: 19.738065719604492\n",
            "D_real: 0.002815644256770611, D_fake: 0.00023773388238623738\n",
            "Gradient Penalty: 19.733240127563477\n",
            "D_real: 0.003529641544446349, D_fake: 1.0299022505932953e-06\n",
            "Gradient Penalty: 19.739377975463867\n",
            "D_real: 0.002789653604850173, D_fake: 0.0004136660718359053\n",
            "Gradient Penalty: 19.734840393066406\n",
            "D_real: 0.0036585743073374033, D_fake: 0.0005165208131074905\n",
            "Gradient Penalty: 19.737892150878906\n",
            "D_real: 0.0031094246078282595, D_fake: 0.0002616680576466024\n",
            "Gradient Penalty: 19.735485076904297\n",
            "D_real: 0.0033077369444072247, D_fake: 0.00022216937213670462\n",
            "Gradient Penalty: 19.732017517089844\n",
            "D_real: 0.003311213105916977, D_fake: 0.00023951276671141386\n",
            "Gradient Penalty: 19.734619140625\n",
            "D_real: 0.002983186859637499, D_fake: 0.00039862008998170495\n",
            "Gradient Penalty: 19.73331642150879\n",
            "D_real: 0.0032345473300665617, D_fake: 0.00028132760780863464\n",
            "Gradient Penalty: 19.737342834472656\n",
            "D_real: 0.0036439169198274612, D_fake: 0.0002441990945953876\n",
            "Gradient Penalty: 19.73579216003418\n",
            "D_real: 0.0034126557875424623, D_fake: 0.0002574104873929173\n",
            "Gradient Penalty: 19.736215591430664\n",
            "D_real: 0.0031281374394893646, D_fake: 0.00014770013513043523\n",
            "Gradient Penalty: 19.7359676361084\n",
            "D_real: 0.003876028349623084, D_fake: 0.0002633686235640198\n",
            "Gradient Penalty: 19.735748291015625\n",
            "D_real: 0.0023902524262666702, D_fake: 0.00042316794861108065\n",
            "Gradient Penalty: 19.73150062561035\n",
            "D_real: 0.0030474767554551363, D_fake: 0.00028572013252414763\n",
            "Gradient Penalty: 19.736209869384766\n",
            "D_real: 0.0032717580907046795, D_fake: 0.0002845363924279809\n",
            "Gradient Penalty: 19.73440933227539\n",
            "D_real: 0.0037731188349425793, D_fake: 0.00033395912032574415\n",
            "Gradient Penalty: 19.736146926879883\n",
            "D_real: 0.0022529861889779568, D_fake: 0.0004324473557062447\n",
            "Gradient Penalty: 19.735435485839844\n",
            "D_real: 0.0033421411644667387, D_fake: 0.00043763406574726105\n",
            "Gradient Penalty: 19.736642837524414\n",
            "D_real: 0.004445970989763737, D_fake: 0.0003390952479094267\n",
            "Gradient Penalty: 19.738561630249023\n",
            "D_real: 0.0038740236777812243, D_fake: 0.00038003912777639925\n",
            "Gradient Penalty: 19.73659324645996\n",
            "D_real: 0.00287819211371243, D_fake: 0.00042649684473872185\n",
            "Gradient Penalty: 19.735742568969727\n",
            "D_real: 0.0024316716007888317, D_fake: 0.0002882254484575242\n",
            "Gradient Penalty: 19.740760803222656\n",
            "D_real: 0.004241020418703556, D_fake: 0.00030700580100528896\n",
            "Gradient Penalty: 19.737667083740234\n",
            "D_real: 0.0037452620454132557, D_fake: 0.00020264076010789722\n",
            "Gradient Penalty: 19.736364364624023\n",
            "D_real: 0.0034608524292707443, D_fake: 0.0002655976568348706\n",
            "Gradient Penalty: 19.734033584594727\n",
            "D_real: 0.003976659849286079, D_fake: 0.0004210654296912253\n",
            "Gradient Penalty: 19.734289169311523\n",
            "D_real: 0.0028325123712420464, D_fake: 0.00029249058570712805\n",
            "Gradient Penalty: 19.73548698425293\n",
            "D_real: 0.0030484888702630997, D_fake: 0.00034548708936199546\n",
            "Gradient Penalty: 19.735414505004883\n",
            "D_real: 0.0027836617082357407, D_fake: 0.00030839850660413504\n",
            "Gradient Penalty: 19.728675842285156\n",
            "D_real: 0.003136718412861228, D_fake: 0.00027831544866785407\n",
            "Gradient Penalty: 19.73566436767578\n",
            "D_real: 0.003143132198601961, D_fake: 0.0003440103610046208\n",
            "Gradient Penalty: 19.73611831665039\n",
            "D_real: 0.003860944416373968, D_fake: 0.000250650045927614\n",
            "Gradient Penalty: 19.737857818603516\n",
            "D_real: 0.0029222562443464994, D_fake: 0.00027912293444387615\n",
            "Gradient Penalty: 19.738147735595703\n",
            "D_real: 0.0034494653809815645, D_fake: 0.00031301594572141767\n",
            "Gradient Penalty: 19.734943389892578\n",
            "D_real: 0.0039985282346606255, D_fake: 0.00026821758365258574\n",
            "Gradient Penalty: 19.73554039001465\n",
            "D_real: 0.002354695927351713, D_fake: 0.0003319185343571007\n",
            "Gradient Penalty: 19.733781814575195\n",
            "D_real: 0.0030780714005231857, D_fake: 0.00023877629428170621\n",
            "Gradient Penalty: 19.73668670654297\n",
            "D_real: 0.0032829316332936287, D_fake: 0.0002477138477843255\n",
            "Gradient Penalty: 19.73667335510254\n",
            "D_real: 0.002584133530035615, D_fake: 0.00022101310605648905\n",
            "Gradient Penalty: 19.7359676361084\n",
            "D_real: 0.0036719748750329018, D_fake: 0.00024842098355293274\n",
            "Gradient Penalty: 19.736465454101562\n",
            "D_real: 0.0031053125858306885, D_fake: 0.0003095765714533627\n",
            "Gradient Penalty: 19.73574447631836\n",
            "D_real: 0.0021533453837037086, D_fake: 0.0003549876855686307\n",
            "Gradient Penalty: 19.733985900878906\n",
            "D_real: 0.0020164079032838345, D_fake: 0.00047897506738081574\n",
            "Gradient Penalty: 19.7353515625\n",
            "D_real: 0.0026481091044843197, D_fake: 0.00018199917394667864\n",
            "Gradient Penalty: 19.731769561767578\n",
            "D_real: 0.002721237251535058, D_fake: 0.00029675563564524055\n",
            "Gradient Penalty: 19.737565994262695\n",
            "D_real: 0.0028317845426499844, D_fake: 0.00038985576247796416\n",
            "Gradient Penalty: 19.73526382446289\n",
            "D_real: 0.00287158926948905, D_fake: 0.00033761566737666726\n",
            "Gradient Penalty: 19.738513946533203\n",
            "D_real: 0.0032477802596986294, D_fake: 0.00028903575730510056\n",
            "Gradient Penalty: 19.7425594329834\n",
            "D_real: 0.003661747556179762, D_fake: 0.0003617272595874965\n",
            "Gradient Penalty: 19.73280143737793\n",
            "D_real: 0.0032804282382130623, D_fake: 0.000265731243416667\n",
            "Gradient Penalty: 19.732301712036133\n",
            "D_real: 0.0030079460702836514, D_fake: 0.00017885802662931383\n",
            "Gradient Penalty: 19.736988067626953\n",
            "D_real: 0.002860529813915491, D_fake: 0.00033678070758469403\n",
            "Gradient Penalty: 19.733509063720703\n",
            "D_real: 0.003373227082192898, D_fake: 0.00030147869256325066\n",
            "Gradient Penalty: 19.739295959472656\n",
            "D_real: 0.0031230011954903603, D_fake: 0.00024857872631400824\n",
            "Gradient Penalty: 19.734817504882812\n",
            "D_real: 0.0035186544992029667, D_fake: 0.0004029280389659107\n",
            "Gradient Penalty: 19.735029220581055\n",
            "D_real: 0.004229268990457058, D_fake: 0.0003491675015538931\n",
            "Gradient Penalty: 19.739744186401367\n",
            "D_real: 0.003100388217717409, D_fake: 0.0001833153364714235\n",
            "Gradient Penalty: 19.73416519165039\n",
            "D_real: 0.0024674891028553247, D_fake: 0.00017360450874548405\n",
            "Gradient Penalty: 19.737689971923828\n",
            "D_real: 0.003394974395632744, D_fake: 0.00032593082869425416\n",
            "Gradient Penalty: 19.73668098449707\n",
            "D_real: 0.003586339298635721, D_fake: 0.00014230117085389793\n",
            "Gradient Penalty: 19.73721694946289\n",
            "D_real: 0.0028112847357988358, D_fake: 0.00022540042118635029\n",
            "Gradient Penalty: 19.733613967895508\n",
            "D_real: 0.0029905103147029877, D_fake: 0.0002784788957796991\n",
            "Gradient Penalty: 19.735410690307617\n",
            "D_real: 0.0027270002756267786, D_fake: 0.0005374214961193502\n",
            "Gradient Penalty: 19.734893798828125\n",
            "D_real: 0.003971880301833153, D_fake: 0.00019921691273339093\n",
            "Gradient Penalty: 19.736248016357422\n",
            "D_real: 0.0024686534889042377, D_fake: 0.0003155290614813566\n",
            "Gradient Penalty: 19.734272003173828\n",
            "D_real: 0.003872244618833065, D_fake: 0.0003758514649234712\n",
            "Gradient Penalty: 19.734304428100586\n",
            "D_real: 0.0024696169421076775, D_fake: 0.000262125045992434\n",
            "Gradient Penalty: 19.73822593688965\n",
            "D_real: 0.003796893637627363, D_fake: 0.00042062229476869106\n",
            "Gradient Penalty: 19.737295150756836\n",
            "D_real: 0.0028017195872962475, D_fake: 0.00032993825152516365\n",
            "Gradient Penalty: 19.735111236572266\n",
            "D_real: 0.0035872869193553925, D_fake: 0.000486815843032673\n",
            "Gradient Penalty: 19.737476348876953\n",
            "D_real: 0.003680730704218149, D_fake: 0.00026459884247742593\n",
            "Gradient Penalty: 19.734498977661133\n",
            "D_real: 0.004458153620362282, D_fake: 0.0001677385880611837\n",
            "Gradient Penalty: 19.736560821533203\n",
            "D_real: 0.0015242863446474075, D_fake: 0.0002708110841922462\n",
            "Gradient Penalty: 19.734310150146484\n",
            "D_real: 0.002904012333601713, D_fake: 0.000408132909797132\n",
            "Gradient Penalty: 19.734745025634766\n",
            "D_real: 0.002878971863538027, D_fake: 0.00035893850144930184\n",
            "Gradient Penalty: 19.734025955200195\n",
            "D_real: 0.002794181928038597, D_fake: 0.00012727426656056195\n",
            "Gradient Penalty: 19.73545265197754\n",
            "D_real: 0.0028427699580788612, D_fake: 0.00032513021142221987\n",
            "Gradient Penalty: 19.73887825012207\n",
            "D_real: 0.002763616619631648, D_fake: 0.00024454016238451004\n",
            "Gradient Penalty: 19.735233306884766\n",
            "D_real: 0.0022134017199277878, D_fake: 0.0002792236628010869\n",
            "Gradient Penalty: 19.734188079833984\n",
            "D_real: 0.004665506072342396, D_fake: 0.0002536289393901825\n",
            "Gradient Penalty: 19.741458892822266\n",
            "D_real: 0.003046551952138543, D_fake: 0.0002141755830962211\n",
            "Gradient Penalty: 19.738964080810547\n",
            "D_real: 0.0043752677738666534, D_fake: 0.0002000604581553489\n",
            "Gradient Penalty: 19.736968994140625\n",
            "D_real: 0.0027636969462037086, D_fake: 0.0002947240718640387\n",
            "Gradient Penalty: 19.731849670410156\n",
            "D_real: 0.002737232018262148, D_fake: 0.00034237554064020514\n",
            "Gradient Penalty: 19.741003036499023\n",
            "D_real: 0.004313603509217501, D_fake: 0.0003089234232902527\n",
            "Gradient Penalty: 19.73514747619629\n",
            "D_real: 0.0036950840149074793, D_fake: 0.00037390930810943246\n",
            "Gradient Penalty: 19.734519958496094\n",
            "D_real: 0.00330872880294919, D_fake: 0.0003942207549698651\n",
            "Gradient Penalty: 19.737714767456055\n",
            "D_real: 0.004177914001047611, D_fake: 0.0003305510908830911\n",
            "Gradient Penalty: 19.735286712646484\n",
            "D_real: 0.004349184222519398, D_fake: 0.0001368398079648614\n",
            "Gradient Penalty: 19.738128662109375\n",
            "D_real: 0.0033516427502036095, D_fake: 0.0001295225665671751\n",
            "Gradient Penalty: 19.735061645507812\n",
            "D_real: 0.0035877744667232037, D_fake: 0.0002447861770633608\n",
            "Gradient Penalty: 19.735305786132812\n",
            "D_real: 0.0028614606708288193, D_fake: 0.00043519242899492383\n",
            "Gradient Penalty: 19.733211517333984\n",
            "D_real: 0.002950419206172228, D_fake: 0.00036775850458070636\n",
            "Gradient Penalty: 19.73609161376953\n",
            "D_real: 0.00392119912430644, D_fake: 0.00031476045842282474\n",
            "Gradient Penalty: 19.739259719848633\n",
            "D_real: 0.003076782915741205, D_fake: 0.0003100107132922858\n",
            "Gradient Penalty: 19.73744773864746\n",
            "D_real: 0.004377187695354223, D_fake: 0.0003254967159591615\n",
            "Gradient Penalty: 19.73733139038086\n",
            "D_real: 0.00204190774820745, D_fake: 0.00044936127960681915\n",
            "Gradient Penalty: 19.734525680541992\n",
            "D_real: 0.0036452626809477806, D_fake: 0.0002686658699531108\n",
            "Gradient Penalty: 19.73543357849121\n",
            "D_real: 0.0031178260687738657, D_fake: 0.0002617951831780374\n",
            "Gradient Penalty: 19.73379135131836\n",
            "D_real: 0.003267851425334811, D_fake: 0.00034688549931161106\n",
            "Gradient Penalty: 19.736299514770508\n",
            "D_real: 0.005039181560277939, D_fake: 0.00039280267083086073\n",
            "Gradient Penalty: 19.734628677368164\n",
            "D_real: 0.0036054840311408043, D_fake: 0.0002931033377535641\n",
            "Gradient Penalty: 19.740192413330078\n",
            "D_real: 0.0027962292078882456, D_fake: 0.00018522783648222685\n",
            "Gradient Penalty: 19.73614501953125\n",
            "D_real: 0.0043633924797177315, D_fake: 0.00023611958022229373\n",
            "Gradient Penalty: 19.737266540527344\n",
            "D_real: 0.0029800855554640293, D_fake: 0.00022075261222198606\n",
            "Gradient Penalty: 19.733503341674805\n",
            "D_real: 0.0032020765356719494, D_fake: 0.0002930181217379868\n",
            "Gradient Penalty: 19.735610961914062\n",
            "D_real: 0.0025646453723311424, D_fake: 0.00026462163077667356\n",
            "Gradient Penalty: 19.73649024963379\n",
            "D_real: 0.0038172462955117226, D_fake: 0.00045599587610922754\n",
            "Gradient Penalty: 19.739208221435547\n",
            "D_real: 0.0036722535733133554, D_fake: 0.00022622062533628196\n",
            "Gradient Penalty: 19.73884391784668\n",
            "D_real: 0.00280974293127656, D_fake: 0.00026473894831724465\n",
            "Gradient Penalty: 19.734128952026367\n",
            "D_real: 0.003774318378418684, D_fake: 0.0003115949220955372\n",
            "Gradient Penalty: 19.739599227905273\n",
            "D_real: 0.0032812936697155237, D_fake: 0.000291129807010293\n",
            "Gradient Penalty: 19.73672866821289\n",
            "D_real: 0.004091358277946711, D_fake: 0.00034224468981847167\n",
            "Gradient Penalty: 19.73440170288086\n",
            "D_real: 0.0022316239774227142, D_fake: 0.0002273970312671736\n",
            "Gradient Penalty: 19.734251022338867\n",
            "D_real: 0.0030770953744649887, D_fake: 0.00012969497765880078\n",
            "Gradient Penalty: 19.736305236816406\n",
            "D_real: 0.004106553271412849, D_fake: 0.00029155192896723747\n",
            "Gradient Penalty: 19.73619842529297\n",
            "D_real: 0.0024914080277085304, D_fake: 0.0002638392033986747\n",
            "Gradient Penalty: 19.73533821105957\n",
            "D_real: 0.0027605842333287, D_fake: 0.00017839978681877255\n",
            "Gradient Penalty: 19.739395141601562\n",
            "D_real: 0.003627457655966282, D_fake: 0.00021221002680249512\n",
            "Gradient Penalty: 19.737529754638672\n",
            "D_real: 0.0036040847189724445, D_fake: 0.00022041409101802856\n",
            "Gradient Penalty: 19.73798942565918\n",
            "D_real: 0.003905212739482522, D_fake: 0.00028778513660654426\n",
            "Gradient Penalty: 19.739274978637695\n",
            "D_real: 0.0022108573466539383, D_fake: 0.00018943024042528123\n",
            "Gradient Penalty: 19.733932495117188\n",
            "D_real: 0.003538385033607483, D_fake: 0.0002854694612324238\n",
            "Gradient Penalty: 19.734268188476562\n",
            "D_real: 0.0030719246715307236, D_fake: 0.00034832360688596964\n",
            "Gradient Penalty: 19.73545265197754\n",
            "D_real: 0.0036118743009865284, D_fake: 0.0002560070133768022\n",
            "Gradient Penalty: 19.736675262451172\n",
            "D_real: 0.002714935690164566, D_fake: 0.00041609344771131873\n",
            "Gradient Penalty: 19.740324020385742\n",
            "D_real: 0.004138736519962549, D_fake: 0.00021514095715247095\n",
            "Gradient Penalty: 19.737060546875\n",
            "D_real: 0.0025458047166466713, D_fake: 0.00032522925175726414\n",
            "Gradient Penalty: 19.738128662109375\n",
            "D_real: 0.0037488993257284164, D_fake: 0.0003758492530323565\n",
            "Gradient Penalty: 19.736635208129883\n",
            "D_real: 0.0033124552574008703, D_fake: 0.0003180227940902114\n",
            "Gradient Penalty: 19.734844207763672\n",
            "D_real: 0.004448586143553257, D_fake: 0.00037015340058133006\n",
            "Gradient Penalty: 19.739744186401367\n",
            "D_real: 0.0032283056061714888, D_fake: 0.00039974492392502725\n",
            "Gradient Penalty: 19.737289428710938\n",
            "D_real: 0.001753157819621265, D_fake: 0.00026968427118845284\n",
            "Gradient Penalty: 19.73419189453125\n",
            "D_real: 0.003192322328686714, D_fake: 0.0002740705676842481\n",
            "Gradient Penalty: 19.73932456970215\n",
            "D_real: 0.0032155574299395084, D_fake: 0.00032276922138407826\n",
            "Gradient Penalty: 19.73341941833496\n",
            "D_real: 0.003803479252383113, D_fake: 0.0003295157221145928\n",
            "Gradient Penalty: 19.733259201049805\n",
            "D_real: 0.002512311562895775, D_fake: 0.0003635053290054202\n",
            "Gradient Penalty: 19.734209060668945\n",
            "D_real: 0.002630434464663267, D_fake: 0.0004091892915312201\n",
            "Gradient Penalty: 19.73640251159668\n",
            "D_real: 0.0031276885420084, D_fake: 0.0004138037911616266\n",
            "Gradient Penalty: 19.736587524414062\n",
            "D_real: 0.0023854600731283426, D_fake: 0.000268916308414191\n",
            "Gradient Penalty: 19.734272003173828\n",
            "D_real: 0.002680444624274969, D_fake: 0.00027723779203370214\n",
            "Gradient Penalty: 19.73503303527832\n",
            "D_real: 0.003436540486291051, D_fake: 0.00034781487192958593\n",
            "Gradient Penalty: 19.7386531829834\n",
            "D_real: 0.0035901498049497604, D_fake: 0.0004032544675283134\n",
            "Gradient Penalty: 19.73743438720703\n",
            "D_real: 0.0028792712837457657, D_fake: 0.00038276606937870383\n",
            "Gradient Penalty: 19.73351287841797\n",
            "D_real: 0.002975305076688528, D_fake: 0.0002543126174714416\n",
            "Gradient Penalty: 19.734355926513672\n",
            "D_real: 0.0033786455169320107, D_fake: 0.00034150545252487063\n",
            "Gradient Penalty: 19.73444366455078\n",
            "D_real: 0.0030408850871026516, D_fake: 0.00039499648846685886\n",
            "Gradient Penalty: 19.738065719604492\n",
            "D_real: 0.002638115081936121, D_fake: 0.0003225704422220588\n",
            "Gradient Penalty: 19.73427963256836\n",
            "D_real: 0.002921726554632187, D_fake: 0.00028751714853569865\n",
            "Gradient Penalty: 19.737049102783203\n",
            "D_real: 0.0026782844215631485, D_fake: 0.0003023854806087911\n",
            "Gradient Penalty: 19.731121063232422\n",
            "D_real: 0.004204676020890474, D_fake: 0.00030559877632185817\n",
            "Gradient Penalty: 19.740116119384766\n",
            "D_real: 0.0037466774228960276, D_fake: 0.0002846505958586931\n",
            "Gradient Penalty: 19.73600196838379\n",
            "D_real: 0.003291178960353136, D_fake: 0.00020199839491397142\n",
            "Gradient Penalty: 19.73603057861328\n",
            "D_real: 0.0024881334975361824, D_fake: 0.00028410361846908927\n",
            "Gradient Penalty: 19.738765716552734\n",
            "D_real: 0.0025404158513993025, D_fake: 0.0002860875683836639\n",
            "Gradient Penalty: 19.73779296875\n",
            "D_real: 0.0037194429896771908, D_fake: 0.0002846216957550496\n",
            "Gradient Penalty: 19.737018585205078\n",
            "D_real: 0.0032885102555155754, D_fake: 0.00040957299643196166\n",
            "Gradient Penalty: 19.738264083862305\n",
            "D_real: 0.002752959728240967, D_fake: 0.00024267937988042831\n",
            "Gradient Penalty: 19.733509063720703\n",
            "D_real: 0.002338895108550787, D_fake: 0.0003594903973862529\n",
            "Gradient Penalty: 19.734222412109375\n",
            "D_real: 0.00338614359498024, D_fake: 0.0002448324812576175\n",
            "Gradient Penalty: 19.736312866210938\n",
            "D_real: 0.0035113170742988586, D_fake: 0.0004234600637573749\n",
            "Gradient Penalty: 19.736968994140625\n",
            "D_real: 0.002960232552140951, D_fake: 0.0003500984748825431\n",
            "Gradient Penalty: 19.73824691772461\n",
            "D_real: 0.003856304567307234, D_fake: 0.0003624095697887242\n",
            "Gradient Penalty: 19.73462677001953\n",
            "D_real: 0.002604242181405425, D_fake: 0.00030286514083854854\n",
            "Gradient Penalty: 19.736324310302734\n",
            "D_real: 0.0039633214473724365, D_fake: 0.00016459275502711535\n",
            "Gradient Penalty: 19.739809036254883\n",
            "D_real: 0.0013595223426818848, D_fake: 0.0003576186136342585\n",
            "Gradient Penalty: 19.733278274536133\n",
            "D_real: 0.0034910363610833883, D_fake: 0.0003503934131003916\n",
            "Gradient Penalty: 19.735204696655273\n",
            "D_real: 0.002296761143952608, D_fake: 0.0003711571334861219\n",
            "Gradient Penalty: 19.731611251831055\n",
            "D_real: 0.002697568852454424, D_fake: 0.0001535957562737167\n",
            "Gradient Penalty: 19.737537384033203\n",
            "D_real: 0.003489219816401601, D_fake: 0.0003739686217159033\n",
            "Gradient Penalty: 19.740631103515625\n",
            "D_real: 0.0029429136775434017, D_fake: 0.00018641786300577223\n",
            "Gradient Penalty: 19.734966278076172\n",
            "D_real: 0.002078825142234564, D_fake: 0.00034681957913562655\n",
            "Gradient Penalty: 19.733083724975586\n",
            "D_real: 0.0029160401318222284, D_fake: 0.00019273351063020527\n",
            "Gradient Penalty: 19.737627029418945\n",
            "D_real: 0.0030760494992136955, D_fake: 0.00036412180634215474\n",
            "Gradient Penalty: 19.733503341674805\n",
            "D_real: 0.003616117872297764, D_fake: 0.00014652201207354665\n",
            "Gradient Penalty: 19.73836898803711\n",
            "D_real: 0.003445715643465519, D_fake: 0.0003821966238319874\n",
            "Gradient Penalty: 19.73707389831543\n",
            "D_real: 0.003673349041491747, D_fake: 0.00021031851065345109\n",
            "Gradient Penalty: 19.736709594726562\n",
            "D_real: 0.0043541984632611275, D_fake: 0.0003160973428748548\n",
            "Gradient Penalty: 19.736764907836914\n",
            "D_real: 0.0032903056126087904, D_fake: 0.00023028143914416432\n",
            "Gradient Penalty: 19.736509323120117\n",
            "D_real: 0.003341741394251585, D_fake: 0.00029760278994217515\n",
            "Gradient Penalty: 19.738195419311523\n",
            "D_real: 0.0028962078504264355, D_fake: 0.00030561984749510884\n",
            "Gradient Penalty: 19.736135482788086\n",
            "D_real: 0.0036768075078725815, D_fake: 0.00035857377224601805\n",
            "Gradient Penalty: 19.737747192382812\n",
            "D_real: 0.0033900244161486626, D_fake: 0.00044376635923981667\n",
            "Gradient Penalty: 19.73404312133789\n",
            "D_real: 0.002629949012771249, D_fake: 0.00021019305859226733\n",
            "Gradient Penalty: 19.734678268432617\n",
            "D_real: 0.0036164463963359594, D_fake: 0.00030632459674961865\n",
            "Gradient Penalty: 19.737241744995117\n",
            "D_real: 0.002308944007381797, D_fake: 0.00023115727526601404\n",
            "Gradient Penalty: 19.732397079467773\n",
            "D_real: 0.0035871476866304874, D_fake: 0.00038628801121376455\n",
            "Gradient Penalty: 19.739778518676758\n",
            "D_real: 0.004418494179844856, D_fake: 0.00021400385594461113\n",
            "Gradient Penalty: 19.734804153442383\n",
            "D_real: 0.0026701679453253746, D_fake: 0.00032552910852245986\n",
            "Gradient Penalty: 19.735517501831055\n",
            "D_real: 0.003221332561224699, D_fake: 0.0002843305701389909\n",
            "Gradient Penalty: 19.737077713012695\n",
            "D_real: 0.0028611719608306885, D_fake: 0.0002772055449895561\n",
            "Gradient Penalty: 19.736400604248047\n",
            "D_real: 0.0033611170947551727, D_fake: 0.0002346041437704116\n",
            "Gradient Penalty: 19.737667083740234\n",
            "D_real: 0.003612673841416836, D_fake: 0.0002486617013346404\n",
            "Gradient Penalty: 19.737016677856445\n",
            "D_real: 0.003727543167769909, D_fake: 0.00030207011150196195\n",
            "Gradient Penalty: 19.738628387451172\n",
            "D_real: 0.002334284596145153, D_fake: 0.00021648957044817507\n",
            "Gradient Penalty: 19.735496520996094\n",
            "D_real: 0.0028990618884563446, D_fake: 0.00043000990990549326\n",
            "Gradient Penalty: 19.737674713134766\n",
            "D_real: 0.0035091619938611984, D_fake: 0.00023145187878981233\n",
            "Gradient Penalty: 19.736347198486328\n",
            "D_real: 0.0022963890805840492, D_fake: 0.00015255052130669355\n",
            "Gradient Penalty: 19.736080169677734\n",
            "D_real: 0.003728155279532075, D_fake: 0.00023539505491498858\n",
            "Gradient Penalty: 19.737640380859375\n",
            "D_real: 0.0024832438211888075, D_fake: 0.00031731699709780514\n",
            "Gradient Penalty: 19.737682342529297\n",
            "D_real: 0.0023969640024006367, D_fake: 0.0003620508359745145\n",
            "Gradient Penalty: 19.733177185058594\n",
            "D_real: 0.0028442861512303352, D_fake: 0.00024165835930034518\n",
            "Gradient Penalty: 19.73585319519043\n",
            "D_real: 0.003369927406311035, D_fake: 0.0002150182262994349\n",
            "Gradient Penalty: 19.73154640197754\n",
            "D_real: 0.002733058761805296, D_fake: 0.00027499927091412246\n",
            "Gradient Penalty: 19.738027572631836\n",
            "D_real: 0.003966107964515686, D_fake: 0.00033268207334913313\n",
            "Gradient Penalty: 19.738338470458984\n",
            "D_real: 0.003199494443833828, D_fake: 0.00016225292347371578\n",
            "Gradient Penalty: 19.73434066772461\n",
            "D_real: 0.0038060483057051897, D_fake: 0.00028389450744725764\n",
            "Gradient Penalty: 19.736000061035156\n",
            "D_real: 0.002995578106492758, D_fake: 0.0003800572012551129\n",
            "Gradient Penalty: 19.737285614013672\n",
            "D_real: 0.002931111492216587, D_fake: 0.0002138670824933797\n",
            "Gradient Penalty: 19.738283157348633\n",
            "D_real: 0.003545033745467663, D_fake: 0.0003517029108479619\n",
            "Gradient Penalty: 19.733997344970703\n",
            "D_real: 0.003588088322430849, D_fake: 0.00026234096731059253\n",
            "Gradient Penalty: 19.735857009887695\n",
            "D_real: 0.004235797096043825, D_fake: 0.0003256969794165343\n",
            "Gradient Penalty: 19.73374366760254\n",
            "D_real: 0.0028633163310587406, D_fake: 0.00022264636936597526\n",
            "Gradient Penalty: 19.736356735229492\n",
            "D_real: 0.003312467597424984, D_fake: 0.00023240045993588865\n",
            "Gradient Penalty: 19.73533058166504\n",
            "D_real: 0.002217306988313794, D_fake: 0.0002829001168720424\n",
            "Gradient Penalty: 19.736845016479492\n",
            "D_real: 0.002672816626727581, D_fake: 0.0004089012509211898\n",
            "Gradient Penalty: 19.740127563476562\n",
            "D_real: 0.0029331333935260773, D_fake: 0.0004477780603338033\n",
            "Gradient Penalty: 19.736602783203125\n",
            "D_real: 0.003951173275709152, D_fake: 0.0003131481644231826\n",
            "Gradient Penalty: 19.73461151123047\n",
            "D_real: 0.001751789590343833, D_fake: 0.00029693785472773015\n",
            "Gradient Penalty: 19.736337661743164\n",
            "D_real: 0.0024237497709691525, D_fake: 0.00045708936522714794\n",
            "Gradient Penalty: 19.73463249206543\n",
            "D_real: 0.0026110843755304813, D_fake: 0.0003093159757554531\n",
            "Gradient Penalty: 19.733749389648438\n",
            "D_real: 0.004009542521089315, D_fake: 0.0002780200447887182\n",
            "Gradient Penalty: 19.735776901245117\n",
            "D_real: 0.0031669377349317074, D_fake: 0.000251558783929795\n",
            "Gradient Penalty: 19.73508644104004\n",
            "D_real: 0.004111659713089466, D_fake: 0.00030710920691490173\n",
            "Gradient Penalty: 19.741004943847656\n",
            "D_real: 0.0024258089251816273, D_fake: 0.0003366480814293027\n",
            "Gradient Penalty: 19.736478805541992\n",
            "D_real: 0.004229196347296238, D_fake: 0.0002917993697337806\n",
            "Gradient Penalty: 19.736478805541992\n",
            "D_real: 0.0034451496321707964, D_fake: 0.00019331199291627854\n",
            "Gradient Penalty: 19.7340145111084\n",
            "D_real: 0.0025527654215693474, D_fake: 0.0002765841782093048\n",
            "Gradient Penalty: 19.7329158782959\n",
            "D_real: 0.002576244994997978, D_fake: 0.0002858911466319114\n",
            "Gradient Penalty: 19.737621307373047\n",
            "D_real: 0.0035705282352864742, D_fake: 0.00037752336356788874\n",
            "Gradient Penalty: 19.734615325927734\n",
            "D_real: 0.0036104200407862663, D_fake: 0.00035412737634032965\n",
            "Gradient Penalty: 19.737403869628906\n",
            "D_real: 0.003089352510869503, D_fake: 0.00017454408225603402\n",
            "Gradient Penalty: 19.73565673828125\n",
            "D_real: 0.004650611896067858, D_fake: 0.00017274767742492259\n",
            "Gradient Penalty: 19.738059997558594\n",
            "D_real: 0.0037259594537317753, D_fake: 0.0002494962536729872\n",
            "Gradient Penalty: 19.738231658935547\n",
            "D_real: 0.003740068292245269, D_fake: 0.0002894372446462512\n",
            "Gradient Penalty: 19.73712730407715\n",
            "D_real: 0.003337422851473093, D_fake: 0.00034846048220060766\n",
            "Gradient Penalty: 19.739286422729492\n",
            "D_real: 0.0030604349449276924, D_fake: 0.00029024307150393724\n",
            "Gradient Penalty: 19.73492431640625\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFM0lEQVR4nO3cMW7EMAwAQSvw/7/MdIs0AXyFYl9uplbBbsFCXDMzBwAcx/F19wAAPIcoABBRACCiAEBEAYCIAgARBQAiCgDkvPpwrbVzDgA2u/JX2aYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOe8eAF4xM5ffrrU2TgL/k00BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgBx5oK34nQF7GVTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAzrsHAD7TzLz0fq21aRJ+sikAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDOXAC3cLbimWwKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyHn3AMBzzcxL79damybhr9gUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgbh8Bv3LL6PPYFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCcVx/OzM45AHgAmwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAPkGFvwWHLbh6voAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.0.weight - Grad Norm: 0.0002571547811385244\n",
            "main.2.weight - Grad Norm: 0.00025181990349665284\n",
            "main.4.weight - Grad Norm: 0.0002471710613463074\n",
            "main.6.weight - Grad Norm: 0.00018979428568854928\n",
            "Epoch [1/100] - D Loss: 19.7332, G Loss: -0.0000\n",
            "D_real: 0.007316629868000746, D_fake: 0.0004251599602866918\n",
            "Gradient Penalty: 19.724523544311523\n",
            "D_real: 0.006453275214880705, D_fake: 0.0003685657284222543\n",
            "Gradient Penalty: 19.7248477935791\n",
            "D_real: 0.006845131516456604, D_fake: 0.0004556328058242798\n",
            "Gradient Penalty: 19.726245880126953\n",
            "D_real: 0.00714823929592967, D_fake: 0.00038770679384469986\n",
            "Gradient Penalty: 19.721683502197266\n",
            "D_real: 0.006285489536821842, D_fake: 0.00055810681078583\n",
            "Gradient Penalty: 19.725414276123047\n",
            "D_real: 0.006592260207980871, D_fake: 0.0005213418626226485\n",
            "Gradient Penalty: 19.719812393188477\n",
            "D_real: 0.00664263591170311, D_fake: 0.0005943350843153894\n",
            "Gradient Penalty: 19.72224235534668\n",
            "D_real: 0.006536013912409544, D_fake: 0.00043003587052226067\n",
            "Gradient Penalty: 19.72332000732422\n",
            "D_real: 0.0078078205697238445, D_fake: 0.0006070284871384501\n",
            "Gradient Penalty: 19.721233367919922\n",
            "D_real: 0.007197350729256868, D_fake: 0.0006025787442922592\n",
            "Gradient Penalty: 19.725744247436523\n",
            "D_real: 0.007076701149344444, D_fake: 0.0004617935628630221\n",
            "Gradient Penalty: 19.722537994384766\n",
            "D_real: 0.007044391706585884, D_fake: 0.0004977242788299918\n",
            "Gradient Penalty: 19.723913192749023\n",
            "D_real: 0.0069600800052285194, D_fake: 0.0005206624045968056\n",
            "Gradient Penalty: 19.72064208984375\n",
            "D_real: 0.006499360781162977, D_fake: 0.0007400395115837455\n",
            "Gradient Penalty: 19.726369857788086\n",
            "D_real: 0.007323442492634058, D_fake: 0.0005692393751814961\n",
            "Gradient Penalty: 19.722736358642578\n",
            "D_real: 0.007749614305794239, D_fake: 0.00044698105193674564\n",
            "Gradient Penalty: 19.72356414794922\n",
            "D_real: 0.0076422602869570255, D_fake: 0.00039662630297243595\n",
            "Gradient Penalty: 19.71902084350586\n",
            "D_real: 0.006987046450376511, D_fake: 0.0003705705748870969\n",
            "Gradient Penalty: 19.71991539001465\n",
            "D_real: 0.007214732468128204, D_fake: 0.00044257938861846924\n",
            "Gradient Penalty: 19.723011016845703\n",
            "D_real: 0.0068547772243618965, D_fake: 0.0004366012872196734\n",
            "Gradient Penalty: 19.72132110595703\n",
            "D_real: 0.007515795528888702, D_fake: 0.0003942233743146062\n",
            "Gradient Penalty: 19.725568771362305\n",
            "D_real: 0.0062888809479773045, D_fake: 0.0006091835093684494\n",
            "Gradient Penalty: 19.722103118896484\n",
            "D_real: 0.007577162701636553, D_fake: 0.0005730950506404042\n",
            "Gradient Penalty: 19.722082138061523\n",
            "D_real: 0.00648771645501256, D_fake: 0.0003924529009964317\n",
            "Gradient Penalty: 19.72189712524414\n",
            "D_real: 0.006779045797884464, D_fake: 0.0005243772175163031\n",
            "Gradient Penalty: 19.722854614257812\n",
            "D_real: 0.007590333465486765, D_fake: 0.00038613140350207686\n",
            "Gradient Penalty: 19.72059440612793\n",
            "D_real: 0.006149365101009607, D_fake: 0.0006064280751161277\n",
            "Gradient Penalty: 19.7207088470459\n",
            "D_real: 0.007029072381556034, D_fake: 0.0005843151011504233\n",
            "Gradient Penalty: 19.721511840820312\n",
            "D_real: 0.0076809474267065525, D_fake: 0.0004993148613721132\n",
            "Gradient Penalty: 19.722301483154297\n",
            "D_real: 0.006464743986725807, D_fake: 0.0005454985657706857\n",
            "Gradient Penalty: 19.71780776977539\n",
            "D_real: 0.007134627550840378, D_fake: 0.0004460610798560083\n",
            "Gradient Penalty: 19.723609924316406\n",
            "D_real: 0.007729630917310715, D_fake: 0.00045981467701494694\n",
            "Gradient Penalty: 19.72392463684082\n",
            "D_real: 0.007687038742005825, D_fake: 0.0005387357086874545\n",
            "Gradient Penalty: 19.717226028442383\n",
            "D_real: 0.007175164297223091, D_fake: 0.0004210784682072699\n",
            "Gradient Penalty: 19.72284698486328\n",
            "D_real: 0.006704867817461491, D_fake: 0.00044124171836301684\n",
            "Gradient Penalty: 19.72336196899414\n",
            "D_real: 0.006935785058885813, D_fake: 0.0005920623661950231\n",
            "Gradient Penalty: 19.721803665161133\n",
            "D_real: 0.005748532712459564, D_fake: 0.0004973025643266737\n",
            "Gradient Penalty: 19.718936920166016\n",
            "D_real: 0.006472169421613216, D_fake: 0.00044009037083014846\n",
            "Gradient Penalty: 19.721298217773438\n",
            "D_real: 0.007051830645650625, D_fake: 0.00043193920282647014\n",
            "Gradient Penalty: 19.72135353088379\n",
            "D_real: 0.00744245108217001, D_fake: 0.0005132196238264441\n",
            "Gradient Penalty: 19.718725204467773\n",
            "D_real: 0.006820136681199074, D_fake: 0.000608468777500093\n",
            "Gradient Penalty: 19.72319793701172\n",
            "D_real: 0.006179812829941511, D_fake: 0.0003389459743630141\n",
            "Gradient Penalty: 19.719093322753906\n",
            "D_real: 0.006486303638666868, D_fake: 0.00048701316700316966\n",
            "Gradient Penalty: 19.72292137145996\n",
            "D_real: 0.008440088480710983, D_fake: 0.0003620292991399765\n",
            "Gradient Penalty: 19.725095748901367\n",
            "D_real: 0.006665571592748165, D_fake: 0.00030472344951704144\n",
            "Gradient Penalty: 19.72174835205078\n",
            "D_real: 0.0077184829860925674, D_fake: 0.0005404041148722172\n",
            "Gradient Penalty: 19.72195816040039\n",
            "D_real: 0.0066484916023910046, D_fake: 0.0004679668927565217\n",
            "Gradient Penalty: 19.721893310546875\n",
            "D_real: 0.0064263129606842995, D_fake: 0.00039930548518896103\n",
            "Gradient Penalty: 19.719743728637695\n",
            "D_real: 0.006516154855489731, D_fake: 0.0005009223241358995\n",
            "Gradient Penalty: 19.721813201904297\n",
            "D_real: 0.0077150482684373856, D_fake: 0.00045985812903381884\n",
            "Gradient Penalty: 19.724538803100586\n",
            "D_real: 0.005331001244485378, D_fake: 0.0004314187681302428\n",
            "Gradient Penalty: 19.71698760986328\n",
            "D_real: 0.005998759064823389, D_fake: 0.0005240550963208079\n",
            "Gradient Penalty: 19.71752166748047\n",
            "D_real: 0.0063982694409787655, D_fake: 0.0003504058695398271\n",
            "Gradient Penalty: 19.724294662475586\n",
            "D_real: 0.006363573018461466, D_fake: 0.00040079921018332243\n",
            "Gradient Penalty: 19.71914291381836\n",
            "D_real: 0.005312248133122921, D_fake: 0.0005213561817072332\n",
            "Gradient Penalty: 19.721458435058594\n",
            "D_real: 0.006776438094675541, D_fake: 0.0008112852810882032\n",
            "Gradient Penalty: 19.72067642211914\n",
            "D_real: 0.007465238682925701, D_fake: 0.0003755364450626075\n",
            "Gradient Penalty: 19.72016716003418\n",
            "D_real: 0.007388320751488209, D_fake: 0.0005491037736646831\n",
            "Gradient Penalty: 19.723430633544922\n",
            "D_real: 0.0061722369864583015, D_fake: 0.0004994836635887623\n",
            "Gradient Penalty: 19.720460891723633\n",
            "D_real: 0.006686123088002205, D_fake: 0.00030436774250119925\n",
            "Gradient Penalty: 19.722713470458984\n",
            "D_real: 0.007892764173448086, D_fake: 0.00048452027840539813\n",
            "Gradient Penalty: 19.725975036621094\n",
            "D_real: 0.0071471440605819225, D_fake: 0.00042323459638282657\n",
            "Gradient Penalty: 19.717714309692383\n",
            "D_real: 0.006362477317452431, D_fake: 0.0005454912316054106\n",
            "Gradient Penalty: 19.720508575439453\n",
            "D_real: 0.006782833486795425, D_fake: 0.000526625313796103\n",
            "Gradient Penalty: 19.721200942993164\n",
            "D_real: 0.007163398899137974, D_fake: 0.000533939222805202\n",
            "Gradient Penalty: 19.72412109375\n",
            "D_real: 0.005914650391787291, D_fake: 0.00044810620602220297\n",
            "Gradient Penalty: 19.717370986938477\n",
            "D_real: 0.0064745754934847355, D_fake: 0.0005362127558328211\n",
            "Gradient Penalty: 19.721965789794922\n",
            "D_real: 0.007935430854558945, D_fake: 0.0004106889246031642\n",
            "Gradient Penalty: 19.7188720703125\n",
            "D_real: 0.005589366424828768, D_fake: 0.0006194055895321071\n",
            "Gradient Penalty: 19.722761154174805\n",
            "D_real: 0.007004087790846825, D_fake: 0.00045147124910727143\n",
            "Gradient Penalty: 19.72589111328125\n",
            "D_real: 0.006397878751158714, D_fake: 0.000469392747618258\n",
            "Gradient Penalty: 19.717729568481445\n",
            "D_real: 0.00728908646851778, D_fake: 0.0005461977561935782\n",
            "Gradient Penalty: 19.72114372253418\n",
            "D_real: 0.005812280811369419, D_fake: 0.00047987813013605773\n",
            "Gradient Penalty: 19.721942901611328\n",
            "D_real: 0.007997753098607063, D_fake: 0.0006682268576696515\n",
            "Gradient Penalty: 19.720041275024414\n",
            "D_real: 0.006910647265613079, D_fake: 0.00036846991861239076\n",
            "Gradient Penalty: 19.719642639160156\n",
            "D_real: 0.0077691469341516495, D_fake: 0.0004031541757285595\n",
            "Gradient Penalty: 19.72592544555664\n",
            "D_real: 0.007224424742162228, D_fake: 0.0005644378252327442\n",
            "Gradient Penalty: 19.721271514892578\n",
            "D_real: 0.007130770478397608, D_fake: 0.0005595230031758547\n",
            "Gradient Penalty: 19.720664978027344\n",
            "D_real: 0.006749772932380438, D_fake: 0.00042843096889555454\n",
            "Gradient Penalty: 19.723060607910156\n",
            "D_real: 0.006738392636179924, D_fake: 0.0004396035219542682\n",
            "Gradient Penalty: 19.72438621520996\n",
            "D_real: 0.0057814400643110275, D_fake: 0.00047358934534713626\n",
            "Gradient Penalty: 19.719165802001953\n",
            "D_real: 0.007834291085600853, D_fake: 0.00032574188662692904\n",
            "Gradient Penalty: 19.723405838012695\n",
            "D_real: 0.006480145268142223, D_fake: 0.00048720138147473335\n",
            "Gradient Penalty: 19.718196868896484\n",
            "D_real: 0.005941759794950485, D_fake: 0.00041317762224934995\n",
            "Gradient Penalty: 19.720523834228516\n",
            "D_real: 0.006408669985830784, D_fake: 0.0006332001648843288\n",
            "Gradient Penalty: 19.723880767822266\n",
            "D_real: 0.007443994749337435, D_fake: 0.000579391373321414\n",
            "Gradient Penalty: 19.721609115600586\n",
            "D_real: 0.0068610552698373795, D_fake: 0.00044855085434392095\n",
            "Gradient Penalty: 19.725536346435547\n",
            "D_real: 0.00629868358373642, D_fake: 0.00058302644174546\n",
            "Gradient Penalty: 19.722553253173828\n",
            "D_real: 0.007974894717335701, D_fake: 0.0006994919385761023\n",
            "Gradient Penalty: 19.728124618530273\n",
            "D_real: 0.006528071127831936, D_fake: 0.0005235594580881298\n",
            "Gradient Penalty: 19.722675323486328\n",
            "D_real: 0.006293759681284428, D_fake: 0.0005330407293513417\n",
            "Gradient Penalty: 19.72305679321289\n",
            "D_real: 0.00621063681319356, D_fake: 0.0004744541074614972\n",
            "Gradient Penalty: 19.724273681640625\n",
            "D_real: 0.006027241237461567, D_fake: 0.0004941149381920695\n",
            "Gradient Penalty: 19.71756362915039\n",
            "D_real: 0.007323506288230419, D_fake: 0.0005070999031886458\n",
            "Gradient Penalty: 19.718496322631836\n",
            "D_real: 0.007312291767448187, D_fake: 0.00046011299127712846\n",
            "Gradient Penalty: 19.725849151611328\n",
            "D_real: 0.0071426499634981155, D_fake: 0.0006131666013970971\n",
            "Gradient Penalty: 19.716964721679688\n",
            "D_real: 0.005803944543004036, D_fake: 0.0005504703149199486\n",
            "Gradient Penalty: 19.724098205566406\n",
            "D_real: 0.008009831421077251, D_fake: 0.0004986068233847618\n",
            "Gradient Penalty: 19.719314575195312\n",
            "D_real: 0.006388546898961067, D_fake: 0.0004996181232854724\n",
            "Gradient Penalty: 19.719438552856445\n",
            "D_real: 0.0065370164811611176, D_fake: 0.0006547646480612457\n",
            "Gradient Penalty: 19.719329833984375\n",
            "D_real: 0.00657285563647747, D_fake: 0.0004971496528014541\n",
            "Gradient Penalty: 19.721691131591797\n",
            "D_real: 0.006324430927634239, D_fake: 0.0005115907406434417\n",
            "Gradient Penalty: 19.71730613708496\n",
            "D_real: 0.007943319156765938, D_fake: 0.00042917230166494846\n",
            "Gradient Penalty: 19.722463607788086\n",
            "D_real: 0.006620761007070541, D_fake: 0.00038256729021668434\n",
            "Gradient Penalty: 19.717361450195312\n",
            "D_real: 0.007415967993438244, D_fake: 0.0004051380092278123\n",
            "Gradient Penalty: 19.722274780273438\n",
            "D_real: 0.0070548583753407, D_fake: 0.0005661973846144974\n",
            "Gradient Penalty: 19.719844818115234\n",
            "D_real: 0.006457377690821886, D_fake: 0.00039003448910079896\n",
            "Gradient Penalty: 19.72195053100586\n",
            "D_real: 0.00608066376298666, D_fake: 0.0004389382665976882\n",
            "Gradient Penalty: 19.72431182861328\n",
            "D_real: 0.007134907878935337, D_fake: 0.0006433177622966468\n",
            "Gradient Penalty: 19.72136688232422\n",
            "D_real: 0.006430977024137974, D_fake: 0.0005063108983449638\n",
            "Gradient Penalty: 19.72150993347168\n",
            "D_real: 0.006247472018003464, D_fake: 0.0005122080910950899\n",
            "Gradient Penalty: 19.72370719909668\n",
            "D_real: 0.0079038729891181, D_fake: 0.00041686600889079273\n",
            "Gradient Penalty: 19.725051879882812\n",
            "D_real: 0.007177264895290136, D_fake: 0.000511805759742856\n",
            "Gradient Penalty: 19.72389030456543\n",
            "D_real: 0.0065805260092020035, D_fake: 0.00048352041631005704\n",
            "Gradient Penalty: 19.721765518188477\n",
            "D_real: 0.006529381964355707, D_fake: 0.0005757855251431465\n",
            "Gradient Penalty: 19.721956253051758\n",
            "D_real: 0.007328318897634745, D_fake: 0.0005390867590904236\n",
            "Gradient Penalty: 19.715604782104492\n",
            "D_real: 0.0070442454889416695, D_fake: 0.00045455124927684665\n",
            "Gradient Penalty: 19.719463348388672\n",
            "D_real: 0.006446992978453636, D_fake: 0.0004981037927791476\n",
            "Gradient Penalty: 19.721315383911133\n",
            "D_real: 0.007567514665424824, D_fake: 0.0005352750886231661\n",
            "Gradient Penalty: 19.721813201904297\n",
            "D_real: 0.007083099335432053, D_fake: 0.0006061532767489552\n",
            "Gradient Penalty: 19.722043991088867\n",
            "D_real: 0.007411791943013668, D_fake: 0.0003519222664181143\n",
            "Gradient Penalty: 19.723289489746094\n",
            "D_real: 0.006333548575639725, D_fake: 0.0006197711918503046\n",
            "Gradient Penalty: 19.721391677856445\n",
            "D_real: 0.006578086409717798, D_fake: 0.00045981816947460175\n",
            "Gradient Penalty: 19.726959228515625\n",
            "D_real: 0.007794559467583895, D_fake: 0.0005519351689144969\n",
            "Gradient Penalty: 19.722097396850586\n",
            "D_real: 0.006139182485640049, D_fake: 0.0005743368528783321\n",
            "Gradient Penalty: 19.723106384277344\n",
            "D_real: 0.006661420688033104, D_fake: 0.0004841847694478929\n",
            "Gradient Penalty: 19.71929931640625\n",
            "D_real: 0.007181841880083084, D_fake: 0.0004933021264150739\n",
            "Gradient Penalty: 19.722824096679688\n",
            "D_real: 0.006435631774365902, D_fake: 0.0003725877613760531\n",
            "Gradient Penalty: 19.720670700073242\n",
            "D_real: 0.0076431576162576675, D_fake: 0.00047876100870780647\n",
            "Gradient Penalty: 19.72542953491211\n",
            "D_real: 0.006787043064832687, D_fake: 0.0006337867234833539\n",
            "Gradient Penalty: 19.724803924560547\n",
            "D_real: 0.004346099682152271, D_fake: 0.000469028513180092\n",
            "Gradient Penalty: 19.723772048950195\n",
            "D_real: 0.008863876573741436, D_fake: 0.00048225390492007136\n",
            "Gradient Penalty: 19.72480583190918\n",
            "D_real: 0.006813625805079937, D_fake: 0.00041435437742620707\n",
            "Gradient Penalty: 19.72357749938965\n",
            "D_real: 0.008279809728264809, D_fake: 0.0005910656764172018\n",
            "Gradient Penalty: 19.7221622467041\n",
            "D_real: 0.00716580543667078, D_fake: 0.0005253930576145649\n",
            "Gradient Penalty: 19.721908569335938\n",
            "D_real: 0.006643087603151798, D_fake: 0.0003882323217112571\n",
            "Gradient Penalty: 19.722537994384766\n",
            "D_real: 0.007507490459829569, D_fake: 0.0003900387091562152\n",
            "Gradient Penalty: 19.72068977355957\n",
            "D_real: 0.006114283110946417, D_fake: 0.0005098822293803096\n",
            "Gradient Penalty: 19.7196044921875\n",
            "D_real: 0.0068299914710223675, D_fake: 0.0004304450994823128\n",
            "Gradient Penalty: 19.720264434814453\n",
            "D_real: 0.005829189904034138, D_fake: 0.00040882639586925507\n",
            "Gradient Penalty: 19.72229766845703\n",
            "D_real: 0.00766235776245594, D_fake: 0.0004926050314679742\n",
            "Gradient Penalty: 19.723175048828125\n",
            "D_real: 0.005543607287108898, D_fake: 0.0004495411121752113\n",
            "Gradient Penalty: 19.720699310302734\n",
            "D_real: 0.007678613997995853, D_fake: 0.0003690627927426249\n",
            "Gradient Penalty: 19.722766876220703\n",
            "D_real: 0.007232732139527798, D_fake: 0.0004744933103211224\n",
            "Gradient Penalty: 19.722043991088867\n",
            "D_real: 0.0071401773020625114, D_fake: 0.0005677058361470699\n",
            "Gradient Penalty: 19.72101593017578\n",
            "D_real: 0.00682549923658371, D_fake: 0.00046484003541991115\n",
            "Gradient Penalty: 19.719329833984375\n",
            "D_real: 0.0070013925433158875, D_fake: 0.0004478679911699146\n",
            "Gradient Penalty: 19.722564697265625\n",
            "D_real: 0.0059874169528484344, D_fake: 0.0005106932949274778\n",
            "Gradient Penalty: 19.722307205200195\n",
            "D_real: 0.0066754939034581184, D_fake: 0.0004361921164672822\n",
            "Gradient Penalty: 19.720111846923828\n",
            "D_real: 0.00718029635027051, D_fake: 0.0005344193195924163\n",
            "Gradient Penalty: 19.723255157470703\n",
            "D_real: 0.006614961661398411, D_fake: 0.0005873157642781734\n",
            "Gradient Penalty: 19.723142623901367\n",
            "D_real: 0.006855820305645466, D_fake: 0.00044059849460609257\n",
            "Gradient Penalty: 19.719675064086914\n",
            "D_real: 0.007086700294166803, D_fake: 0.0004134882183279842\n",
            "Gradient Penalty: 19.714143753051758\n",
            "D_real: 0.007100328803062439, D_fake: 0.00036127533530816436\n",
            "Gradient Penalty: 19.724672317504883\n",
            "D_real: 0.006054811645299196, D_fake: 0.0004194452194496989\n",
            "Gradient Penalty: 19.72272491455078\n",
            "D_real: 0.006587293930351734, D_fake: 0.0005400069640018046\n",
            "Gradient Penalty: 19.719524383544922\n",
            "D_real: 0.007224476430565119, D_fake: 0.0003652168670669198\n",
            "Gradient Penalty: 19.720911026000977\n",
            "D_real: 0.006374443415552378, D_fake: 0.0006620044587180018\n",
            "Gradient Penalty: 19.72049331665039\n",
            "D_real: 0.007816413417458534, D_fake: 0.0003832060028798878\n",
            "Gradient Penalty: 19.725751876831055\n",
            "D_real: 0.00605905894190073, D_fake: 0.000573839177377522\n",
            "Gradient Penalty: 19.722068786621094\n",
            "D_real: 0.006944806780666113, D_fake: 0.0004384292697068304\n",
            "Gradient Penalty: 19.71977996826172\n",
            "D_real: 0.007390798069536686, D_fake: 0.0006411827052943408\n",
            "Gradient Penalty: 19.723201751708984\n",
            "D_real: 0.006162363104522228, D_fake: 0.00042008626041933894\n",
            "Gradient Penalty: 19.71879005432129\n",
            "D_real: 0.005768755450844765, D_fake: 0.0005164674948900938\n",
            "Gradient Penalty: 19.724021911621094\n",
            "D_real: 0.006316053681075573, D_fake: 0.0007086403202265501\n",
            "Gradient Penalty: 19.72102165222168\n",
            "D_real: 0.006404794752597809, D_fake: 0.0003895278205163777\n",
            "Gradient Penalty: 19.721904754638672\n",
            "D_real: 0.006812037900090218, D_fake: 0.0004953474272042513\n",
            "Gradient Penalty: 19.72265625\n",
            "D_real: 0.006064215209335089, D_fake: 0.000499599555041641\n",
            "Gradient Penalty: 19.724882125854492\n",
            "D_real: 0.0069052791222929955, D_fake: 0.0005965802120044827\n",
            "Gradient Penalty: 19.720762252807617\n",
            "D_real: 0.007050528656691313, D_fake: 0.0005654290434904397\n",
            "Gradient Penalty: 19.721466064453125\n",
            "D_real: 0.006389979273080826, D_fake: 0.0005870271124877036\n",
            "Gradient Penalty: 19.71843910217285\n",
            "D_real: 0.007254383526742458, D_fake: 0.00044635243830271065\n",
            "Gradient Penalty: 19.72406768798828\n",
            "D_real: 0.007098396308720112, D_fake: 0.0005120933055877686\n",
            "Gradient Penalty: 19.721439361572266\n",
            "D_real: 0.00683988630771637, D_fake: 0.0004596968647092581\n",
            "Gradient Penalty: 19.722911834716797\n",
            "D_real: 0.00752644706517458, D_fake: 0.0003607712278608233\n",
            "Gradient Penalty: 19.722835540771484\n",
            "D_real: 0.00663867499679327, D_fake: 0.0005757701583206654\n",
            "Gradient Penalty: 19.72127914428711\n",
            "D_real: 0.0072214785031974316, D_fake: 0.0005665900534950197\n",
            "Gradient Penalty: 19.724010467529297\n",
            "D_real: 0.00756628205999732, D_fake: 0.0005283075734041631\n",
            "Gradient Penalty: 19.724687576293945\n",
            "D_real: 0.0066322991624474525, D_fake: 0.00039721321081742644\n",
            "Gradient Penalty: 19.718873977661133\n",
            "D_real: 0.007411975413560867, D_fake: 0.0004959158832207322\n",
            "Gradient Penalty: 19.721389770507812\n",
            "D_real: 0.006390339694917202, D_fake: 0.0005181710585020483\n",
            "Gradient Penalty: 19.7226505279541\n",
            "D_real: 0.006482962518930435, D_fake: 0.0005049731116741896\n",
            "Gradient Penalty: 19.723167419433594\n",
            "D_real: 0.00664161890745163, D_fake: 0.000467963662231341\n",
            "Gradient Penalty: 19.7141170501709\n",
            "D_real: 0.006736868526786566, D_fake: 0.0004524204705376178\n",
            "Gradient Penalty: 19.716501235961914\n",
            "D_real: 0.006871170364320278, D_fake: 0.00042975801625289023\n",
            "Gradient Penalty: 19.719017028808594\n",
            "D_real: 0.006925816647708416, D_fake: 0.0005374296451918781\n",
            "Gradient Penalty: 19.72121238708496\n",
            "D_real: 0.006095842458307743, D_fake: 0.000317626167088747\n",
            "Gradient Penalty: 19.719736099243164\n",
            "D_real: 0.007470574229955673, D_fake: 0.0006168030085973442\n",
            "Gradient Penalty: 19.723844528198242\n",
            "D_real: 0.006488547660410404, D_fake: 0.0004487858386710286\n",
            "Gradient Penalty: 19.722476959228516\n",
            "D_real: 0.006931674666702747, D_fake: 0.0004708612395916134\n",
            "Gradient Penalty: 19.722305297851562\n",
            "D_real: 0.007559492718428373, D_fake: 0.0005180619191378355\n",
            "Gradient Penalty: 19.72380828857422\n",
            "D_real: 0.006213996559381485, D_fake: 0.000409752014093101\n",
            "Gradient Penalty: 19.721569061279297\n",
            "D_real: 0.007095244247466326, D_fake: 0.000521225156262517\n",
            "Gradient Penalty: 19.720256805419922\n",
            "D_real: 0.007228077854961157, D_fake: 0.0004547516582533717\n",
            "Gradient Penalty: 19.719411849975586\n",
            "D_real: 0.007693325635045767, D_fake: 0.0004805498174391687\n",
            "Gradient Penalty: 19.723400115966797\n",
            "D_real: 0.005131859332323074, D_fake: 0.0004681951832026243\n",
            "Gradient Penalty: 19.72266960144043\n",
            "D_real: 0.007481837645173073, D_fake: 0.000587421702221036\n",
            "Gradient Penalty: 19.724258422851562\n",
            "D_real: 0.006443725898861885, D_fake: 0.0006843923474662006\n",
            "Gradient Penalty: 19.72042465209961\n",
            "D_real: 0.0074443635530769825, D_fake: 0.0006231798324733973\n",
            "Gradient Penalty: 19.718875885009766\n",
            "D_real: 0.006652747746556997, D_fake: 0.0005249652313068509\n",
            "Gradient Penalty: 19.72324562072754\n",
            "D_real: 0.0071137286722660065, D_fake: 0.0003329986648168415\n",
            "Gradient Penalty: 19.722576141357422\n",
            "D_real: 0.008101120591163635, D_fake: 0.0004936609766446054\n",
            "Gradient Penalty: 19.724349975585938\n",
            "D_real: 0.007244606502354145, D_fake: 0.00042629832751117647\n",
            "Gradient Penalty: 19.720396041870117\n",
            "D_real: 0.0061464374884963036, D_fake: 0.00040522930794395506\n",
            "Gradient Penalty: 19.720523834228516\n",
            "D_real: 0.006282880436629057, D_fake: 0.0006399594130925834\n",
            "Gradient Penalty: 19.72054672241211\n",
            "D_real: 0.006772737018764019, D_fake: 0.0005558755365200341\n",
            "Gradient Penalty: 19.72431182861328\n",
            "D_real: 0.007498453371226788, D_fake: 0.0005043676937930286\n",
            "Gradient Penalty: 19.719701766967773\n",
            "D_real: 0.007252779323607683, D_fake: 0.0005497601814568043\n",
            "Gradient Penalty: 19.721500396728516\n",
            "D_real: 0.007586379535496235, D_fake: 0.0004382361366879195\n",
            "Gradient Penalty: 19.723159790039062\n",
            "D_real: 0.007010608911514282, D_fake: 0.0005717502208426595\n",
            "Gradient Penalty: 19.72308921813965\n",
            "D_real: 0.0073884837329387665, D_fake: 0.0005064605502411723\n",
            "Gradient Penalty: 19.72357749938965\n",
            "D_real: 0.006923484615981579, D_fake: 0.00042809583828784525\n",
            "Gradient Penalty: 19.719993591308594\n",
            "D_real: 0.006729858927428722, D_fake: 0.000456752663012594\n",
            "Gradient Penalty: 19.72126007080078\n",
            "D_real: 0.007745511829853058, D_fake: 0.000591767078731209\n",
            "Gradient Penalty: 19.724214553833008\n",
            "D_real: 0.00732521153986454, D_fake: 0.00039530504727736115\n",
            "Gradient Penalty: 19.723684310913086\n",
            "D_real: 0.0066016861237585545, D_fake: 0.0005772458971478045\n",
            "Gradient Penalty: 19.71950340270996\n",
            "D_real: 0.007863439619541168, D_fake: 0.0006263414979912341\n",
            "Gradient Penalty: 19.725196838378906\n",
            "D_real: 0.006990249268710613, D_fake: 0.0004282743320800364\n",
            "Gradient Penalty: 19.72226905822754\n",
            "D_real: 0.006296664476394653, D_fake: 0.0004239859408698976\n",
            "Gradient Penalty: 19.722898483276367\n",
            "D_real: 0.006446778774261475, D_fake: 0.00045681465417146683\n",
            "Gradient Penalty: 19.720924377441406\n",
            "D_real: 0.005845597479492426, D_fake: 0.0005862067919224501\n",
            "Gradient Penalty: 19.720233917236328\n",
            "D_real: 0.006465184036642313, D_fake: 0.000395783077692613\n",
            "Gradient Penalty: 19.724931716918945\n",
            "D_real: 0.0077265105210244656, D_fake: 0.0005409338627941906\n",
            "Gradient Penalty: 19.723299026489258\n",
            "D_real: 0.007548318710178137, D_fake: 0.0006465904298238456\n",
            "Gradient Penalty: 19.721355438232422\n",
            "D_real: 0.00542234443128109, D_fake: 0.00043989764526486397\n",
            "Gradient Penalty: 19.717247009277344\n",
            "D_real: 0.005907677114009857, D_fake: 0.0005766319809481502\n",
            "Gradient Penalty: 19.720705032348633\n",
            "D_real: 0.006854040082544088, D_fake: 0.00044132283073849976\n",
            "Gradient Penalty: 19.723798751831055\n",
            "D_real: 0.006800708826631308, D_fake: 0.00041047061677090824\n",
            "Gradient Penalty: 19.719615936279297\n",
            "D_real: 0.007233360782265663, D_fake: 0.000463788746856153\n",
            "Gradient Penalty: 19.721683502197266\n",
            "D_real: 0.006019096355885267, D_fake: 0.0004277659172657877\n",
            "Gradient Penalty: 19.72088050842285\n",
            "D_real: 0.006515034940093756, D_fake: 0.0006257182685658336\n",
            "Gradient Penalty: 19.721267700195312\n",
            "D_real: 0.007133259437978268, D_fake: 0.00045482872519642115\n",
            "Gradient Penalty: 19.719846725463867\n",
            "D_real: 0.007411278784275055, D_fake: 0.00041634897934272885\n",
            "Gradient Penalty: 19.720279693603516\n",
            "D_real: 0.006837326101958752, D_fake: 0.0005384684773162007\n",
            "Gradient Penalty: 19.718019485473633\n",
            "D_real: 0.007017193362116814, D_fake: 0.0005852974718436599\n",
            "Gradient Penalty: 19.719898223876953\n",
            "D_real: 0.007064334116876125, D_fake: 0.00046259997179731727\n",
            "Gradient Penalty: 19.720783233642578\n",
            "D_real: 0.007153315003961325, D_fake: 0.0005210511153563857\n",
            "Gradient Penalty: 19.720876693725586\n",
            "D_real: 0.005917700938880444, D_fake: 0.0005090493941679597\n",
            "Gradient Penalty: 19.722230911254883\n",
            "D_real: 0.0069953808560967445, D_fake: 0.00047711789375171065\n",
            "Gradient Penalty: 19.723045349121094\n",
            "D_real: 0.006734744645655155, D_fake: 0.00037074132706038654\n",
            "Gradient Penalty: 19.720252990722656\n",
            "D_real: 0.006470514461398125, D_fake: 0.0007345514604821801\n",
            "Gradient Penalty: 19.72003173828125\n",
            "D_real: 0.007958185859024525, D_fake: 0.00037089973920956254\n",
            "Gradient Penalty: 19.721092224121094\n",
            "D_real: 0.008209027349948883, D_fake: 0.0003976314328610897\n",
            "Gradient Penalty: 19.72396469116211\n",
            "D_real: 0.006995670031756163, D_fake: 0.00042426236905157566\n",
            "Gradient Penalty: 19.725431442260742\n",
            "D_real: 0.0070451050996780396, D_fake: 0.0004459102638065815\n",
            "Gradient Penalty: 19.72182273864746\n",
            "D_real: 0.0062949443235993385, D_fake: 0.0005883095436729491\n",
            "Gradient Penalty: 19.72370147705078\n",
            "D_real: 0.00730972969904542, D_fake: 0.0004985446576029062\n",
            "Gradient Penalty: 19.721088409423828\n",
            "D_real: 0.00646883063018322, D_fake: 0.00044659903505817056\n",
            "Gradient Penalty: 19.71769142150879\n",
            "D_real: 0.008635295554995537, D_fake: 0.000579035491682589\n",
            "Gradient Penalty: 19.727296829223633\n",
            "D_real: 0.006847939919680357, D_fake: 0.0005697146989405155\n",
            "Gradient Penalty: 19.717941284179688\n",
            "D_real: 0.006649951450526714, D_fake: 0.00039502145955339074\n",
            "Gradient Penalty: 19.716550827026367\n",
            "D_real: 0.007961411960422993, D_fake: 0.00040202387026511133\n",
            "Gradient Penalty: 19.722536087036133\n",
            "D_real: 0.007004585582762957, D_fake: 0.0005031792097724974\n",
            "Gradient Penalty: 19.722583770751953\n",
            "D_real: 0.0067941187880933285, D_fake: 0.0005531259812414646\n",
            "Gradient Penalty: 19.722457885742188\n",
            "D_real: 0.006973655428737402, D_fake: 0.0004949010326527059\n",
            "Gradient Penalty: 19.722454071044922\n",
            "D_real: 0.006332889199256897, D_fake: 0.00048478200915269554\n",
            "Gradient Penalty: 19.72305679321289\n",
            "D_real: 0.006447901483625174, D_fake: 0.0005338937044143677\n",
            "Gradient Penalty: 19.726221084594727\n",
            "D_real: 0.007733394391834736, D_fake: 0.00047950720181688666\n",
            "Gradient Penalty: 19.724441528320312\n",
            "D_real: 0.006581004709005356, D_fake: 0.0005184386391192675\n",
            "Gradient Penalty: 19.72062873840332\n",
            "D_real: 0.007166579365730286, D_fake: 0.000590708339586854\n",
            "Gradient Penalty: 19.723060607910156\n",
            "D_real: 0.0072097936645150185, D_fake: 0.0006549864774569869\n",
            "Gradient Penalty: 19.721803665161133\n",
            "D_real: 0.007588020525872707, D_fake: 0.0006440456490963697\n",
            "Gradient Penalty: 19.71969985961914\n",
            "D_real: 0.005481944419443607, D_fake: 0.0004976984346285462\n",
            "Gradient Penalty: 19.717567443847656\n",
            "D_real: 0.0073690093122422695, D_fake: 0.0004290993674658239\n",
            "Gradient Penalty: 19.72154998779297\n",
            "D_real: 0.006402833852916956, D_fake: 0.00039234591531567276\n",
            "Gradient Penalty: 19.72340202331543\n",
            "D_real: 0.006521513685584068, D_fake: 0.00044938491191715\n",
            "Gradient Penalty: 19.722148895263672\n",
            "D_real: 0.007244972512125969, D_fake: 0.000476422777865082\n",
            "Gradient Penalty: 19.725692749023438\n",
            "D_real: 0.00890430063009262, D_fake: 0.00045170600060373545\n",
            "Gradient Penalty: 19.72080421447754\n",
            "D_real: 0.00785414595156908, D_fake: 0.000616774836089462\n",
            "Gradient Penalty: 19.724943161010742\n",
            "D_real: 0.006624727975577116, D_fake: 0.0005688002565875649\n",
            "Gradient Penalty: 19.72408103942871\n",
            "D_real: 0.006991900969296694, D_fake: 0.00039518391713500023\n",
            "Gradient Penalty: 19.724576950073242\n",
            "D_real: 0.005884193815290928, D_fake: 0.000571270240470767\n",
            "Gradient Penalty: 19.72064781188965\n",
            "D_real: 0.0072864932008087635, D_fake: 0.0005320330383256078\n",
            "Gradient Penalty: 19.723346710205078\n",
            "D_real: 0.00613065343350172, D_fake: 0.0005606100894510746\n",
            "Gradient Penalty: 19.720365524291992\n",
            "D_real: 0.006246830802410841, D_fake: 0.0005017922376282513\n",
            "Gradient Penalty: 19.71820640563965\n",
            "D_real: 0.006549350451678038, D_fake: 0.000345336040481925\n",
            "Gradient Penalty: 19.723461151123047\n",
            "D_real: 0.006944918539375067, D_fake: 0.0005382897797971964\n",
            "Gradient Penalty: 19.721813201904297\n",
            "D_real: 0.008051874116063118, D_fake: 0.0004396073054522276\n",
            "Gradient Penalty: 19.722578048706055\n",
            "D_real: 0.006924991961568594, D_fake: 0.0004537873319350183\n",
            "Gradient Penalty: 19.721887588500977\n",
            "D_real: 0.007847831584513187, D_fake: 0.0005718031898140907\n",
            "Gradient Penalty: 19.72224998474121\n",
            "D_real: 0.006153922528028488, D_fake: 0.00046874169493094087\n",
            "Gradient Penalty: 19.718067169189453\n",
            "D_real: 0.007340561598539352, D_fake: 0.0005042990669608116\n",
            "Gradient Penalty: 19.721660614013672\n",
            "D_real: 0.005937499925494194, D_fake: 0.0004535617190413177\n",
            "Gradient Penalty: 19.721948623657227\n",
            "D_real: 0.007110441103577614, D_fake: 0.00046122391358949244\n",
            "Gradient Penalty: 19.72408676147461\n",
            "D_real: 0.007214682176709175, D_fake: 0.0005843597464263439\n",
            "Gradient Penalty: 19.724239349365234\n",
            "D_real: 0.006969441194087267, D_fake: 0.0005417911452241242\n",
            "Gradient Penalty: 19.72274398803711\n",
            "D_real: 0.00757380248978734, D_fake: 0.0004943131934851408\n",
            "Gradient Penalty: 19.72281265258789\n",
            "D_real: 0.006239787675440311, D_fake: 0.0006176111055538058\n",
            "Gradient Penalty: 19.720754623413086\n",
            "D_real: 0.007033936679363251, D_fake: 0.00043382984586060047\n",
            "Gradient Penalty: 19.718671798706055\n",
            "D_real: 0.007252458017319441, D_fake: 0.0004360078019089997\n",
            "Gradient Penalty: 19.72201156616211\n",
            "D_real: 0.006851213052868843, D_fake: 0.00040564214577898383\n",
            "Gradient Penalty: 19.719837188720703\n",
            "D_real: 0.007622968405485153, D_fake: 0.0005550288478843868\n",
            "Gradient Penalty: 19.721805572509766\n",
            "D_real: 0.006849369965493679, D_fake: 0.0004920662613585591\n",
            "Gradient Penalty: 19.718990325927734\n",
            "D_real: 0.006655232515186071, D_fake: 0.0005522400024347007\n",
            "Gradient Penalty: 19.71942710876465\n",
            "D_real: 0.006670184433460236, D_fake: 0.0004099260549992323\n",
            "Gradient Penalty: 19.72293472290039\n",
            "D_real: 0.007617480121552944, D_fake: 0.0005053839995525777\n",
            "Gradient Penalty: 19.721527099609375\n",
            "D_real: 0.005953130777925253, D_fake: 0.00045914892689324915\n",
            "Gradient Penalty: 19.721763610839844\n",
            "D_real: 0.0066591110080480576, D_fake: 0.0006951107643544674\n",
            "Gradient Penalty: 19.723392486572266\n",
            "D_real: 0.007054390385746956, D_fake: 0.0005801821826025844\n",
            "Gradient Penalty: 19.718666076660156\n",
            "D_real: 0.00718563050031662, D_fake: 0.0004600382235366851\n",
            "Gradient Penalty: 19.724123001098633\n",
            "D_real: 0.00713915703818202, D_fake: 0.0005543199367821217\n",
            "Gradient Penalty: 19.719310760498047\n",
            "D_real: 0.007462234236299992, D_fake: 0.0005071108462288976\n",
            "Gradient Penalty: 19.722787857055664\n",
            "D_real: 0.006632077507674694, D_fake: 0.0003966905060224235\n",
            "Gradient Penalty: 19.72040557861328\n",
            "D_real: 0.0072516705840826035, D_fake: 0.00043310021283105016\n",
            "Gradient Penalty: 19.72388458251953\n",
            "D_real: 0.0064271725714206696, D_fake: 0.00046411241055466235\n",
            "Gradient Penalty: 19.722822189331055\n",
            "D_real: 0.007665335666388273, D_fake: 0.0005375598557293415\n",
            "Gradient Penalty: 19.725242614746094\n",
            "D_real: 0.007324548903852701, D_fake: 0.0004301277513150126\n",
            "Gradient Penalty: 19.723628997802734\n",
            "D_real: 0.007380795665085316, D_fake: 0.0004899508785456419\n",
            "Gradient Penalty: 19.724119186401367\n",
            "D_real: 0.007496998179703951, D_fake: 0.00046109454706311226\n",
            "Gradient Penalty: 19.7257137298584\n",
            "D_real: 0.006286595016717911, D_fake: 0.0005789115093648434\n",
            "Gradient Penalty: 19.71860694885254\n",
            "D_real: 0.005476393736898899, D_fake: 0.0005444553098641336\n",
            "Gradient Penalty: 19.717823028564453\n",
            "D_real: 0.006132776848971844, D_fake: 0.0005264973733574152\n",
            "Gradient Penalty: 19.722591400146484\n",
            "D_real: 0.007300068624317646, D_fake: 0.0002808378776535392\n",
            "Gradient Penalty: 19.718067169189453\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFIElEQVR4nO3cMY7DMAwAQevg/3+Z122VAE6h2EBmahbsFiykNTNzAMBxHH93LwDAc4gCABEFACIKAEQUAIgoABBRACCiAEDOq4NrrZ17ALDZlbfKLgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAHLevQDwXDPz0fxaa9MmfItLAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8c0F8JZvK36PSwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEDOuxfgtZn5aH6ttWkT4Je4FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIP4+eih/GQF3cCkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAOS8OjgzO/cA4AFcCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoA5B+7KBMVrDRKaAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.0.weight - Grad Norm: 0.0002668013039510697\n",
            "main.2.weight - Grad Norm: 0.0002670749672688544\n",
            "main.4.weight - Grad Norm: 0.0002709153341129422\n",
            "main.6.weight - Grad Norm: 0.00027214817237108946\n",
            "Epoch [2/100] - D Loss: 19.7154, G Loss: -0.0000\n",
            "D_real: 0.010564880445599556, D_fake: 0.000520136789418757\n",
            "Gradient Penalty: 19.703950881958008\n",
            "D_real: 0.01097901165485382, D_fake: 0.0007446478120982647\n",
            "Gradient Penalty: 19.704360961914062\n",
            "D_real: 0.010330529883503914, D_fake: 0.000749406754039228\n",
            "Gradient Penalty: 19.709068298339844\n",
            "D_real: 0.010249335318803787, D_fake: 0.0005415311316028237\n",
            "Gradient Penalty: 19.710041046142578\n",
            "D_real: 0.010640495456755161, D_fake: 0.0006693274481222034\n",
            "Gradient Penalty: 19.70821762084961\n",
            "D_real: 0.011861768551170826, D_fake: 0.0009479719446972013\n",
            "Gradient Penalty: 19.708354949951172\n",
            "D_real: 0.01127631589770317, D_fake: 0.0006486561615020037\n",
            "Gradient Penalty: 19.70241355895996\n",
            "D_real: 0.010348329320549965, D_fake: 0.000555996666662395\n",
            "Gradient Penalty: 19.70679473876953\n",
            "D_real: 0.010637925006449223, D_fake: 0.000559620326384902\n",
            "Gradient Penalty: 19.706823348999023\n",
            "D_real: 0.011115219444036484, D_fake: 0.0006556706503033638\n",
            "Gradient Penalty: 19.70545196533203\n",
            "D_real: 0.010949406772851944, D_fake: 0.0007053309818729758\n",
            "Gradient Penalty: 19.70840835571289\n",
            "D_real: 0.010344864800572395, D_fake: 0.0006546977674588561\n",
            "Gradient Penalty: 19.70876121520996\n",
            "D_real: 0.012203292921185493, D_fake: 0.0006488856161013246\n",
            "Gradient Penalty: 19.703657150268555\n",
            "D_real: 0.010668483562767506, D_fake: 0.0007147999131120741\n",
            "Gradient Penalty: 19.70584487915039\n",
            "D_real: 0.010080454871058464, D_fake: 0.0007406342774629593\n",
            "Gradient Penalty: 19.70187759399414\n",
            "D_real: 0.011489796452224255, D_fake: 0.000757777423132211\n",
            "Gradient Penalty: 19.701860427856445\n",
            "D_real: 0.009871666319668293, D_fake: 0.0007988320430740714\n",
            "Gradient Penalty: 19.70170783996582\n",
            "D_real: 0.01015443541109562, D_fake: 0.0006865888135507703\n",
            "Gradient Penalty: 19.707412719726562\n",
            "D_real: 0.01103043183684349, D_fake: 0.0006772085907869041\n",
            "Gradient Penalty: 19.7055606842041\n",
            "D_real: 0.00981414970010519, D_fake: 0.0006552473641932011\n",
            "Gradient Penalty: 19.70384979248047\n",
            "D_real: 0.01108007412403822, D_fake: 0.0007108325371518731\n",
            "Gradient Penalty: 19.70574188232422\n",
            "D_real: 0.011370928026735783, D_fake: 0.00084069708827883\n",
            "Gradient Penalty: 19.706016540527344\n",
            "D_real: 0.010267328470945358, D_fake: 0.000729787047021091\n",
            "Gradient Penalty: 19.704389572143555\n",
            "D_real: 0.01100087072700262, D_fake: 0.0008223239565268159\n",
            "Gradient Penalty: 19.70220375061035\n",
            "D_real: 0.011365632526576519, D_fake: 0.0007566662388853729\n",
            "Gradient Penalty: 19.706090927124023\n",
            "D_real: 0.010532265529036522, D_fake: 0.000651578651741147\n",
            "Gradient Penalty: 19.70730972290039\n",
            "D_real: 0.01044666487723589, D_fake: 0.0007448437390848994\n",
            "Gradient Penalty: 19.70501708984375\n",
            "D_real: 0.012247422710061073, D_fake: 0.0007156142964959145\n",
            "Gradient Penalty: 19.704936981201172\n",
            "D_real: 0.01080450601875782, D_fake: 0.0007560956873930991\n",
            "Gradient Penalty: 19.704570770263672\n",
            "D_real: 0.010453004390001297, D_fake: 0.0007897127070464194\n",
            "Gradient Penalty: 19.705081939697266\n",
            "D_real: 0.0109324362128973, D_fake: 0.0007874388829804957\n",
            "Gradient Penalty: 19.710472106933594\n",
            "D_real: 0.01033194363117218, D_fake: 0.0006454288377426565\n",
            "Gradient Penalty: 19.70940399169922\n",
            "D_real: 0.009566830471158028, D_fake: 0.0007072777952998877\n",
            "Gradient Penalty: 19.704574584960938\n",
            "D_real: 0.011874602176249027, D_fake: 0.0006560668116435409\n",
            "Gradient Penalty: 19.709278106689453\n",
            "D_real: 0.009774090722203255, D_fake: 0.000647583045065403\n",
            "Gradient Penalty: 19.706588745117188\n",
            "D_real: 0.011970153078436852, D_fake: 0.0007739880820736289\n",
            "Gradient Penalty: 19.714635848999023\n",
            "D_real: 0.011116434819996357, D_fake: 0.0007902565412223339\n",
            "Gradient Penalty: 19.703094482421875\n",
            "D_real: 0.010755637660622597, D_fake: 0.0006594931473955512\n",
            "Gradient Penalty: 19.70716094970703\n",
            "D_real: 0.01096759270876646, D_fake: 0.0007230999181047082\n",
            "Gradient Penalty: 19.702041625976562\n",
            "D_real: 0.010466232895851135, D_fake: 0.0007821773178875446\n",
            "Gradient Penalty: 19.710575103759766\n",
            "D_real: 0.009938707575201988, D_fake: 0.0008500197436660528\n",
            "Gradient Penalty: 19.705745697021484\n",
            "D_real: 0.010778049938380718, D_fake: 0.0007775286212563515\n",
            "Gradient Penalty: 19.709487915039062\n",
            "D_real: 0.010713453404605389, D_fake: 0.0007194971549324691\n",
            "Gradient Penalty: 19.70260238647461\n",
            "D_real: 0.010862760245800018, D_fake: 0.0006746025756001472\n",
            "Gradient Penalty: 19.706586837768555\n",
            "D_real: 0.009888213127851486, D_fake: 0.0007517747581005096\n",
            "Gradient Penalty: 19.705446243286133\n",
            "D_real: 0.010887203738093376, D_fake: 0.000685977516695857\n",
            "Gradient Penalty: 19.707035064697266\n",
            "D_real: 0.011438712477684021, D_fake: 0.0008499843534082174\n",
            "Gradient Penalty: 19.705768585205078\n",
            "D_real: 0.00996939092874527, D_fake: 0.0007217488018795848\n",
            "Gradient Penalty: 19.70855712890625\n",
            "D_real: 0.010527530685067177, D_fake: 0.0007207858143374324\n",
            "Gradient Penalty: 19.706802368164062\n",
            "D_real: 0.011081034317612648, D_fake: 0.0006689808215014637\n",
            "Gradient Penalty: 19.69989013671875\n",
            "D_real: 0.01099567860364914, D_fake: 0.0007072689477354288\n",
            "Gradient Penalty: 19.70836639404297\n",
            "D_real: 0.010588464327156544, D_fake: 0.0006630186107940972\n",
            "Gradient Penalty: 19.70865249633789\n",
            "D_real: 0.01157725602388382, D_fake: 0.0007058840710669756\n",
            "Gradient Penalty: 19.702980041503906\n",
            "D_real: 0.01106688566505909, D_fake: 0.0008089120383374393\n",
            "Gradient Penalty: 19.71328353881836\n",
            "D_real: 0.01066244300454855, D_fake: 0.0007742418674752116\n",
            "Gradient Penalty: 19.70710563659668\n",
            "D_real: 0.01133248582482338, D_fake: 0.0006635214085690677\n",
            "Gradient Penalty: 19.7077693939209\n",
            "D_real: 0.010896636173129082, D_fake: 0.0007537504425272346\n",
            "Gradient Penalty: 19.713523864746094\n",
            "D_real: 0.01047696452587843, D_fake: 0.0009079907322302461\n",
            "Gradient Penalty: 19.700651168823242\n",
            "D_real: 0.010718798264861107, D_fake: 0.0007269415073096752\n",
            "Gradient Penalty: 19.704408645629883\n",
            "D_real: 0.010946953669190407, D_fake: 0.0008209001971408725\n",
            "Gradient Penalty: 19.699810028076172\n",
            "D_real: 0.009898927062749863, D_fake: 0.0005910978652536869\n",
            "Gradient Penalty: 19.708261489868164\n",
            "D_real: 0.009475117549300194, D_fake: 0.0005398088833317161\n",
            "Gradient Penalty: 19.70589256286621\n",
            "D_real: 0.011871017515659332, D_fake: 0.0006342277629300952\n",
            "Gradient Penalty: 19.704994201660156\n",
            "D_real: 0.011882256716489792, D_fake: 0.0006492013344541192\n",
            "Gradient Penalty: 19.70172119140625\n",
            "D_real: 0.011607473716139793, D_fake: 0.0005922256386838853\n",
            "Gradient Penalty: 19.707599639892578\n",
            "D_real: 0.01003984548151493, D_fake: 0.0005989137571305037\n",
            "Gradient Penalty: 19.706266403198242\n",
            "D_real: 0.01154262199997902, D_fake: 0.0007262135623022914\n",
            "Gradient Penalty: 19.70880126953125\n",
            "D_real: 0.009843038395047188, D_fake: 0.0006765915313735604\n",
            "Gradient Penalty: 19.70699691772461\n",
            "D_real: 0.011630145832896233, D_fake: 0.0006891023367643356\n",
            "Gradient Penalty: 19.704893112182617\n",
            "D_real: 0.011668318882584572, D_fake: 0.0006855272222310305\n",
            "Gradient Penalty: 19.705490112304688\n",
            "D_real: 0.011297401040792465, D_fake: 0.0007052848814055324\n",
            "Gradient Penalty: 19.703807830810547\n",
            "D_real: 0.010761898010969162, D_fake: 0.0007774428813718259\n",
            "Gradient Penalty: 19.70235824584961\n",
            "D_real: 0.01097902748733759, D_fake: 0.0006475221598520875\n",
            "Gradient Penalty: 19.699512481689453\n",
            "D_real: 0.010959772393107414, D_fake: 0.0006161894416436553\n",
            "Gradient Penalty: 19.704925537109375\n",
            "D_real: 0.010966066271066666, D_fake: 0.0006838190602138638\n",
            "Gradient Penalty: 19.704294204711914\n",
            "D_real: 0.009964941069483757, D_fake: 0.0007378190057352185\n",
            "Gradient Penalty: 19.712326049804688\n",
            "D_real: 0.010656241327524185, D_fake: 0.0007935373578220606\n",
            "Gradient Penalty: 19.712528228759766\n",
            "D_real: 0.011086128652095795, D_fake: 0.0006400751881301403\n",
            "Gradient Penalty: 19.706613540649414\n",
            "D_real: 0.01051756739616394, D_fake: 0.0009009751374833286\n",
            "Gradient Penalty: 19.70514678955078\n",
            "D_real: 0.00910796970129013, D_fake: 0.0006724604172632098\n",
            "Gradient Penalty: 19.705501556396484\n",
            "D_real: 0.011020981706678867, D_fake: 0.0007848436944186687\n",
            "Gradient Penalty: 19.704219818115234\n",
            "D_real: 0.011636001989245415, D_fake: 0.000761760922614485\n",
            "Gradient Penalty: 19.709897994995117\n",
            "D_real: 0.010531056672334671, D_fake: 0.00083457853179425\n",
            "Gradient Penalty: 19.704999923706055\n",
            "D_real: 0.009725311771035194, D_fake: 0.0005769698182120919\n",
            "Gradient Penalty: 19.705835342407227\n",
            "D_real: 0.010143006220459938, D_fake: 0.0005902549019083381\n",
            "Gradient Penalty: 19.711549758911133\n",
            "D_real: 0.01094310637563467, D_fake: 0.0006285464041866362\n",
            "Gradient Penalty: 19.704593658447266\n",
            "D_real: 0.010801833122968674, D_fake: 0.0007375198183581233\n",
            "Gradient Penalty: 19.70265769958496\n",
            "D_real: 0.009430095553398132, D_fake: 0.000666755368001759\n",
            "Gradient Penalty: 19.70813751220703\n",
            "D_real: 0.010717998258769512, D_fake: 0.0006244114483706653\n",
            "Gradient Penalty: 19.714954376220703\n",
            "D_real: 0.010457037948071957, D_fake: 0.0007704391609877348\n",
            "Gradient Penalty: 19.70441436767578\n",
            "D_real: 0.011481464840471745, D_fake: 0.00059524952666834\n",
            "Gradient Penalty: 19.709531784057617\n",
            "D_real: 0.011658708564937115, D_fake: 0.0006616051541641355\n",
            "Gradient Penalty: 19.70674705505371\n",
            "D_real: 0.012183284386992455, D_fake: 0.0006827673059888184\n",
            "Gradient Penalty: 19.71166229248047\n",
            "D_real: 0.010748396627604961, D_fake: 0.0007220588158816099\n",
            "Gradient Penalty: 19.705175399780273\n",
            "D_real: 0.009506773203611374, D_fake: 0.0005631800158880651\n",
            "Gradient Penalty: 19.702983856201172\n",
            "D_real: 0.010812543332576752, D_fake: 0.0006744411075487733\n",
            "Gradient Penalty: 19.701129913330078\n",
            "D_real: 0.0102864820510149, D_fake: 0.0006802377756685019\n",
            "Gradient Penalty: 19.711044311523438\n",
            "D_real: 0.01095366571098566, D_fake: 0.0006617421749979258\n",
            "Gradient Penalty: 19.703994750976562\n",
            "D_real: 0.00961607601493597, D_fake: 0.0008159607532434165\n",
            "Gradient Penalty: 19.702842712402344\n",
            "D_real: 0.011015173979103565, D_fake: 0.0006950317183509469\n",
            "Gradient Penalty: 19.709232330322266\n",
            "D_real: 0.011308250948786736, D_fake: 0.0006658757920376956\n",
            "Gradient Penalty: 19.70474624633789\n",
            "D_real: 0.010735254734754562, D_fake: 0.0006729939486831427\n",
            "Gradient Penalty: 19.708873748779297\n",
            "D_real: 0.009925097227096558, D_fake: 0.000893118791282177\n",
            "Gradient Penalty: 19.705459594726562\n",
            "D_real: 0.00948502030223608, D_fake: 0.000700275064446032\n",
            "Gradient Penalty: 19.709125518798828\n",
            "D_real: 0.010071594268083572, D_fake: 0.0007895039161667228\n",
            "Gradient Penalty: 19.70553207397461\n",
            "D_real: 0.01033550500869751, D_fake: 0.0007077058544382453\n",
            "Gradient Penalty: 19.705150604248047\n",
            "D_real: 0.009805044159293175, D_fake: 0.0007522878004238009\n",
            "Gradient Penalty: 19.705881118774414\n",
            "D_real: 0.010554987005889416, D_fake: 0.0007774826372042298\n",
            "Gradient Penalty: 19.70491600036621\n",
            "D_real: 0.010641142725944519, D_fake: 0.0007380453171208501\n",
            "Gradient Penalty: 19.705219268798828\n",
            "D_real: 0.010562380775809288, D_fake: 0.0007059191702865064\n",
            "Gradient Penalty: 19.708816528320312\n",
            "D_real: 0.009858782403171062, D_fake: 0.0006431328365579247\n",
            "Gradient Penalty: 19.71283721923828\n",
            "D_real: 0.011300168000161648, D_fake: 0.0007608789019286633\n",
            "Gradient Penalty: 19.705982208251953\n",
            "D_real: 0.010743261314928532, D_fake: 0.00059192453045398\n",
            "Gradient Penalty: 19.712072372436523\n",
            "D_real: 0.01067502610385418, D_fake: 0.0006735072238370776\n",
            "Gradient Penalty: 19.699878692626953\n",
            "D_real: 0.01021565217524767, D_fake: 0.0005891750915907323\n",
            "Gradient Penalty: 19.70654296875\n",
            "D_real: 0.010226118378341198, D_fake: 0.0007272962247952819\n",
            "Gradient Penalty: 19.707386016845703\n",
            "D_real: 0.012084306217730045, D_fake: 0.0007211682386696339\n",
            "Gradient Penalty: 19.705745697021484\n",
            "D_real: 0.010827131569385529, D_fake: 0.0007755265687592328\n",
            "Gradient Penalty: 19.70992660522461\n",
            "D_real: 0.010683240368962288, D_fake: 0.0006444314494729042\n",
            "Gradient Penalty: 19.706043243408203\n",
            "D_real: 0.01045214757323265, D_fake: 0.0008452111505903304\n",
            "Gradient Penalty: 19.708059310913086\n",
            "D_real: 0.011310390196740627, D_fake: 0.0007038497133180499\n",
            "Gradient Penalty: 19.702672958374023\n",
            "D_real: 0.011080369353294373, D_fake: 0.0008171171648427844\n",
            "Gradient Penalty: 19.705703735351562\n",
            "D_real: 0.011427309364080429, D_fake: 0.0007186979637481272\n",
            "Gradient Penalty: 19.700592041015625\n",
            "D_real: 0.010452916845679283, D_fake: 0.0006533013074658811\n",
            "Gradient Penalty: 19.70838737487793\n",
            "D_real: 0.011550767347216606, D_fake: 0.0007810269016772509\n",
            "Gradient Penalty: 19.710187911987305\n",
            "D_real: 0.01035884115844965, D_fake: 0.0006679232465103269\n",
            "Gradient Penalty: 19.703081130981445\n",
            "D_real: 0.010702457278966904, D_fake: 0.0008507061284035444\n",
            "Gradient Penalty: 19.704490661621094\n",
            "D_real: 0.01041758619248867, D_fake: 0.0007744984468445182\n",
            "Gradient Penalty: 19.70480728149414\n",
            "D_real: 0.011681749485433102, D_fake: 0.00066646549385041\n",
            "Gradient Penalty: 19.704483032226562\n",
            "D_real: 0.011407732963562012, D_fake: 0.0006843357696197927\n",
            "Gradient Penalty: 19.706424713134766\n",
            "D_real: 0.011071911081671715, D_fake: 0.0008305949158966541\n",
            "Gradient Penalty: 19.707773208618164\n",
            "D_real: 0.008938439190387726, D_fake: 0.0008041097316890955\n",
            "Gradient Penalty: 19.708574295043945\n",
            "D_real: 0.012041183188557625, D_fake: 0.0007140551460906863\n",
            "Gradient Penalty: 19.70756721496582\n",
            "D_real: 0.01056364644318819, D_fake: 0.000593113130889833\n",
            "Gradient Penalty: 19.70474624633789\n",
            "D_real: 0.010275476612150669, D_fake: 0.0007458311156369746\n",
            "Gradient Penalty: 19.704273223876953\n",
            "D_real: 0.010984553024172783, D_fake: 0.0006296432111412287\n",
            "Gradient Penalty: 19.70907974243164\n",
            "D_real: 0.010820926167070866, D_fake: 0.0006957808509469032\n",
            "Gradient Penalty: 19.705501556396484\n",
            "D_real: 0.010898762382566929, D_fake: 0.0007865941151976585\n",
            "Gradient Penalty: 19.703826904296875\n",
            "D_real: 0.009807639755308628, D_fake: 0.0007457947940565646\n",
            "Gradient Penalty: 19.705379486083984\n",
            "D_real: 0.009980819188058376, D_fake: 0.0007076237234286964\n",
            "Gradient Penalty: 19.707332611083984\n",
            "D_real: 0.011303812265396118, D_fake: 0.0005849393783137202\n",
            "Gradient Penalty: 19.70909881591797\n",
            "D_real: 0.010315654799342155, D_fake: 0.0005926763406023383\n",
            "Gradient Penalty: 19.703571319580078\n",
            "D_real: 0.010610982775688171, D_fake: 0.0006729181623086333\n",
            "Gradient Penalty: 19.705427169799805\n",
            "D_real: 0.010742833837866783, D_fake: 0.0006131352274678648\n",
            "Gradient Penalty: 19.710861206054688\n",
            "D_real: 0.010805806145071983, D_fake: 0.0006653431337326765\n",
            "Gradient Penalty: 19.7044620513916\n",
            "D_real: 0.010615740902721882, D_fake: 0.0005470524192787707\n",
            "Gradient Penalty: 19.709077835083008\n",
            "D_real: 0.010091925039887428, D_fake: 0.0006019780994392931\n",
            "Gradient Penalty: 19.707359313964844\n",
            "D_real: 0.010488012805581093, D_fake: 0.0007934460882097483\n",
            "Gradient Penalty: 19.71236801147461\n",
            "D_real: 0.01016930676996708, D_fake: 0.0005715612787753344\n",
            "Gradient Penalty: 19.7120361328125\n",
            "D_real: 0.010723443701863289, D_fake: 0.0006243382813408971\n",
            "Gradient Penalty: 19.706607818603516\n",
            "D_real: 0.01028599962592125, D_fake: 0.0007704384042881429\n",
            "Gradient Penalty: 19.70514488220215\n",
            "D_real: 0.009880910627543926, D_fake: 0.0007521322695538402\n",
            "Gradient Penalty: 19.70958709716797\n",
            "D_real: 0.011241784319281578, D_fake: 0.000816005514934659\n",
            "Gradient Penalty: 19.70406723022461\n",
            "D_real: 0.01028533186763525, D_fake: 0.0006054604309611022\n",
            "Gradient Penalty: 19.704730987548828\n",
            "D_real: 0.011203041300177574, D_fake: 0.0006912080571055412\n",
            "Gradient Penalty: 19.707643508911133\n",
            "D_real: 0.010775646194815636, D_fake: 0.0006813135114498436\n",
            "Gradient Penalty: 19.711149215698242\n",
            "D_real: 0.010884540155529976, D_fake: 0.000655577692668885\n",
            "Gradient Penalty: 19.70370101928711\n",
            "D_real: 0.010448729619383812, D_fake: 0.0005396559135988355\n",
            "Gradient Penalty: 19.707124710083008\n",
            "D_real: 0.010179642587900162, D_fake: 0.0006587737007066607\n",
            "Gradient Penalty: 19.704788208007812\n",
            "D_real: 0.010466013103723526, D_fake: 0.0005968493642285466\n",
            "Gradient Penalty: 19.707382202148438\n",
            "D_real: 0.010799822397530079, D_fake: 0.0006269282894209027\n",
            "Gradient Penalty: 19.70594024658203\n",
            "D_real: 0.009646367281675339, D_fake: 0.0006726991850882769\n",
            "Gradient Penalty: 19.70691680908203\n",
            "D_real: 0.011131031438708305, D_fake: 0.0006691047456115484\n",
            "Gradient Penalty: 19.709383010864258\n",
            "D_real: 0.010338245891034603, D_fake: 0.0005804285174235702\n",
            "Gradient Penalty: 19.704008102416992\n",
            "D_real: 0.01126832515001297, D_fake: 0.0007552747847512364\n",
            "Gradient Penalty: 19.70755386352539\n",
            "D_real: 0.010047124698758125, D_fake: 0.0007687321049161255\n",
            "Gradient Penalty: 19.706016540527344\n",
            "D_real: 0.009429261088371277, D_fake: 0.0006523790070787072\n",
            "Gradient Penalty: 19.70698356628418\n",
            "D_real: 0.010221986100077629, D_fake: 0.0006773159257136285\n",
            "Gradient Penalty: 19.707462310791016\n",
            "D_real: 0.011327166110277176, D_fake: 0.0005783314118161798\n",
            "Gradient Penalty: 19.706958770751953\n",
            "D_real: 0.011781580746173859, D_fake: 0.0007537599885836244\n",
            "Gradient Penalty: 19.704395294189453\n",
            "D_real: 0.010995591059327126, D_fake: 0.0006118196761235595\n",
            "Gradient Penalty: 19.705095291137695\n",
            "D_real: 0.010580230504274368, D_fake: 0.0006953441770747304\n",
            "Gradient Penalty: 19.70897674560547\n",
            "D_real: 0.010061172768473625, D_fake: 0.0008087820606306195\n",
            "Gradient Penalty: 19.706989288330078\n",
            "D_real: 0.011659055016934872, D_fake: 0.0007607016013935208\n",
            "Gradient Penalty: 19.709444046020508\n",
            "D_real: 0.01047247089445591, D_fake: 0.0006579388282261789\n",
            "Gradient Penalty: 19.708019256591797\n",
            "D_real: 0.010788924060761929, D_fake: 0.000598275160882622\n",
            "Gradient Penalty: 19.705459594726562\n",
            "D_real: 0.00896547269076109, D_fake: 0.0006502708420157433\n",
            "Gradient Penalty: 19.704090118408203\n",
            "D_real: 0.010423926636576653, D_fake: 0.0008323884103447199\n",
            "Gradient Penalty: 19.70798110961914\n",
            "D_real: 0.011262480169534683, D_fake: 0.0007108377176336944\n",
            "Gradient Penalty: 19.710004806518555\n",
            "D_real: 0.011265034787356853, D_fake: 0.0008179732831194997\n",
            "Gradient Penalty: 19.70318031311035\n",
            "D_real: 0.009874653071165085, D_fake: 0.0006639368366450071\n",
            "Gradient Penalty: 19.707015991210938\n",
            "D_real: 0.01117720641195774, D_fake: 0.0008005502168089151\n",
            "Gradient Penalty: 19.708986282348633\n",
            "D_real: 0.011477283202111721, D_fake: 0.0007449277327395976\n",
            "Gradient Penalty: 19.705890655517578\n",
            "D_real: 0.010036180727183819, D_fake: 0.0007298850687220693\n",
            "Gradient Penalty: 19.701396942138672\n",
            "D_real: 0.009067448787391186, D_fake: 0.0006559530738741159\n",
            "Gradient Penalty: 19.706005096435547\n",
            "D_real: 0.009827956557273865, D_fake: 0.0007501859799958766\n",
            "Gradient Penalty: 19.70293426513672\n",
            "D_real: 0.01007360965013504, D_fake: 0.000712446984834969\n",
            "Gradient Penalty: 19.705045700073242\n",
            "D_real: 0.010170899331569672, D_fake: 0.0007984635885804892\n",
            "Gradient Penalty: 19.703256607055664\n",
            "D_real: 0.010421503335237503, D_fake: 0.0007607943844050169\n",
            "Gradient Penalty: 19.701692581176758\n",
            "D_real: 0.009510891512036324, D_fake: 0.000747120357118547\n",
            "Gradient Penalty: 19.70762825012207\n",
            "D_real: 0.010234637185931206, D_fake: 0.0007631730404682457\n",
            "Gradient Penalty: 19.710004806518555\n",
            "D_real: 0.011378801427781582, D_fake: 0.0007746444316580892\n",
            "Gradient Penalty: 19.707033157348633\n",
            "D_real: 0.011428968980908394, D_fake: 0.0006344770081341267\n",
            "Gradient Penalty: 19.709217071533203\n",
            "D_real: 0.012081892229616642, D_fake: 0.0008260591421276331\n",
            "Gradient Penalty: 19.707950592041016\n",
            "D_real: 0.010359330102801323, D_fake: 0.0007136169588193297\n",
            "Gradient Penalty: 19.702045440673828\n",
            "D_real: 0.01127585582435131, D_fake: 0.0006805548910051584\n",
            "Gradient Penalty: 19.706295013427734\n",
            "D_real: 0.009814322926104069, D_fake: 0.0006226562545634806\n",
            "Gradient Penalty: 19.703857421875\n",
            "D_real: 0.010051006451249123, D_fake: 0.0006422084989026189\n",
            "Gradient Penalty: 19.709884643554688\n",
            "D_real: 0.011524404399096966, D_fake: 0.0008514403598383069\n",
            "Gradient Penalty: 19.704801559448242\n",
            "D_real: 0.011499117128551006, D_fake: 0.0007347713108174503\n",
            "Gradient Penalty: 19.703004837036133\n",
            "D_real: 0.01050119660794735, D_fake: 0.0007159521337598562\n",
            "Gradient Penalty: 19.705259323120117\n",
            "D_real: 0.009791729971766472, D_fake: 0.0006946291541680694\n",
            "Gradient Penalty: 19.70452880859375\n",
            "D_real: 0.010143935680389404, D_fake: 0.000686370418407023\n",
            "Gradient Penalty: 19.708972930908203\n",
            "D_real: 0.010336717590689659, D_fake: 0.0007131669553928077\n",
            "Gradient Penalty: 19.706707000732422\n",
            "D_real: 0.010135667398571968, D_fake: 0.0007087012636475265\n",
            "Gradient Penalty: 19.706552505493164\n",
            "D_real: 0.010425837710499763, D_fake: 0.0007895071757957339\n",
            "Gradient Penalty: 19.70450782775879\n",
            "D_real: 0.010179297998547554, D_fake: 0.0007229417096823454\n",
            "Gradient Penalty: 19.708635330200195\n",
            "D_real: 0.010292284190654755, D_fake: 0.0006313449703156948\n",
            "Gradient Penalty: 19.70836067199707\n",
            "D_real: 0.010137144476175308, D_fake: 0.0006804863223806024\n",
            "Gradient Penalty: 19.708370208740234\n",
            "D_real: 0.01230902411043644, D_fake: 0.0007490644929930568\n",
            "Gradient Penalty: 19.702917098999023\n",
            "D_real: 0.010964172892272472, D_fake: 0.0007708386401645839\n",
            "Gradient Penalty: 19.705150604248047\n",
            "D_real: 0.010693183168768883, D_fake: 0.0007580884266644716\n",
            "Gradient Penalty: 19.708471298217773\n",
            "D_real: 0.010547732003033161, D_fake: 0.0009995757136493921\n",
            "Gradient Penalty: 19.706174850463867\n",
            "D_real: 0.011396952904760838, D_fake: 0.0009354045614600182\n",
            "Gradient Penalty: 19.708293914794922\n",
            "D_real: 0.009611694142222404, D_fake: 0.0006781984120607376\n",
            "Gradient Penalty: 19.703893661499023\n",
            "D_real: 0.011746769770979881, D_fake: 0.0007508103735744953\n",
            "Gradient Penalty: 19.711002349853516\n",
            "D_real: 0.010323617607355118, D_fake: 0.0007199827232398093\n",
            "Gradient Penalty: 19.705059051513672\n",
            "D_real: 0.01080936100333929, D_fake: 0.0006166170351207256\n",
            "Gradient Penalty: 19.707487106323242\n",
            "D_real: 0.009504512883722782, D_fake: 0.0007651670603081584\n",
            "Gradient Penalty: 19.710208892822266\n",
            "D_real: 0.010497900657355785, D_fake: 0.0006630621501244605\n",
            "Gradient Penalty: 19.704477310180664\n",
            "D_real: 0.011070206761360168, D_fake: 0.0007805743953213096\n",
            "Gradient Penalty: 19.70277976989746\n",
            "D_real: 0.01063032541424036, D_fake: 0.0006936784484423697\n",
            "Gradient Penalty: 19.707904815673828\n",
            "D_real: 0.010212797671556473, D_fake: 0.0008176269475370646\n",
            "Gradient Penalty: 19.706241607666016\n",
            "D_real: 0.011114736087620258, D_fake: 0.0007878156611695886\n",
            "Gradient Penalty: 19.701988220214844\n",
            "D_real: 0.010441355407238007, D_fake: 0.0006914748810231686\n",
            "Gradient Penalty: 19.707828521728516\n",
            "D_real: 0.01065027341246605, D_fake: 0.0008089006296359003\n",
            "Gradient Penalty: 19.702880859375\n",
            "D_real: 0.010474813170731068, D_fake: 0.0008581571746617556\n",
            "Gradient Penalty: 19.704893112182617\n",
            "D_real: 0.010691625997424126, D_fake: 0.0008151771617121994\n",
            "Gradient Penalty: 19.703861236572266\n",
            "D_real: 0.012075815349817276, D_fake: 0.000685383565723896\n",
            "Gradient Penalty: 19.70516014099121\n",
            "D_real: 0.011601128615438938, D_fake: 0.0007287673652172089\n",
            "Gradient Penalty: 19.70884132385254\n",
            "D_real: 0.009396388195455074, D_fake: 0.0006678461795672774\n",
            "Gradient Penalty: 19.710796356201172\n",
            "D_real: 0.010558590292930603, D_fake: 0.0007412758423015475\n",
            "Gradient Penalty: 19.707130432128906\n",
            "D_real: 0.010434146039187908, D_fake: 0.0006851105717942119\n",
            "Gradient Penalty: 19.7049617767334\n",
            "D_real: 0.010903555899858475, D_fake: 0.0006995260482653975\n",
            "Gradient Penalty: 19.708166122436523\n",
            "D_real: 0.009300069883465767, D_fake: 0.0009528217487968504\n",
            "Gradient Penalty: 19.70459747314453\n",
            "D_real: 0.010000869631767273, D_fake: 0.00045724783558398485\n",
            "Gradient Penalty: 19.70259666442871\n",
            "D_real: 0.011158941313624382, D_fake: 0.0004903939552605152\n",
            "Gradient Penalty: 19.70943832397461\n",
            "D_real: 0.010814686305820942, D_fake: 0.0007668992038816214\n",
            "Gradient Penalty: 19.71204948425293\n",
            "D_real: 0.00977993942797184, D_fake: 0.0006726249121129513\n",
            "Gradient Penalty: 19.706937789916992\n",
            "D_real: 0.009799320250749588, D_fake: 0.0005767739494331181\n",
            "Gradient Penalty: 19.70279884338379\n",
            "D_real: 0.010793954133987427, D_fake: 0.0006988695240579545\n",
            "Gradient Penalty: 19.706127166748047\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-53b9002182ea>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Move real images to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mx_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;31m# When any inputs are FakeScriptObject, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}