{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = '/content/gdrive/My Drive/character_font.npz'\n",
    "\n",
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, npz_file, transform=None, filter_label=None, num_samples=None):\n",
    "        # Load the data from the .npz file\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['labels']\n",
    "        self.transform = transform\n",
    "\n",
    "        # Filter by label if specified\n",
    "        if filter_label is not None:\n",
    "            # Find indices of the desired label\n",
    "            label_indices = np.where(self.labels == filter_label)[0]\n",
    "\n",
    "            # If num_samples is specified, limit the number of samples\n",
    "            if num_samples is not None:\n",
    "                label_indices = label_indices[:num_samples]\n",
    "\n",
    "            # Filter images and labels\n",
    "            self.images = self.images[label_indices]\n",
    "            self.labels = self.labels[label_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image and label for the given index\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Reshape the image to add a channel dimension\n",
    "        image = image[np.newaxis, ...]  # Add channel dimension at the beginning\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to PyTorch tensor if necessary\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.init_size = 2  # Initial spatial size\n",
    "        #self.latent_dim = latent_dim\n",
    "        \n",
    "        # In the paper it is not specified how z transforms into a 4x4x512 (for 64x64 data) so we will use a linear layer to do so.\n",
    "        self.fc = nn.Linear(latent_dim, 512 * self.init_size * self.init_size)\n",
    "        # No relu because we just had to reshape the latent vector to something that when flattened is 2 * 32^2, which 2x2x512 is\n",
    "\n",
    "        \n",
    "        # We make the transposed convolutional layers (fractionally strided convolutions)\n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            # Block 1: Input (2x2x512) -> Output (4x4x256)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 2: Input (4x4x256)-> Output (8x8x128)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 3: Input (8x8x128) -> Output (16x16x64)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final Block: Input (16x16x64) -> Output (32x32x1)\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Pass the latent vector through the fully connected layer\n",
    "        out = self.fc(z)\n",
    "        \n",
    "        # Reshape to match the initial feature map dimensions. out.size(0) = batch size.  \n",
    "        out = out.view(out.size(0), 512, self.init_size, self.init_size)\n",
    "        \n",
    "        # We pass \"out\" through the transposed convolutional blocks\n",
    "        img = self.deconv_blocks(out)\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # We make the transposed convolutional layers (fractionally strided convolutions)\n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            # Block 1: Input (32x32x1) -> Output (16x16x64)\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Block 2: Input (16x16x64)-> Output (8x8x128)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Block 3: Input (8x8x128) -> Output (4x4x256)\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Final Block: Input (4x4x256) -> Output (2x2x512)\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \n",
    "        # We pass the image through the convolutional blocks\n",
    "        result = self.deconv_blocks(img)\n",
    "        \n",
    "        return result.view(-1, 1) # The dimensions will be (batchsize, 1) where 1 is the prediction (0 or 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
