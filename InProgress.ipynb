{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_directml \n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import random_split\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = '/content/gdrive/My Drive/character_font.npz'\n",
    "\n",
    "\n",
    "\n",
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, npz_file, transform=None, filter_label=None, num_samples=None):\n",
    "        # Load the data from the .npz file\n",
    "        data = np.load(npz_file)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['labels']\n",
    "        self.transform = transform\n",
    "\n",
    "        # Filter by label if specified\n",
    "        if filter_label is not None:\n",
    "            # Find indices of the desired label\n",
    "            label_indices = np.where(self.labels == filter_label)[0]\n",
    "\n",
    "            # If num_samples is specified, limit the number of samples\n",
    "            if num_samples is not None:\n",
    "                label_indices = label_indices[:num_samples]\n",
    "\n",
    "            # Filter images and labels\n",
    "            self.images = self.images[label_indices]\n",
    "            self.labels = self.labels[label_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image and label for the given index\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Reshape the image to add a channel dimension\n",
    "        image = image[np.newaxis, ...]  # Add channel dimension at the beginning\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to PyTorch tensor if necessary\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.init_size = 2  # Initial spatial size\n",
    "        #self.latent_dim = latent_dim\n",
    "        \n",
    "        # In the paper it is not specified how z transforms into a 4x4x512 (for 64x64 data) so we will use a linear layer to do so.\n",
    "        self.fc = nn.Linear(latent_dim, 512 * self.init_size * self.init_size)\n",
    "        # No relu because we just had to reshape the latent vector to something that when flattened is 2 * 32^2, which 2x2x512 is\n",
    "\n",
    "        \n",
    "        # We make the transposed convolutional layers (fractionally strided convolutions)\n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            # Block 1: Input (2x2x512) -> Output (4x4x256)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 2: Input (4x4x256)-> Output (8x8x128)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 3: Input (8x8x128) -> Output (16x16x64)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final Block: Input (16x16x64) -> Output (32x32x1)\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Pass the latent vector through the fully connected layer\n",
    "        out = self.fc(z)\n",
    "        \n",
    "        # Reshape to match the initial feature map dimensions. out.size(0) = batch size.  \n",
    "        out = out.view(out.size(0), 512, self.init_size, self.init_size)\n",
    "        \n",
    "        # We pass \"out\" through the transposed convolutional blocks\n",
    "        img = self.deconv_blocks(out)\n",
    "        \n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # The input of the discriminator is already an image so we know it has 1 channel (greyscale) and is 32x32\n",
    "\n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            # Block 1: Input (32x32x1) -> Output (16x16x64)\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Block 2: Input (16x16x64)-> Output (8x8x128)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Block 3: Input (8x8x128) -> Output (4x4x256)\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Final Block: Input (4x4x256) -> Output (2x2x512)\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \n",
    "        # We pass the image through the convolutional blocks\n",
    "        result = self.deconv_blocks(img)\n",
    "        \n",
    "        return result.view(-1, 1) # The dimensions will be (batchsize, 1) where 1 is the prediction (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayGeneratedImage(class_index, generator, z_dim, num_classes, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates and displays an image for a given class using the generator.\n",
    "\n",
    "    Args:\n",
    "        class_index (int): Index of the character class to generate (0 to num_classes - 1).\n",
    "        generator (nn.Module): Pre-trained generator model.\n",
    "        z_dim (int): Dimension of the style vector.\n",
    "        num_classes (int): Number of character classes.\n",
    "        device (str): Device for computation ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    # Ensure the class index is valid\n",
    "    if not (0 <= class_index < num_classes):\n",
    "        raise ValueError(f\"Invalid class_index: {class_index}. Must be in range [0, {num_classes - 1}].\")\n",
    "\n",
    "    # Create the one-hot vector for the class\n",
    "    z_c = torch.zeros(1, num_classes, device=device)\n",
    "    z_c[0, class_index] = 1  # Set the desired class\n",
    "\n",
    "    # Create the random style vector\n",
    "    z_s = torch.randn(1, z_dim, device=device)\n",
    "\n",
    "    # Concatenate the style and class vectors\n",
    "    z = torch.cat((z_s, z_c), dim=1)\n",
    "\n",
    "    # Generate the image\n",
    "    with torch.no_grad():\n",
    "        generated_img = generator(z).cpu().numpy()[0, 0]  # Extract the first batch and first channel\n",
    "\n",
    "    # Rescale the image from [-1, 1] to [0, 255]\n",
    "    generated_img = np.rot90(generated_img, k=-1)\n",
    "    generated_img = np.uint8(np.interp(generated_img, (-1, 1), (0, 255)))\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(generated_img, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Generated Image for Class {class_index}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc): Linear(in_features=126, out_features=2048, bias=True)\n",
       "  (deconv_blocks): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 100\n",
    "num_classes = 26  \n",
    "img_size = 32  \n",
    "\n",
    "\n",
    "# Specify is nvdia or amd\n",
    "\n",
    "amd = True\n",
    "if amd: \n",
    "    dml = torch_directml.device()\n",
    "    device = (\n",
    "    dml if torch_directml.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "else: \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator(latent_dim=126).to(device)\n",
    "\n",
    "# inital weights of the discriminator and generator  https://stats.stackexchange.com/questions/319323/whats-the-difference-between-variance-scaling-initializer-and-xavier-initialize\n",
    "\n",
    "# Since we have more Leaky and Relu activations we chose to go with a normally distributed kaimning initialization \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d): \n",
    "        nn.init.kaiming_normal_(m.weight, a=0.2)  \n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "generator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/character_font.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m n_critic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of discriminator updates per generator update\u001b[39;00m\n\u001b[0;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[1;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNPZDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_file\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      8\u001b[0m dlosses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m glosses \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mNPZDataset.__init__\u001b[1;34m(self, npz_file, transform, filter_label, num_samples)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, npz_file, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filter_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Load the data from the .npz file\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thomas Brejner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/character_font.npz'"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 1024\n",
    "lr = 0.0002\n",
    "lambda_gp = 10  \n",
    "n_critic = 5  # Number of discriminator updates per generator update\n",
    "epochs = 2500\n",
    "dataset = NPZDataset(npz_file) \n",
    "dlosses = []\n",
    "\n",
    "glosses = []\n",
    "gloss = 0\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.99))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.99))\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    for c in range(num_classes):\n",
    "        class_samples = [sample for sample in train_dataset if sample[1] == c]\n",
    "        class_loader = DataLoader(class_samples, batch_size=batch_size, shuffle=True)\n",
    "        one_hot = torch.zeros(num_classes, device=device)\n",
    "        one_hot[c] = 1\n",
    "\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        gloss = 0\n",
    "\n",
    "        for i in range(n_critic): \n",
    "            \n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            dloss = 0\n",
    "\n",
    "\n",
    "            for batch in class_loader:\n",
    "                real_data, _ = batch\n",
    "                real_data = real_data.to(device)  \n",
    "                real_data.requires_grad = True # If needed later \n",
    "                \n",
    "                epsilon = torch.rand(size=batch_size)\n",
    "                epsilon = torch.tensor(epsilon, device=device).view(batch_size, 1, 1, 1)  # Make it into a 4D tensor \n",
    "\n",
    "                \n",
    "                z_s = torch.rand(batch_size, 100, device=device) \n",
    "                z_combined = torch.cat((z_s, one_hot.expand(batch_size, -1)), dim=1)\n",
    "\n",
    "                # Here we do the interpolation between real and fake fonts (generator(z_combined))\n",
    "                xHat = real_data * epsilon + (1-epsilon) * generator(z_combined)\n",
    "                xHat.requires_grad = True\n",
    "                \n",
    "\n",
    "                probability_interpolated = discriminator(xHat) \n",
    "                grad_outputs = torch.ones_like(probability_interpolated, device=device)\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=probability_interpolated,\n",
    "                    inputs=xHat,\n",
    "                    grad_outputs=grad_outputs,\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "\n",
    "                grad_norm = gradients.view(gradients.size(0), -1).norm(2, dim=1)\n",
    "\n",
    "                gradient_penalty = lambda_gp * ((grad_norm - 1) ** 2).mean()\n",
    "\n",
    "                dloss += discriminator(generator(z_combined)).mean() - discriminator(real_data).mean() + gradient_penalty\n",
    "                \n",
    "            \n",
    "            avg_Dloss = dloss/len(class_loader) \n",
    "            dlosses.append(avg_Dloss)\n",
    "            \n",
    "            # Backpropagation \n",
    "            avg_Dloss.backward() \n",
    "            optimizer_D.step()\n",
    "\n",
    "\n",
    "        z_s = torch.rand(batch_size, 100, device=device) \n",
    "        z_combined = torch.cat((z_s, one_hot.expand(batch_size, -1)), dim=1)\n",
    "        gloss = -discriminator(generator(z_combined)).mean()\n",
    "        glosses.append(gloss)\n",
    "\n",
    "        # Backpropagation \n",
    "        gloss.backward() \n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Display generated image every 20 epochs (Can be modified to what you prefer)\n",
    "    if epoch % 20 == 0: \n",
    "        displayGeneratedImage(class_index=c, generator=generator, z_dim=z_dim, num_classes=num_classes, device='cuda')\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], D_loss: {dloss.item()}, G_loss: {gloss.item()}\")   \n",
    "        \n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
